<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小歪的博客</title>
  <subtitle>人生苦短，我学Python</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangslob.github.io/"/>
  <updated>2017-12-23T13:48:09.728Z</updated>
  <id>https://zhangslob.github.io/</id>
  
  <author>
    <name>小歪</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>采集方案策略之App抓包</title>
    <link href="https://zhangslob.github.io/2017/12/23/%E9%87%87%E9%9B%86%E6%96%B9%E6%A1%88%E7%AD%96%E7%95%A5%E4%B9%8BApp%E6%8A%93%E5%8C%85/"/>
    <id>https://zhangslob.github.io/2017/12/23/采集方案策略之App抓包/</id>
    <published>2017-12-23T13:40:26.000Z</published>
    <updated>2017-12-23T13:48:09.728Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十二篇原创文章
</code></pre><p><img src="https://i.imgur.com/NRBPj4t.jpg" alt=""></p>
<a id="more"></a>
<h1 id="采集方案策略设计"><a href="#采集方案策略设计" class="headerlink" title="采集方案策略设计"></a>采集方案策略设计</h1><p>在群里看到有人询问饿了么的参数，正好感兴趣，就来时间一番。</p>
<p>这里引用下大佬的一段话：</p>
<p>首先大的地方，我们想抓取某个数据源，我们要知道大概有哪些路径可以获取到数据源，基本上无外乎三种：</p>
<ul>
<li>PC端网站</li>
<li>针对移动设备响应式设计的网站（也就是很多人说的H5, 虽然不一定是H5）；</li>
<li>移动App</li>
</ul>
<p>原则是能抓移动App的，最好抓移动App，如果有针对移动设备优化的网站，就抓针对移动设备优化的网站，最后考虑PC网站。因为移动App基本都是API很简单，而移动设备访问优化的网站一般来讲都是结构简单清晰的HTML，而PC网站自然是最复杂的了；<br>针对PC端网站和移动网站的做法一样，分析思路可以一起讲，移动App单独分析。</p>
<p>其实很多网页都有移动端，像微博，我知道这三个：</p>
<ol>
<li>weibo.com</li>
<li>weibo.cn</li>
<li>m.weibo.cn</li>
</ol>
<p>最简单的当然是第二种了，对于今天的受害者——饿了么来说，当然，首选也是移动端。</p>
<h1 id="饿了么抓包分析"><a href="#饿了么抓包分析" class="headerlink" title="饿了么抓包分析"></a>饿了么抓包分析</h1><p>这里抓包工具选择<strong>Fiddler</strong>，这里不讲如何配置，具体参考 <a href="https://www.cnblogs.com/abao0/p/7008995.html" target="_blank" rel="external">用Fiddler对Android应用进行抓包</a></p>
<p>下面打开手机的饿了么，原本以为会有数据，结果，竟然是这样</p>
<p><img src="https://i.imgur.com/KmL5L9d.png" alt=""></p>
<p>去询问了专业人士，了解了有些应用不允许用户抓包，会有相应的限制。好吧，这就能难倒我了吗？？</p>
<p>当我切换到<strong>发现</strong>类目下，发现有奇怪的提示</p>
<p><img src="https://i.imgur.com/zF1e1iV.png" alt=""></p>
<p>在疯狂点击<strong>继续访问</strong>后，我终于可以正常访问了。</p>
<p><img src="https://i.imgur.com/DwDXNM8.jpg" alt=""></p>
<p>那么就可以在Fiddler中查看对应的数据了。这里直接把接口展示出来：<a href="https://restapi.ele.me/shopping/v1/find/recommendation?offset=20&amp;limit=40&amp;latitude=39.93245&amp;longitude=116.50097" target="_blank" rel="external">饿了么接口</a></p>
<p>浏览器直接打开，貌似没有啥验证</p>
<p><img src="https://i.imgur.com/S2IcQvP.png" alt=""></p>
<p>具体分析里面的参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">offset:20</div><div class="line">limit:40</div><div class="line">latitude:39.93245</div><div class="line">longitude:116.50097</div></pre></td></tr></table></figure>
<p>有4个参数， <code>offset</code> 和 <code>limit</code> 就很常见了，翻页和每页的数据，至于 <code>latitude</code> 和 <code>longitude</code> 仔细观察就知道，经纬度嘛，把它该修改为你想采集的位置的经纬度就好。</p>
<p>抓包分析之后，接下来采集数据就很简单了，数据字段标识：</p>
<p><code>food_id</code> 是商品ID，接口是：<code>https://www.ele.me/restapi/shopping/v1/foods?food_ids%5B%5D=712859937</code></p>
<p>打开此链接就是商品详情内容</p>
<p><img src="https://i.imgur.com/n5zMOEl.png" alt=""></p>
<p><img src="https://i.imgur.com/YzUyohN.jpg" alt=""></p>
<blockquote>
<p>可爱的小笼包</p>
</blockquote>
<p><code>restaurant_id</code> 是店铺ID，接口 <code>https://www.ele.me/shop/157458556</code></p>
<p>打开就是店铺详情页，当然，也有移动端：<code>https://h5.ele.me/shop/#id=157458556</code></p>
<p>这样进行商品采集就比较轻松了。</p>
<p>好饿，容我先点个外卖。</p>
<hr>
<h1 id="微信公众号抓包分析"><a href="#微信公众号抓包分析" class="headerlink" title="微信公众号抓包分析"></a>微信公众号抓包分析</h1><p>既然都看了饿了么，那也来看看微信吧。</p>
<p>使用Fiddler抓出来的curl命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -k -i --raw -o 0.dat &quot;https://mp.weixin.qq.com/mp/profile_ext?action=getmsg&amp;__biz=MzA4NzA1OTc5Nw==&amp;f=json&amp;offset=20&amp;count=10&amp;appmsg_token=936_iWFH%%252F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~&quot; -H &quot;User-Agent: Mozilla/5.0 (Linux; Android 7.1.1; MI 6 Build/NMF26X; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.49 Mobile MQQBrowser/6.2 TBS/043632 Safari/537.36 MicroMessenger/6.5.23.1180 NetType/WIFI Language/zh_CN&quot; -H &quot;Accept-Encoding: gzip, deflate&quot; -H &quot;Accept: */*&quot; -H &quot;Connection: keep-alive&quot; -H &quot;Host: mp.weixin.qq.com&quot; -H &quot;X-Requested-With: XMLHttpRequest&quot; -H &quot;Referer: https://mp.weixin.qq.com/mp/profile_ext?action=home&amp;__biz=MzA4NzA1OTc5Nw==&amp;scene=124&amp;devicetype=android-25&amp;version=26051732&amp;lang=zh_CN&amp;nettype=WIFI&amp;a8scene=3&amp;pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8%%2BLUbHoWGGBEp&amp;wx_header=1&quot; -H &quot;Accept-Language: zh-CN,en-US;q=0.8&quot; -H &quot;Cookie: pgv_pvi=4831552512; pgv_si=s989715456; sd_userid=18991505459750403; sd_cookie_crttime=1505459750403; tvfe_boss_uuid=a8e4e4f1ab6cd93d; pgv_info=ssid=s8735681072; pgv_pvid=4201362299; rewardsn=8d8b49dfb1811092eefe; wxtokenkey=19643e9f2ee569a10857d365bba88556d220fd33c1a0666b5d028a72b5bcd901; wxuin=838107840; devicetype=android-25; version=26051732; lang=zh_CN; pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8+LUbHoWGGBEp; wap_sid2=CMCF0o8DElxUVDVJR3o1ZldpbDlHWWdjQ0xMU3lxM3BWTUozTFFuZFhrUEJaanhoSmZ1aEVncnU0VzFIaWR3QkVVVXFuTUlMTlkxNFZjTnRCMEt1VHJjV3UzQVNOYWdEQUFBfjD6rvjRBTgMQJRO&quot; -H &quot;Q-UA2: QV=3&amp;PL=ADR&amp;PR=WX&amp;PP=com.tencent.mm&amp;PPVN=6.5.23&amp;TBSVC=43602&amp;CO=BK&amp;COVC=043632&amp;PB=GE&amp;VE=GA&amp;DE=PHONE&amp;CHID=0&amp;LCID=9422&amp;MO= MI6 &amp;RL=1080*1920&amp;OS=7.1.1&amp;API=25&quot; -H &quot;Q-GUID: 569ade09b5931656e4f49098113e88cb&quot; -H &quot;Q-Auth: 31045b957cf33acf31e40be2f3e71c5217597676a9729f1b&quot; -H &quot;Content-Type: application/json; charset=UTF-8&quot; -H &quot;Cache-Control: no-cache, must-revalidate&quot; -H &quot;RetKey: 14&quot; -H &quot;LogicRet: 0&quot;</div></pre></td></tr></table></figure>
<p>直接在浏览器中打开，会提示错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">ret: -3,</div><div class="line">errmsg: &quot;no session&quot;,</div><div class="line">cookie_count: 0</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>使用postman分析，最后Python的代码是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">url = <span class="string">"https://mp.weixin.qq.com/mp/profile_ext"</span></div><div class="line"></div><div class="line">querystring = &#123;<span class="string">"action"</span>:<span class="string">"getmsg"</span>,<span class="string">"__biz"</span>:<span class="string">"MzA4NzA1OTc5Nw=="</span>,<span class="string">"f"</span>:<span class="string">"json"</span>,<span class="string">"offset"</span>:<span class="string">"20"</span>,<span class="string">"count"</span>:<span class="string">"10"</span>,<span class="string">"appmsg_token"</span>:<span class="string">"936_iWFH%2F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~"</span>&#125;</div><div class="line"></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">'user-agent'</span>: <span class="string">"Mozilla/5.0 (Linux; Android 7.1.1; MI 6 Build/NMF26X; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.49 Mobile MQQBrowser/6.2 TBS/043632 Safari/537.36 MicroMessenger/6.5.23.1180 NetType/WIFI Language/zh_CN"</span>,</div><div class="line">    <span class="string">'accept-encoding'</span>: <span class="string">"gzip, deflate"</span>,</div><div class="line">    <span class="string">'accept'</span>: <span class="string">"*/*"</span>,</div><div class="line">    <span class="string">'connection'</span>: <span class="string">"keep-alive"</span>,</div><div class="line">    <span class="string">'host'</span>: <span class="string">"mp.weixin.qq.com"</span>,</div><div class="line">    <span class="string">'x-requested-with'</span>: <span class="string">"XMLHttpRequest"</span>,</div><div class="line">    <span class="string">'referer'</span>: <span class="string">"https://mp.weixin.qq.com/mp/profile_ext?action=home&amp;__biz=MzA4NzA1OTc5Nw==&amp;scene=124&amp;devicetype=android-25&amp;version=26051732&amp;lang=zh_CN&amp;nettype=WIFI&amp;a8scene=3&amp;pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8%%2BLUbHoWGGBEp&amp;wx_header=1"</span>,</div><div class="line">    <span class="string">'accept-language'</span>: <span class="string">"zh-CN,en-US;q=0.8"</span>,</div><div class="line">    <span class="string">'cookie'</span>: <span class="string">"pgv_pvi=4831552512; pgv_si=s989715456; sd_userid=18991505459750403; sd_cookie_crttime=1505459750403; tvfe_boss_uuid=a8e4e4f1ab6cd93d; pgv_info=ssid=s8735681072; pgv_pvid=4201362299; rewardsn=8d8b49dfb1811092eefe; wxtokenkey=19643e9f2ee569a10857d365bba88556d220fd33c1a0666b5d028a72b5bcd901; wxuin=838107840; devicetype=android-25; version=26051732; lang=zh_CN; pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8+LUbHoWGGBEp; wap_sid2=CMCF0o8DElxUVDVJR3o1ZldpbDlHWWdjQ0xMU3lxM3BWTUozTFFuZFhrUEJaanhoSmZ1aEVncnU0VzFIaWR3QkVVVXFuTUlMTlkxNFZjTnRCMEt1VHJjV3UzQVNOYWdEQUFBfjD6rvjRBTgMQJRO"</span>,</div><div class="line">    <span class="string">'q-ua2'</span>: <span class="string">"QV=3&amp;PL=ADR&amp;PR=WX&amp;PP=com.tencent.mm&amp;PPVN=6.5.23&amp;TBSVC=43602&amp;CO=BK&amp;COVC=043632&amp;PB=GE&amp;VE=GA&amp;DE=PHONE&amp;CHID=0&amp;LCID=9422&amp;MO= MI6 &amp;RL=1080*1920&amp;OS=7.1.1&amp;API=25"</span>,</div><div class="line">    <span class="string">'q-guid'</span>: <span class="string">"569ade09b5931656e4f49098113e88cb"</span>,</div><div class="line">    <span class="string">'q-auth'</span>: <span class="string">"31045b957cf33acf31e40be2f3e71c5217597676a9729f1b"</span>,</div><div class="line">    <span class="string">'content-type'</span>: <span class="string">"application/json; charset=UTF-8"</span>,</div><div class="line">    <span class="string">'cache-control'</span>: <span class="string">"no-cache"</span>,</div><div class="line">    <span class="string">'retkey'</span>: <span class="string">"14"</span>,</div><div class="line">    <span class="string">'logicret'</span>: <span class="string">"0"</span>,</div><div class="line">    &#125;</div><div class="line"></div><div class="line">response = requests.request(<span class="string">"GET"</span>, url, headers=headers, params=querystring, verify=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">print(response.json())</div></pre></td></tr></table></figure>
<p>这个时候的参数有</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">action:getmsg</div><div class="line">__biz:MzA4NzA1OTc5Nw==</div><div class="line">f:json</div><div class="line">offset:20</div><div class="line">count:10</div><div class="line">appmsg_token:936_iWFH%%252F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~</div></pre></td></tr></table></figure>
<p>目前还不清楚这些参数的作用，再抓一个试试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">action:getmsg</div><div class="line">__biz:MjM5NzI3NDg4MA==</div><div class="line">f:json</div><div class="line">offset:10</div><div class="line">count:10</div><div class="line">appmsg_token:936_kFNdYU3DJ%%252B%%252BVfHfEGImXqB5DMbIeqtSR75ZFZQ~~</div></pre></td></tr></table></figure>
<p>估计就是 <code>__biz</code> 和 <code>appmsg_token</code> 这两个参数对应不同的公众号</p>
<p>对了，上面的代码会出现一个问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">InsecureRequestWarning: Unverified HTTPS request <span class="keyword">is</span> being made. Adding certificate verification <span class="keyword">is</span> strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html<span class="comment">#ssl-warnings</span></div><div class="line">  InsecureRequestWarning)</div></pre></td></tr></table></figure>
<p>解决方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</div><div class="line">urllib3.disable_warnings()</div></pre></td></tr></table></figure>
<h1 id="App的分析"><a href="#App的分析" class="headerlink" title="App的分析"></a>App的分析</h1><p>App类使用的工具是Fidder，手机和电脑在一个局域网内，先用Fidder配置好端口，然后手机设置代理，ip为电脑的ip，端口为设置的端口，然后如果手机上请求网络内容时，Fidder会显示相应地请求，那么就ok了，分析的大体逻辑基本一致，限制会相对少很多，但是也有几种情况需要注意：</p>
<ul>
<li>加密，App有时候也有一些加密的字段，这个时候,一般来讲都会进行反编译进行分析，找到对应的代码片段，逆推出加密方法；</li>
<li>gzip压缩或者base64编码，base64编码的辨别度较高，有时候数据被gzip压缩了，不过Charles都是有自动解密的；</li>
<li>https证书，有的https请求会验证证书, Fidder提供了证书，可以在官网找到，手机访问，然后信任添加就可以。</li>
</ul>
<p>最后，<strong>祝大家圣诞节快乐</strong></p>
<p><img src="https://i.imgur.com/zQTLLCy.gif" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十二篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/NRBPj4t.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="https://zhangslob.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="抓包" scheme="https://zhangslob.github.io/tags/%E6%8A%93%E5%8C%85/"/>
    
  </entry>
  
  <entry>
    <title>【RNG vs SKT】弹幕的自然语言的初步分析</title>
    <link href="https://zhangslob.github.io/2017/12/20/%E3%80%90RNG-vs-SKT%E3%80%91%E5%BC%B9%E5%B9%95%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%9A%84%E5%88%9D%E6%AD%A5%E5%88%86%E6%9E%90/"/>
    <id>https://zhangslob.github.io/2017/12/20/【RNG-vs-SKT】弹幕的自然语言的初步分析/</id>
    <published>2017-12-20T14:17:35.000Z</published>
    <updated>2017-12-20T14:40:46.728Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十一篇原创文章
</code></pre><p><img src="https://i.imgur.com/xf7kEI8.jpg" alt=""></p>
<a id="more"></a>
<p>前排 @皇族电子竞技俱乐部</p>
<p>==================================</p>
<p>S7中RNG对阵SKT，想必是全世界LOL玩家关注的重点。在比赛开始前，使用小葫芦把斗鱼S7直播间的弹幕都抓下来，想着做一小点分析，看看会得出什么结论。</p>
<p>因为数据量和分析深度等原因，以下内容仅供娱乐观赏</p>
<h1 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h1><p>使用小葫芦采集2万多条弹幕数据，RNG对阵SKT斗鱼直播间的弹幕，最后得到约4万可用数据</p>
<h1 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h1><p>使用jieba分词，算法如下</p>
<ul>
<li>基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)</li>
<li>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合</li>
<li>对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法</li>
</ul>
<p>简单处理之后，看起来是这样</p>
<p><img src="https://i.imgur.com/Zm6mMti.jpg" alt=""></p>
<p>顺便做个统计，看看都在说什么。不加停用词是这样的，全是 “<strong>666</strong>”</p>
<p><img src="https://i.imgur.com/df59MQw.jpg" alt=""></p>
<p>RNG和牛逼是出现最多的词语，同时也发现“<strong>卢本伟牛逼</strong>”</p>
<p><img src="https://i.imgur.com/6PcshNl.jpg" alt=""></p>
<blockquote>
<p>弱弱问一句“唐梦琼”是谁</p>
</blockquote>
<p>下面是词云，Python的词云做不来不好看，所以我使用的工具 <a href="https://timdream.org/wordcloud/#" target="_blank" rel="external">HTML5 Word Cloud</a></p>
<p><img src="https://i.imgur.com/R6ltujq.jpg" alt=""></p>
<blockquote>
<p>弹幕内容词云</p>
</blockquote>
<p><img src="https://i.imgur.com/PMISApU.jpg" alt=""></p>
<blockquote>
<p>用户昵称词云</p>
</blockquote>
<h1 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h1><p>这里使用的是 <a href="https://github.com/isnowfy/snownlp" target="_blank" rel="external">isnowfy/snownlp</a>。SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了<a href="https://github.com/sloria/TextBlob" target="_blank" rel="external">TextBlob</a>的启发而写的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> snownlp <span class="keyword">import</span> SnowNLP</div><div class="line"></div><div class="line">s = SnowNLP(<span class="string">u'这个东西真心很赞'</span>)</div><div class="line">s.sentiments    <span class="comment"># 0.9769663402895832 positive的概率</span></div></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/iEuDbFT.jpg" alt=""></p>
<p>有点难看，直接看数据吧，得到的结果是在 [0，1] 之间的<strong>positive的概率</strong></p>
<p>51659中有44705个大于0.5，占比86.54%，有 6954条弹幕低于0.5，占比13.46%。</p>
<p>弹幕中积极的概率还是相当高的，说明观众还是比较赞赏比赛的。</p>
<p><img src="https://i.imgur.com/FAOkrny.jpg" alt=""></p>
<h1 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h1><p>TF-IDF是信息检索领域非常重要的搜索词重要性度量；tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。</p>
<p>词频TF(Term Frequency)</p>
<p>词w在文档d中出现次数count(w, d)和文档d中总词数size(d)的比值：</p>
<p><img src="https://i.imgur.com/OttArJD.jpg" alt=""></p>
<p>基于 TF-IDF 算法的关键词抽取：</p>
<ul>
<li>RNG</li>
<li>rng</li>
<li>贺电</li>
<li>加油</li>
<li>发来</li>
<li>666</li>
<li>6666</li>
<li>66666</li>
<li>恭喜</li>
<li>66666666666666</li>
<li>666666</li>
<li>66666666666</li>
<li>6666666</li>
<li>skt</li>
<li>李哥</li>
<li>66666666666666666666</li>
<li>SKT</li>
<li>66666666</li>
<li>666666666</li>
<li>马甲</li>
</ul>
<p>在没有加上停用词的前提下，可以看到效果并不理想</p>
<p>TextRank是在Google的PageRank算法启发下，针对文本里的句子设计的权重算法，目标是自动摘要。它利用投票的原理，让每一个单词给它的邻居（术语称窗口）投赞成票，票的权重取决于自己的票数。</p>
<p><img src="https://i.imgur.com/AI6dJ5P.jpg" alt=""></p>
<p>基于 TextRank 算法的关键词抽取：</p>
<ul>
<li>发来</li>
<li>贺电</li>
<li>加油</li>
<li>学院</li>
<li>机器人</li>
<li>大学</li>
<li>职业</li>
<li>船长</li>
<li>技术</li>
<li>小炮</li>
<li>没有</li>
<li>解说</li>
<li>中国</li>
<li>经济</li>
<li>开始</li>
<li>无敌</li>
<li>香锅</li>
<li>垃圾</li>
<li>老鼠</li>
<li>科技</li>
</ul>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>Word2vec是一个将单词表征成向量的形式，它可以把文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度。</p>
<h2 id="计算相似度："><a href="#计算相似度：" class="headerlink" title="计算相似度："></a>计算相似度：</h2><p>首先是RNG：</p>
<ul>
<li>rng 0.9893965721130371</li>
<li>加油 0.9829007983207703</li>
<li>必胜 0.9790929555892944</li>
<li>Rng 0.9743078947067261</li>
<li>恭喜 0.9733642339706421</li>
<li>中国队 0.9397183656692505</li>
<li>关 0.9283092021942139</li>
<li>&lt; 0.9278833866119385</li>
<li>燕尾港 0.9222617149353027</li>
<li>B 0.9143030643463135</li>
</ul>
<p><strong>RNG加油</strong></p>
<p>看看SKT：</p>
<ul>
<li>skt 0.9911665320396423</li>
<li>皇族 0.972029983997345</li>
<li>＾ 0.970970094203949</li>
<li>稳住 0.9653508067131042</li>
<li>干死 0.9643667340278625</li>
<li>牛比 0.9629441499710083</li>
<li>一起 0.9625348448753357</li>
<li>@ 0.9619969725608826</li>
<li>鸟巢 0.9608791470527649</li>
<li>冠军 0.9608250856399536</li>
</ul>
<p><strong>稳住，干死SKT？</strong></p>
<p>贺电：</p>
<ul>
<li>学院 0.9811943769454956</li>
<li>大学 0.980491042137146</li>
<li>技术 0.9766334295272827</li>
<li>职业 0.9691535234451294</li>
<li>电子科技 0.9668481349945068</li>
<li>发来 0.9619022607803345</li>
<li>科技 0.9594647884368896</li>
<li>山东 0.9568137526512146</li>
<li>重庆 0.9440888166427612</li>
<li>哈尔滨 0.939836859703064</li>
</ul>
<p><strong>山东XX学院发来贺电</strong></p>
<p>加油：</p>
<ul>
<li>必胜 0.9876022934913635</li>
<li>RNG 0.9829007983207703</li>
<li>rng 0.960281252861023</li>
<li>Rng 0.9591789841651917</li>
<li>恭喜 0.9551880359649658</li>
<li>中国队 0.9436988830566406</li>
<li>关 0.94183349609375</li>
<li>！ 0.921385645866394</li>
<li>~ 0.9148629903793335</li>
<li>@ 0.9062787294387817</li>
</ul>
<p><strong>RNG加油，RNG必胜</strong></p>
<p>小狗：</p>
<ul>
<li>吹 0.9970275163650513</li>
<li>无敌 0.996719241142273</li>
<li>神超 0.996111273765564</li>
<li>后期 0.9959050416946411</li>
<li>厉害 0.9957337975502014</li>
<li>凶 0.9957261681556702</li>
<li>强 0.9955072402954102</li>
<li>一个 0.9954395890235901</li>
<li>干 0.99541175365448</li>
<li>起来 0.9952359199523926</li>
</ul>
<p><strong>狗吹？</strong></p>
<p>李哥：</p>
<ul>
<li>还是 0.9825356602668762</li>
<li>电话 0.9700809717178345</li>
<li>承 0.9697628617286682</li>
<li>心脏 0.9686012864112854</li>
<li>陈文泽在 0.9681863188743591</li>
<li>麻痹 0.9680625200271606</li>
<li>响 0.9674116373062134</li>
<li>以为 0.9664229154586792</li>
<li>狗哥 0.9592204689979553</li>
<li>不 0.9589840769767761</li>
</ul>
<p><strong>你李哥还是你李哥</strong></p>
<p>MLXG：</p>
<ul>
<li>宣告 0.9958090782165527</li>
<li>mlxg 0.9953181147575378</li>
<li>死亡 0.995277464389801</li>
<li>b 0.9949076771736145</li>
<li>6666 0.9947425723075867</li>
<li>丑 0.9943945407867432</li>
<li>10 0.9943088293075562</li>
<li>辣鸡 0.9940722584724426</li>
<li>干死 0.9940391778945923</li>
<li>锤 0.9939616918563843</li>
</ul>
<p><strong>香锅和死亡宣告有啥关系</strong></p>
<p>小虎：</p>
<ul>
<li>笑笑 0.9971799850463867</li>
<li>看到 0.9967395663261414</li>
<li>解说 0.9961692690849304</li>
<li>不是 0.9959656000137329</li>
<li>中单 0.9951503872871399</li>
<li>假 0.9950063824653625</li>
<li>为什么 0.9944812655448914</li>
<li>又 0.9942663908004761</li>
<li>么 0.9938984513282776</li>
<li>里奥 0.9937981367111206</li>
</ul>
<p><strong>小虎与加里奥（：</strong></p>
<p>letme：</p>
<ul>
<li>难受 0.9964221715927124</li>
<li>笑话 0.9959778785705566</li>
<li>哦 0.9958946108818054</li>
<li>世界 0.9958213567733765</li>
<li>毒奶 0.9957934021949768</li>
<li>KPL 0.9957884550094604</li>
<li>上单 0.9956253170967102</li>
<li>瓜皮 0.9955945014953613</li>
<li>快 0.9953423738479614</li>
<li>打团 0.9953156113624573</li>
</ul>
<p><strong>真难受啊</strong></p>
<h1 id="To-Do"><a href="#To-Do" class="headerlink" title="To Do"></a>To Do</h1><ol>
<li>可以使用朴素贝叶斯做分类模型</li>
<li>使用机器学习性能评估指标预测精确率和准确率</li>
<li>欢迎补充</li>
</ol>
<h1 id="可参考资料"><a href="#可参考资料" class="headerlink" title="可参考资料"></a>可参考资料</h1><ol>
<li><a href="http://dsqiu.iteye.com/blog/1704960" target="_blank" rel="external">中文分词基本算法介绍</a></li>
<li><a href="https://my.oschina.net/letiantian/blog/352693" target="_blank" rel="external">ICTCLAS 汉语词性标注集</a></li>
<li><a href="http://www.blogjava.net/zhenandaci/category/31868.html" target="_blank" rel="external">文本分类技术</a></li>
<li><a href="http://blog.csdn.net/zhzhl202/article/details/8197109" target="_blank" rel="external">文本分类与SVM</a></li>
<li><a href="http://blog.csdn.net/tbkken/article/details/8062358" target="_blank" rel="external">基于贝叶斯算法的文本分类算法</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA3MDg0MjgxNQ==&amp;mid=2652389734&amp;idx=1&amp;sn=585d22c0b2aea755a072f5dfefca868b&amp;scene=23&amp;srcid=0530UUkS6jjRAsVoT2icemoY#rd" target="_blank" rel="external">基于libsvm的中文文本分类原型</a></li>
<li><a href="https://cosx.org/2013/03/lda-math-text-modeling#comments" target="_blank" rel="external">LDA-math-文本建模</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_8af1069601019flb.html" target="_blank" rel="external">情感分析资源</a></li>
<li><a href="https://wenku.baidu.com/view/37e374355727a5e9856a61bc.html" target="_blank" rel="external">面向情感分析的特征抽取技术研究</a></li>
<li><a href="http://52opencourse.com/235/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%B8%83%E8%AF%BE-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%EF%BC%88sentiment-analysis%EF%BC%89" target="_blank" rel="external">斯坦福大学自然语言处理第七课-情感分析</a></li>
<li><a href="http://blog.jobbole.com/77709/0" target="_blank" rel="external">深度学习、自然语言处理和表征方法</a></li>
<li><a href="http://licstar.net/archives/328" target="_blank" rel="external">Deep Learning in NLP （一）词向量和语言模型</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十一篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/xf7kEI8.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://zhangslob.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://zhangslob.github.io/tags/NLP/"/>
    
      <category term="自然语言处理" scheme="https://zhangslob.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>scrapy学习实例（四）采集淘宝数据并展示</title>
    <link href="https://zhangslob.github.io/2017/12/19/scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E5%9B%9B%EF%BC%89%E9%87%87%E9%9B%86%E6%B7%98%E5%AE%9D%E6%95%B0%E6%8D%AE%E5%B9%B6%E5%B1%95%E7%A4%BA/"/>
    <id>https://zhangslob.github.io/2017/12/19/scrapy学习实例（四）采集淘宝数据并展示/</id>
    <published>2017-12-19T13:09:07.000Z</published>
    <updated>2017-12-19T14:36:19.258Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十篇原创文章
</code></pre><p><img src="https://i.imgur.com/IzOzuZz.png" alt=""></p>
<a id="more"></a>
<p>本节代码 ： <a href="https://github.com/zhangslob/Taobao_duoshou" target="_blank" rel="external">zhangslob/Taobao_duoshou</a> <strong>万水千山总是情，给个star行不行</strong></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=28830411&auto=1&height=66"></iframe>

<p>铛铛铛，懒惰了一段时间，咱接着学Scrapy。这一期玩点新花样，使用<a href="http://flask.pocoo.org/" target="_blank" rel="external">Flask</a>展示我们的数据。效果如下图：</p>
<p><img src="https://i.imgur.com/IzOzuZz.png" alt=""></p>
<p><img src="https://i.imgur.com/aDqbnMU.png" alt=""></p>
<blockquote>
<p>有些重复的 ╮(╯Д╰)╭</p>
</blockquote>
<p>过程简单来说有两步：</p>
<ol>
<li>使用Scrapy采集淘宝数据</li>
<li>使用Flask展示数据</li>
</ol>
<h1 id="采集数据"><a href="#采集数据" class="headerlink" title="采集数据"></a>采集数据</h1><h2 id="抓包分析"><a href="#抓包分析" class="headerlink" title="抓包分析"></a>抓包分析</h2><p>其实采集淘宝数据的方法真的很多很多，这里不讨论Selenium，只说如何抓包分析，先提供几个接口，供观众老爷观赏：</p>
<ol>
<li><a href="https://s.taobao.com/list?seller_type=taobao&amp;json=on" target="_blank" rel="external">https://s.taobao.com/list?seller_type=taobao&amp;json=on</a></li>
<li><a href="https://s.m.taobao.com/search?event_submit_do_new_search_auction=1&amp;_input_charset=utf-8&amp;topSearch=1&amp;atype=b&amp;searchfrom=1&amp;action=home%3Aredirect_app_action&amp;from=1&amp;q=%E6%B0%B4%E6%9E%9C&amp;sst=1&amp;n=44&amp;buying=buyitnow&amp;m=api4h5&amp;abtest=25&amp;wlsort=25&amp;page=1" target="_blank" rel="external">https://s.m.taobao.com/search?event_submit_do_new_search_auction=1&amp;_input_charset=utf-8&amp;topSearch=1&amp;atype=b&amp;searchfrom=1&amp;action=home%3Aredirect_app_action&amp;from=1&amp;q=%E6%B0%B4%E6%9E%9C&amp;sst=1&amp;n=44&amp;buying=buyitnow&amp;m=api4h5&amp;abtest=25&amp;wlsort=25&amp;page=1</a></li>
<li><a href="http://h5.m.taobao.com/app/selectassistant/www/choiceness/index.html?m=select&amp;vm=nw&amp;ttid=null&amp;utd_id=null&amp;page=2&amp;n=44&amp;q=%E6%B0%B4%E6%9E%9C" target="_blank" rel="external">http://h5.m.taobao.com/app/selectassistant/www/choiceness/index.html?m=select&amp;vm=nw&amp;ttid=null&amp;utd_id=null&amp;page=2&amp;n=44&amp;q=%E6%B0%B4%E6%9E%9C</a></li>
</ol>
<p>这三种接口都可以采集数据，别问我怎么知道的，经历过千百次失败。<br>这里选择的是第二种，大家可以试试这几种的区别。</p>
<p>使用第二种去采集数据时，返回的是json数据，数据量已经很多。</p>
<p><img src="https://i.imgur.com/snWOQbo.png" alt=""></p>
<p>其中有<strong>几点坑</strong>，分享下。</p>
<ol>
<li>URL不同。淘宝和天猫的链接是不同的，移动端和网页端是不同的。</li>
<li>这里显示的 <code>commentCount</code> （评论数量）并不是真实的，你可以打开详情页对比</li>
<li>评论数量的接口。淘宝和天猫的都可以使用 <code>https://rate.taobao.com/detailCount.do?itemId=商品ID</code>，每件商品都有自己的唯一ID</li>
</ol>
<h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>这里直接把主代码贴出来，使用 <code>Mongo</code> 保存数据。 </p>
<p><code>taobao.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> copy</div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> TaobaoInfoItem</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">from</span> traceback <span class="keyword">import</span> format_exc</div><div class="line"></div><div class="line">kw = [<span class="string">'网络服务'</span>, <span class="string">'装潢'</span>, <span class="string">'护理'</span>, <span class="string">'速食'</span>, <span class="string">'运动鞋'</span>, <span class="string">'运动服'</span>, <span class="string">'男装'</span>, <span class="string">'配件'</span>, <span class="string">'蔬果'</span>, <span class="string">'干货'</span>, <span class="string">'攻略'</span>, <span class="string">'地毯'</span>, <span class="string">'文具'</span>, <span class="string">'书籍'</span>, <span class="string">'人偶'</span>, <span class="string">'饰品'</span>, <span class="string">'报纸'</span>, <span class="string">'时尚饰品'</span>, <span class="string">'美发'</span>, <span class="string">'运动包'</span>, <span class="string">'粮油'</span>, <span class="string">'吃喝玩乐折扣券'</span>, <span class="string">'工具'</span>, <span class="string">'彩妆'</span>, <span class="string">'演出'</span>, <span class="string">'童装'</span>, <span class="string">'个性定制'</span>, <span class="string">'数码相机'</span>, <span class="string">'日化'</span>, <span class="string">'游戏'</span>, <span class="string">'尿片'</span>, <span class="string">'安防'</span>, <span class="string">'摄像机'</span>, <span class="string">'厨房电器'</span>, <span class="string">'办公设备'</span>, <span class="string">'网店'</span>, <span class="string">'ZIPPO'</span>, <span class="string">'杂志'</span>, <span class="string">'礼品'</span>, <span class="string">'摄影器材'</span>, <span class="string">'喂哺等用品'</span>, <span class="string">'软件'</span>, <span class="string">'笔记本电脑'</span>, <span class="string">'明星'</span>, <span class="string">'登山'</span>, <span class="string">'居家日用'</span>, <span class="string">'户外'</span>, <span class="string">'电脑硬件'</span>, <span class="string">'流行首饰'</span>, <span class="string">'娃娃'</span>, <span class="string">'收纳'</span>, <span class="string">'影视'</span>, <span class="string">'音乐'</span>, <span class="string">'电玩'</span>, <span class="string">'音像'</span>, <span class="string">'香水'</span>, <span class="string">'水产'</span>, <span class="string">'热销女包'</span>, <span class="string">'大家电'</span>, <span class="string">'其他保健营养品'</span>, <span class="string">'箱包皮具'</span>, <span class="string">'瑞士军刀'</span>, <span class="string">'3C数码配件市场'</span>, <span class="string">'电脑周边'</span>, <span class="string">'男包'</span>, <span class="string">'玩具'</span>, <span class="string">'U盘'</span>, <span class="string">'模型'</span>, <span class="string">'孕妇装'</span>, <span class="string">'窗帘'</span>, <span class="string">'眼镜'</span>, <span class="string">'促销店铺'</span>, <span class="string">'五金'</span>, <span class="string">'旅行'</span>, <span class="string">'洗护'</span>, <span class="string">'清洁'</span>, <span class="string">'移动存储'</span>, <span class="string">'卫浴'</span>, <span class="string">'野营'</span>, <span class="string">'颈环配件'</span>, <span class="string">'童鞋'</span>, <span class="string">'家装饰品'</span>, <span class="string">'显示器'</span>, <span class="string">'闪存卡'</span>, <span class="string">'传统滋补品'</span>, <span class="string">'耗材'</span>, <span class="string">'灯具'</span>]</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TaobaoSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'taobao'</span></div><div class="line">    allowed_domains = [<span class="string">'taobao.com'</span>]</div><div class="line">    start_urls = [<span class="string">'https://s.m.taobao.com/search?event_submit_do_new_search_auction=1&amp;_input_charset=utf-8&amp;topSearch=1&amp;atype=b&amp;searchfrom=1&amp;action=home%3Aredirect_app_action&amp;from=1&amp;q=&#123;&#125;&amp;sst=1&amp;n=44&amp;buying=buyitnow&amp;m=api4h5&amp;abtest=25&amp;wlsort=25&amp;page=&#123;&#125;'</span>.format(i, y) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">101</span>) <span class="keyword">for</span> y <span class="keyword">in</span> kw]</div><div class="line"></div><div class="line">    <span class="comment"># 其他接口 http://h5.m.taobao.com/app/selectassistant/www/choiceness/index.html?m=select&amp;vm=nw&amp;ttid=null&amp;utd_id=null&amp;page=2&amp;n=44&amp;q=%E6%B0%B4%E6%9E%9C</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = TaobaoInfoItem()</div><div class="line">        ListItem = json.loads(response.text)[<span class="string">'listItem'</span>]</div><div class="line"></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> ListItem:</div><div class="line">            item[<span class="string">'name'</span>] = i[<span class="string">'name'</span>]</div><div class="line">            item[<span class="string">'title'</span>] = i[<span class="string">'title'</span>]</div><div class="line">            item[<span class="string">'area'</span>] = i[<span class="string">'area'</span>]</div><div class="line"></div><div class="line">            <span class="comment"># 处理不同的URL</span></div><div class="line">            url = []</div><div class="line">            <span class="keyword">if</span> <span class="string">'https:'</span> <span class="keyword">not</span> <span class="keyword">in</span> i[<span class="string">'url'</span>]:</div><div class="line">                <span class="keyword">if</span> <span class="string">'detail.m.tmall.com'</span> <span class="keyword">in</span> i[<span class="string">'url'</span>]:</div><div class="line">                    url.append(<span class="string">'https:'</span> + i[<span class="string">'url'</span>].replace(<span class="string">'.m'</span>,<span class="string">''</span>))</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    url.append(<span class="string">'https:'</span> + i[<span class="string">'url'</span>])</div><div class="line">            <span class="keyword">if</span> <span class="string">'https:'</span>  <span class="keyword">in</span> i[<span class="string">'url'</span>]:</div><div class="line">                url.append(i[<span class="string">'url'</span>])</div><div class="line">            item[<span class="string">'url'</span>] = url</div><div class="line">            print(item[<span class="string">'url'</span>])</div><div class="line"></div><div class="line">            <span class="comment"># 评论网址</span></div><div class="line">            comment_url = []</div><div class="line">            ur = item[<span class="string">'url'</span>]</div><div class="line">            comment_url.append(ur[<span class="number">0</span>] + <span class="string">'#J_Reviews'</span>)</div><div class="line">            item[<span class="string">'comment_url'</span>] = comment_url</div><div class="line"></div><div class="line">            item[<span class="string">'fastPostFee'</span>] = i[<span class="string">'fastPostFee'</span>]</div><div class="line">            item[<span class="string">'sales'</span>] = i[<span class="string">'act'</span>]</div><div class="line">            item[<span class="string">'price'</span>] = i[<span class="string">'price'</span>]</div><div class="line">            item[<span class="string">'originalPrice'</span>] = i[<span class="string">'originalPrice'</span>]</div><div class="line">            item[<span class="string">'nick'</span>] = i[<span class="string">'nick'</span>]</div><div class="line">            item[<span class="string">'id'</span>] = i[<span class="string">'item_id'</span>]</div><div class="line">            item[<span class="string">'loc'</span>] = i[<span class="string">'sellerLoc'</span>]</div><div class="line"></div><div class="line">            <span class="comment"># 图片链接</span></div><div class="line">            img_url = []</div><div class="line">            img_url.append(<span class="string">'http:'</span> + i[<span class="string">'img2'</span>])</div><div class="line">            item[<span class="string">'img_url'</span>] = img_url</div><div class="line"></div><div class="line">            count_url = []</div><div class="line">            count_url.append(<span class="string">'https://rate.taobao.com/detailCount.do?itemId='</span> + i[<span class="string">'item_id'</span>])</div><div class="line"></div><div class="line">            <span class="keyword">for</span> url <span class="keyword">in</span> count_url:</div><div class="line">                <span class="keyword">yield</span> scrapy.Request(</div><div class="line">                    url,</div><div class="line">                    callback=self.detail_parse,</div><div class="line">                    meta=&#123;<span class="string">'item'</span>: copy.deepcopy(item)&#125;, <span class="comment"># 使用copy.deepcopy深复制，否则数据不对啊</span></div><div class="line">                    dont_filter=<span class="keyword">True</span>,</div><div class="line">                    errback=self.error_back</div><div class="line">                )</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = response.meta[<span class="string">'item'</span>]</div><div class="line">        pat_count = <span class="string">'&#123;"count":(.*?)&#125;'</span></div><div class="line">        item[<span class="string">'count'</span>] = re.findall(pat_count, str(response.body))</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">error_back</span><span class="params">(self, e)</span>:</span></div><div class="line">        _ = e</div><div class="line">        self.logger.error(format_exc())</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">'''</span></div><div class="line">['羽绒服', '毛呢大衣', '毛衣', '冬季外套', '新品', '裤子', '连衣裙', '腔调', '秋冬新品', '淘特莱斯', '淘先生', '拾货', '秋冬外套', '时尚套装', '潮牌', '爸爸装', '春新品', '性感诱惑', '甜美清新', '简约优雅', '奢华高贵', '运动风', '塑身', '基础内衣', '轻薄款', '长款', '短款', '毛领', '加厚', '被子', '鹅绒', '新品', '秋款', '夹克', '卫衣', '西装', '风衣', '皮衣', '毛呢外套', '薄羽绒', '无钢圈', '无痕文胸', '蕾丝内衣', '运动文胸', '聚拢文胸', '大码文胸', '抹胸式', '隐形', '廓形', '双面呢', '羊绒', '中长款', '短款', '毛领', '设计师款', '系带', 'T恤', '长袖T', '打底衫', '纯色', '衬衫', '长袖款', '商务款', '时尚款', '睡衣套装', '睡裙', '睡袍浴袍', '外穿家居', '女士睡衣', '男士睡衣', '情侣睡衣', '亲子睡衣', '马海毛', '貂绒', '羊绒', '羊毛', '开衫', '中长款', '短款', '卡通', '休闲裤', '工装裤', '运动裤', '长裤', '牛仔裤', '小脚裤', '哈伦裤', '直筒裤', '女士内裤', '男士内裤', '三角裤', '平角裤', '丁字裤', '阿罗裤', '星期裤', '低腰', '外套', '套装', '风衣', '卫衣', '真皮皮衣', '马甲', '小西装', '唐装', '中老年', '薄毛衣', '针织开衫', '圆领毛衣', 'V领毛衣', '纯色毛衣', '民族风', '羊毛衫', '羊绒衫', '船袜', '男人袜', '连裤袜', '隐形袜', '收腹裤', '塑身衣', '美体裤', '收腹带', '帆布鞋', '高帮', '低帮', '内增高', '懒人鞋', '厚底', '韩版', '系带', '情侣款', '运动风鞋', '厚底', '内增高', '星星鞋', '系带', '上新', '人气款', '单肩包', '斜挎包', '手提包', '迷你包', '手拿包', '小方包', '棒球帽', '鸭舌帽', '遮阳帽', '渔夫帽', '草帽', '平顶帽', '嘻哈帽', '贝雷帽', '牛仔帽', '爵士帽', '高跟', '平底', '厚底', '中跟', '粗跟', '坡跟', '浅口', '尖头', '圆头', '运动款', '头层牛皮', '内增高', '松糕鞋', '豆豆鞋', '商务', '休闲', '潮范', '胸包', '腰包', '单肩', '斜跨', '手提', '手拿', '帆布', '牛皮', '女士腰带', '男士皮带', '帆布腰带', '腰封', '腰链', '针扣头', '平滑扣', '自动扣', '真皮', '正品', '厚底', '内增高', '星星鞋', '系带', '一脚蹬', '魔术贴', '气垫', '网状', '印花', '铆钉', '水洗皮', '卡通', '原宿', '糖果色', '商务', '运动', '帆布', '牛皮', '女士围巾', '男士围巾', '披肩', '丝巾', '假领', '小方巾', '三角巾', '大方巾', '真丝', '雪纺', '棉质', '亚麻', '蕾丝', '青春潮流', '商务皮鞋', '休闲皮鞋', '正装皮鞋', '商务休闲', '布洛克', '内增高', '反绒皮', '真皮', '潮流低帮', '韩版', '英伦', '复古', '铆钉', '编织', '豹纹', '大头', '拉杆箱', '密码箱', '学生箱', '子母箱', '拉杆包', '万向轮', '飞机轮', '航空箱', '铝框', '女士手套', '男士手套', '真皮手套', '蕾丝手套', '防晒手套', '半指手套', '分指手套', '连指手套', '短款手套', '长款手套', '皮鞋', '低帮', '反绒皮', '大头鞋', '豆豆鞋', '帆船鞋', '懒人鞋', '帆布/板鞋', '高帮', '凉鞋/拖鞋', '沙滩鞋', '人字拖', '皮凉鞋', '洞洞鞋', '钱包', '潮包馆', '真皮包', '手机包', '大牌', 'coach', 'MK', 'MCM', '毛线', '鞋垫', '鞋带', '领带', '领结', '袖扣', '手帕', '布面料', '耳套', '领带夹', '婚纱配件', '皮带扣', '英国牛栏', '英国爱他美', '美赞臣', '雅培', '澳洲爱他美', '可瑞康', '惠氏', '贝因美', '推车', '驱蚊器', '婴儿床', '理发器', '奶瓶', '餐椅', '背带腰凳', '安全座椅', '内衣', '内裤', '喂奶枕', '收腹带', '妈咪包', '待产包', '防辐射服', '储奶袋', '米粉', '肉松', '磨牙棒', '果泥', '益生菌', '清火开胃', '钙铁锌', '维生素', '花王', 'moony', '大王', '帮宝适', '雀氏', '好奇', '妈咪宝贝', '安儿乐', '海淘奶粉', '海淘辅食', '海淘营养品', '直邮花王', '海淘洗护', '海淘奶瓶', '海淘餐具', '海淘孕产', 'T恤', '连衣裙', '泳装', '套装', '衬衫', '防晒服', '半身裙', '短裤', '凉鞋', '沙滩鞋', '洞洞鞋', '网鞋', '学步鞋', '拖鞋', '帆布鞋', '宝宝鞋', '母女裙', '父子装', '亲子T恤', '亲子衬衫', '亲子套装', '母女鞋', '父子鞋', '家庭鞋', '沙滩戏水', '早教启蒙', '拼插益智', '遥控模型', '运动户外', '学习爱好', '卡通公仔', '亲子互动', '电动车', '自行车', '学步车', '手推车', '三轮车', '滑板车', '扭扭车', '儿童轮滑', '早教机', '点读机', '健身架', '布书', '串/绕珠', '床/摇铃', '爬行垫', '木质拼图', '卸妆', '面膜', '洁面', '防晒', '面霜', '爽肤水', '眼霜', '乳液', '补水', '美白', '收缩毛孔', '控油', '祛痘', '祛斑', '去黑眼圈', '去黑头', 'BB霜', '粉底液', '唇膏', '隔离', '遮瑕', '指甲油', '粉饼', '彩妆套装', '女士香水', '男士香水', '中性香水', '淡香水', '古龙水', '香精', '复方精油', '香体乳', '洗发水', '护发素', '染发', '烫发', '造型', '假发', '洗护套装', '假发配件', '美胸', '纤体', '胸部护理', '身体护理', '塑身', '脱毛', '手部保养', '足部护理', '眼线', '睫毛膏', '眼影', '眉笔', '假睫毛', '眼霜', '双眼皮贴', '眼部护理', '劲能醒肤', '清洁面膜', '男性主义', '剃须膏', '男士套装', '男士防晒', '火山岩', '爽身走珠', '抗皱', '抗敏感', '保湿', '去眼袋', '滋润', '抗氧化', '深层清洁', '雅诗兰黛', '兰蔻', '资生堂', '自然乐园', 'SK-II', '悦诗风吟', '水宝宝', '契尔氏', '芦荟胶', '彩妆盘', '腮红', '香氛', '高光棒', '修容', 'V脸', '去角质', '洁面', '爽肤水', '精华', '乳液', '鼻贴', '马油', '牛肉干', '鲜花饼', '红枣', '糖果', '巧克力', '山核桃', '松子', '卤味', '饼干', '话梅', '蔓越莓', '薯片', '奇异果', '芒果', '樱桃', '橙子', '秋葵', '苹果', '番茄', '柠檬', '椰子', '榴莲', '大米', '橄榄油', '小米', '黄豆', '赤豆', '火腿', '香肠', '木耳', '香菇', '豆瓣酱', '海参', '龙虾', '瑶柱', '土鸡', '牛排', '三文鱼', '咸鸭蛋', '皮蛋', '五花肉', '北极贝', '鸡尾酒', '红酒', '啤酒', '白酒', '梅酒', '洋酒', '清酒', '滋补酒', '茅台', '五粮液', '麦片', '咖啡', '牛奶', '柚子茶', '酸梅汤', '矿泉水', '酵素', '藕粉', '姜茶', '酸奶粉', '铁观音', '红茶', '花草茶', '龙井', '普洱', '黑茶', '碧螺春', '毛峰', '袋泡茶', '白茶', '枸杞', '人参', '石斛', '燕窝', '雪蛤', '蜂蜜', '天麻', '花粉', '党参', '红花', '芒果干', '鱼子酱', '咖啡', '橄榄油', '薯片', '巧克力', '咖喱', '方便面', '红酒', '麦片', '项链', '手链', '戒指', '发饰', '银饰', '水晶', '耳饰', '手镯', '翡翠', '彩宝', '蜜蜡', '裸钻', '珍珠', '黄金', '钻石', '金条', '和田玉', '翡翠', '水晶/佛珠', '黄金', '手表', '眼镜', '瑞士表', '机械表', '时装表', '儿童表', '电子表', '情侣表', '石英表', '手表配件', '太阳镜', '偏光镜', '近视镜', '司机镜', '护目镜', '眼镜配件', '运动镜', '老花镜', 'zippo', '电子烟', '烟斗', '瑞士军刀', '绝美酒具', '风格男表', '佛珠', '水晶', '碧玺', '925银', '施华洛', '翡翠', '珍珠', '黄金', '银项链', '流行风格', '天然水晶', '锆石水晶', '佛珠项链', '人造水晶', '925银', '翡翠', '和田玉', '复古泰银', '粉晶手镯', '黄金手镯', '日韩', '甜美', '复古/宫廷', '欧美', '瑞丽', '波西米亚', '民族风', '发饰', '项链', '套装', '耳饰', '韩式', '头饰', '三件套', '合金配件', '银饰', '水晶配珠', '琉璃', '珍珠母贝', '有机玻璃', '人造水晶', '设计师', '半包装修', '全包装修', '全案装修', '装修监理', '清包施工', '局部装修', '验房量房', '装修空气质量检测', '装修污染治理', '整体橱柜', '定制衣柜', '定制吊顶', '定制淋浴房', '门', '窗', '定制柜', '楼梯', '榻榻米定制', '地暖', '吸顶灯', '吊灯', '吸吊两用灯', '筒灯', '射灯', '台灯', '落地灯', '室外灯', '壁灯', '小夜灯', '浴室柜', '普通马桶', '花洒套装', '一体智能马桶', '智能马桶盖板', '淋浴房', '面盆龙头', '地漏', '五金挂件', '浴霸', 'PVC墙纸', '无纺布墙纸', '纯纸墙纸', '墙布', '沙粒墙纸', '绒面墙纸', '定制壁画', '3D墙纸', '实木地板', '实木复合地板', '强化复合地板', '竹地板', '户外地板', 'PVC地板', '防静电地板', '防潮膜', '踢脚线', '地板龙骨', '仿古砖', '釉面砖', '玻化砖', '微晶石', '马赛克', '抛晶砖', '通体砖', '花片', '腰线', '瓷砖背景墙', '插座', '开关', '电线', '监控器材', '智能家居', '防盗报警器材', '消防报警设备', '接线板插头', '布线箱', '断路器', '涂料乳胶漆', '油漆', '水管', '板材', '木方', '阳光房', '线条', '天然大理石', '人造大理石', '防水涂料', '实木床', '布艺床', '皮艺床', '床垫', '衣柜', '斗柜', '梳妆台', '子母床', '床头柜', '儿童床', '皮艺沙发', '布艺沙发', '沙发床', '实木沙发', '懒人沙发', '电视柜', '茶几', '鞋柜', '玄关厅', '衣帽架', '餐桌', '折叠餐桌', '欧式餐桌', '实木餐桌', '大理石餐桌', '餐椅', '餐边柜', '换鞋凳', '角柜', '屏风', '餐桌', '折叠餐桌', '欧式餐桌', '实木餐桌', '大理石餐桌', '餐椅', '餐边柜', '换鞋凳', '角柜', '屏风', '蚊帐', '三开蚊帐', '凉席', '凉席套件', '冰丝席', '藤席', '牛皮席', '夏凉被', '空调被', '天丝套件', '床单', '床笠', '四件套', '全棉套件', '被套', '蚕丝被', '羽绒被', '枕头', '乳胶枕', '记忆枕', '床褥', '毛毯', '定制窗帘', '地毯', '沙发垫', '靠垫', '桌布桌旗', '飘窗垫', '地垫', '餐垫', '防尘罩', '椅垫', '成品窗帘', '沙发罩', '摆件', '花瓶', '仿真花', '台钟闹钟', '香薰炉', '储物罐', '装饰碗盘', '木雕', '烟灰缸', '纸巾盒', '蜡烛烛台', '仿真饰品', '现代装饰画', '无框画', '后现代画', '油画', '挂钟', '照片墙', '新中式', '北欧家饰', '美式乡村', '挂钩搁板', '装饰挂钩', '壁饰', '扇子', '毛巾', '浴巾', '口罩', '隔音耳塞', '竹炭包', '眼罩', '夏季凉拖', '居家鞋', '夏季清凉', '湿巾', '晴雨伞', '驱蚊灯', '驱蚊液', '冰格', '保鲜产品', '密封罐', '防潮制品', '电扇/冰垫', '5元小物', '被子防尘袋', '收纳盒', '收纳袋', '大衣/西服罩', '护洗袋', '收纳凳', '鞋柜', '置物架', '桌用收纳', '内衣收纳', '洗发护发', '沐浴露', '漱口水', '卫生巾', '洗手液', '牙膏', '纸巾', '香皂', '沐浴球/浴擦/浴刷', '指甲刀', '剃须刮毛刀', '沐浴球', '浴室角架', '浴帘杆', '拖把', '垃圾桶', '梳子镜子', '围裙', '百洁布', '海绵擦', '餐具', '锅具', '刀具', '炖锅', '蒸锅', '汤锅', '煎锅', '压力锅', '炒锅', '菜板砧板', '一次性餐桌用品', '酒杯酒具', '咖啡器具', '碗盘碟', '刀叉勺', '餐具瓷器套装', '餐桌小物', '饭盒', '厨房储物', '一次性餐桌用品', '茶具', '茶壶', '飘逸杯', '功夫茶杯', '玻璃杯', '杯垫', '保温杯', '马克杯', '保温壶', '情侣杯', '晒衣篮', '晾衣杆', '脏衣篮', '衣架', '家庭清洁剂', '蓝泡泡', '管道疏通器', '塑胶手套', '医药箱', '垃圾袋', '汽车首页', '新车先购', '车海淘', '二手车', '爱车估价', 'suv', '别克', '大众', '宝马', '座垫', '座套', '脚垫', '香水', '旅行床', '遮阳挡', '挂件摆件', '安全座椅', '专车专用座垫', '脚垫', '安全座椅', '香水', '钥匙包', '挂件', '座套', '后备箱垫', '置物箱', '智能车机', '后视镜', '安卓导航', '便携GPS', 'DVD导航', '电子狗', '流动测速', '导航软件', '记录仪', '预警仪', 'GPS', '车机', '倒车雷达', '智能后视镜', '蓝牙', '防盗器', 'MP3', '4S保养', '电瓶安装', '配件安装', '隔热膜', '洗车卡', '镀晶镀膜', '连锁保养', '上门服务', '行车记录仪', '逆变器', '跟踪器', '充电器', '充气泵', '胎压监测', '车载冰箱', '空气净化', '车衣', 'SUV踏板', '晴雨挡', '改色膜', '汽车车标', '车牌架', '轮胎', '雨刮器', '机油滤芯', '空气滤芯', '空调滤芯', '减震', '刹车片', '火花塞', '轮胎', '雨刮', '机油', '高亮大灯', '挡泥板', '保险杠', '车顶架', '轮眉', '轮毂', '排气', '保险杠', '汽车包围', '氙气灯', '车顶架', '脚踏板', '大灯总成', '尾翼', '轮毂', '汽车装饰灯', '排气筒', '尾喉', '车身饰条', '添加剂', '防冻液', '玻璃水', '车蜡', '补漆笔', '洗车机', '洗车水枪', '车掸蜡拖', '车蜡', '洗车机', '补漆笔', '抛光机', '打蜡海绵', '车用水桶', '擦车巾', '车刷', '装饰条', '车贴', '尾喉', '改色膜', '防爆膜', '晴雨挡', '日行灯', '车衣', '夏季座垫', '遮阳挡', '防眩蓝镜', '防晒手套', 'iPhone', '小米', '华为', '三星', '魅族', '纽扣', '酷派', 'VIVO', 'iPad', '小米', '三星', '10寸', '台电', 'win8', '蓝魔', '华为', 'DIY电脑', '一体机', '路由器', '显示器', '学生', 'CPU', '移动硬盘', '无线鼠标', '苹果', '联想', 'Thinkpad', '戴尔', '华硕', 'Acer', '神州', '三星', '单反', '自拍神器', '拍立得', '佳能', '微单', '镜头', '卡西欧', '尼康', '充电宝', '智能穿戴', '蓝牙耳机', 'iPhone6壳', '电脑包', '手机贴膜', '手机壳套', '三脚架', '保护壳套', '炫彩贴膜', '移动电源', '相机配件', '手机零件', '自拍神器', '移动POS支付', '电池', '儿童手表', 'Apple Watch', '智能手表', '智能手环', '智能配饰', '智能健康', '智能排插', '智能眼镜', '游戏掌机', '家用游戏机', '游戏手柄', 'PS主机', 'XBOX', '任天堂配件', 'PS主机配件', 'XBOX配件', '路由器', '网关', '交换机', '光纤设备', '网络存储设备', '无线上网卡', 'TP-LINK', '小米路由器', 'MP3', 'MP4', '录音笔', '索尼', '飞利浦', 'ipod', '爱国者', '耳机', 'U盘', '闪存卡', '记忆棒', '移动硬盘', '希捷', '三星', 'Sandisk', '金士顿', '电磁炉', '电水壶', '料理机', '电饭煲', '榨汁机', '净水器', '豆浆机', '烤箱', '电风扇', '空调扇', '挂烫机', '扫地机', '吸尘器', '加湿器', '除湿机', '对讲机', '空气净化', '理发器', '电子称', '美容仪', '按摩椅', '按摩披肩', '血压计', '足浴器', '电动牙刷', '剃须刀', '耳机', '音响', '网络机顶盒', '麦克风', '扩音器', 'HiFi套装', '蓝光DVD', '低音炮', '打印机', '投影仪', '硒鼓墨盒', 'A4纸', '一体机', '学生文具', '保险柜', '电纸书', '学习机', '冰箱', '空调', '平板电视', '油烟机', '燃气灶', '消毒柜', '厨电套装', '热水器', '洗衣机', '包装设备', '包装纸箱', '塑料袋', '包装胶带', '铭牌', '快递袋', '气泡膜', '真空机', '笔记本', '文件袋', '钢笔', '胶粘用品', '铅笔', '计算器', '白板', '台历', '设计定制', '企业用品定制', 'T恤印制', '杯子定制', 'ppt模板', '班服定制', '洗照片', '人偶定制', '电子电工', '气动元件', '水泵', '阀门', '电钻', '焊接设备', '万用表', '雕刻机', '办公家具', '商业设施', '办公桌', '陈列柜', '货架', '广告牌', '文件柜', '沙发', '网络设备', '电子元器件', '路由器', '交换机', '光纤设备', '视频会议', '无线安全保密', '机柜', '餐饮美食', '冰淇淋', '火锅', '购物卡券', '体检配镜', '美容美甲', '保险理财', '婚纱摄影', '旅行团购', '住在帝都', '住在魔都', '住在杭州', '住在南京', '住在广州', '住在青岛', '住在宁波', '住在成都', '少儿英语', '小学教育', '潜能开发', '家长训练', '孕产育儿', '少儿绘画', '婴幼早教', '音乐', 'Q币充值', '点卡充值', '充游戏币', '游戏代练', '超值账号', '手游充值', '电竞比赛', '游戏帮派', '潇洒一室', '靠谱二室', '舒适三房', '大四室', '私藏别墅', '景观居所', '轨道沿线', '学区房', '实用英语', '网站制作', 'IT技能', '会计职称', '一对一', '办公软件', '日语', '编程', '英雄联盟', '剑侠情缘3', '征途2', '魔域', '我叫MT', '刀塔传奇', 'DOTA2', 'DNF', '魔兽世界', '自助餐', '个性写真', '儿童写真', '电影票团购', '上门服务', '周边旅游', '境外旅游', '基金理财', '魅力健身', '时尚美妆', '手工DIY', '舞蹈', '减肥瑜伽', '个人形象', '美剧英语', '摄影', '美女陪练', '轻松甩肉', '基金理财', '淘宝美工', '办公技能', '婚纱摄影', '婚礼策划', '三亚婚拍', '厦门婚拍', '青岛婚拍', '北京婚拍', '杭州婚拍', '上海婚拍', '新娘跟妆', '婚礼跟拍', '婚礼司仪', '婚车租赁', '任意洗', '洗外套', '洗西装', '洗鞋', '洗四件套', '洗烫衬衫', '皮包护理', '洗窗帘', '洗地毯', '在线洗衣', '洗礼服', '洗玩具', '开荒保洁', '厨房保洁', '公司保洁', '家电清洗', '空调清洗', '洗油烟机', '冰箱清洗', '擦玻璃', '家政服务', '家庭保洁', '保洁服务', '钟点工', '洗衣机清洗', '卫生间保洁', '上门养车', '洗车', '封釉镀膜', '内饰清洗', '空调清洗', '汽车维修', '充加油卡', '年检代办', '玻璃贴膜', '汽车装饰', '底盘装甲', '四轮定位', '汽车改装', '违章代办', '汽车隔音', '上门按摩', '常规体检', '入职体检', '老人体检', '四维彩超', '孕前检查', '体检报告', '专业洗牙', '烤瓷牙', '胃部检测', '月嫂', '催乳师', '育儿嫂', '营养师', '普通保姆', '涉外保姆', '产后陪护', '临时看护', '管家', '烧饭阿姨', '宠物寄养', '宠物美容', '宠物配种', '宠物洗澡', '宠物摄影', '宠物托运', '宠物训练', '宠物医疗', '水族服务', '宠物绝育', '宠物洗牙', '宠物造型', '宠物体检', '居家搬家', '公司搬运', '空调拆装', '家电搬运', '家具搬运', '打孔', '电路维修', '甲醛测试', '开锁换锁', '杀虫消毒', '高空清洁', '除尘除螨', '网上办事', '代缴费', '代排队', '交罚单', '叫醒服务', '宝宝起名', '学车报名', '代邮代取', '话费充值', '代送鲜花', '水电煤缴费', '同城速递', '代办档案', '宽带费', '机场停车', '专利申请', '法律咨询', '专业翻译', '开发建站', '图片处理', '视频制作', '名片制作', '商标转让', '打印', '复印', '商标注册', '私人律师', '合同文书', '出国翻译', '手机维修', 'pad维修', '修台式机', '相机维修', '修笔记本', '修复印机', '修游戏机', '修导航仪', '软件服务', '延保服务', '硬件维修', '苹果维修', '小米维修', '三星维修', '安卓刷机', '数据恢复', '电脑维修', 'ipad维修', '华为维修', '重装系统', '家电维修', '相机维修', '硬盘维修', '苹果换屏', '换主板', '名企招聘', '高薪岗位', '文案编辑', '网店推广', '开发技术', '活动策划', '美工设计', '金牌客服', '大促客服', '网页设计', '人才认证', '图片设计', '摄影师', '店长', '运营主管', '客服主管', '美工主管', '跑步鞋', '篮球鞋', '休闲鞋', '足球鞋', '帆布鞋', '训练鞋', '徒步鞋', '登山鞋', '限量版', '板鞋', 'Rosherun', '运动套装', '运动卫衣', '长裤', '皮肤风衣', '健身服', '球服', '耐克', '阿迪达斯', '三叶草', '美津浓', '彪马', '狼爪', '山地车', '公路车', '骑行服', '头盔', '装备', '零件', '工具', '护具', '折叠车', '死飞', '水壶架', '行李架', '羽毛球拍', '羽毛球服', '羽毛球', '网球拍', '篮球', '篮球服', '足球', '足球服', '乒乓球拍', '橄榄球', '台球', '高尔夫', '吊床', '头灯', '遮阳棚', '望远镜', '照明', '野营帐篷', '野外照明', '烧烤炉', '望远镜', '潜水镜', '防潮垫', '皮划艇', '皮肤衣', '防晒衣', '冲锋衣', '探路者', '速干裤', '迷彩服', '战术靴', '登山鞋', 'crocs', '溯溪鞋', '户外鞋', '麻将机', '轮滑', '麻将', '象棋', '雀友', '飞镖', '桌上足球', '风筝', '陀螺', '空竹', '沙袋', '太极服', '甩脂机', '轮滑装备', '跑步机', '舞蹈', '瑜伽', '哑铃', '仰卧板', '踏步机', '划船机', '卧推器', '健身车', '呼啦圈', '舞蹈', '瑜伽', '广场舞', '舞蹈鞋', '拉丁鞋', '广场舞套装', '肚皮舞服装', '瑜伽垫', '瑜伽球', '瑜伽服', '鱼饵', '套装', '路亚', '附件', '鱼钩', '钓鱼工具', '船/艇', '台钓竿', '海钓竿', '溪流竿', '路亚竿', '矶钓杆', '单肩背包', '旅行包', '双肩背包', '挎包', '户外摄影包', '头巾', '运动水壶', '防水包', '电池', '电自行车', '平衡车', '滑板车', '头盔', '摩托车', '老年代步', '独轮车', '遮阳伞', '扭扭车', '折叠车', '仿真植物', '干花', 'DIY花', '手捧花', '鲜果蓝', '仿真蔬果', '开业花篮', '花瓶', '绿植同城', '园艺方案', '多肉植物', '桌面盆栽', '蔬菜种子', '水培花卉', '苔藓景观', '空气凤梨', '肥料', '花盆花器', '花卉药剂', '营养土', '园艺工具', '洒水壶', '花架', '铺面石', '热带鱼', '孔雀鱼', '底栖鱼', '虾螺', '龙鱼', '罗汉鱼', '锦鲤', '金鱼', '水母', '灯科鱼', '乌龟', '水草', '底砂', '水草泥', '沉木', '仿真水草', '假山', '氧气泵', '过滤器', '水草灯', '加热棒', '鱼粮', '水质维护', '硝化细菌', '除藻剂', '龟粮', '兔兔', '仓鼠', '龙猫', '雪貂', '粮食零食', '医疗保健', '笼子', '鹦鹉', '鸟笼', '观赏鸟', '蚂蚁工坊', '蜘蛛', '蚕', '大牌狗粮', '宠物服饰', '狗厕所', '宠物窝', '航空箱', '海藻粉', '羊奶粉', '宠物笼', '储粮桶', '剃毛器', '营养膏', '上门服务', '全新钢琴', '智能钢琴', '中古钢琴', '尤克里里', '民谣吉他', '萨克斯风', '口琴', '小提琴', '高达', '手办', '盒蛋', '兵人', '变形金刚', '圣衣神话', '钢铁侠', 'BJD', '拼装', '人偶', '猫砂', '猫粮', '猫爬架', '猫窝', '猫砂盆', '化毛膏', '猫罐头', '喂食器', '折耳猫', '猫抓板', '猫玩具', '猫笼', '古筝', '二胡', '葫芦丝', '战鼓', '古琴', '陶笛', '琵琶', '笛子', '动漫T恤', '动漫抱枕', 'COS', '背包', '项链', '颜文字', '哆啦A梦', '大白', '手表', '盗墓笔记', '海贼', '火影', 'LOL', '杀菌剂', '杀虫剂', '除草剂', '调节剂', '杀螨剂', '杀鼠剂', '敌敌畏', '草甘膦', '园林种苗', '动物种苗', '蔬菜种苗', '水果种苗', '粮油种子', '药材种苗', '食用菌种', '辣木籽', '氮肥', '磷肥', '钾肥', '叶面肥', '新型肥料', '复合肥', '生物肥料', '有机肥', '耕种机械', '收割机械', '农机配件', '植保机械', '拖拉机', '施肥机械', '粮油设备', '微耕机', '塑料薄膜', '大棚膜', '防渗膜', '鱼塘专用', '薄膜', '遮阳网', '篷布', '防虫网', '镰刀', '锹', '高压水枪', '锨', '镐', '耙子', '锄头', '叉', '猪饲料', '羊饲料', '牛饲料', '预混料', '饲料原料', '全价料', '饲料添加剂', '浓缩料', '加工设备', '养殖器械', '渔业用具', '养殖服务', '配种服务', '养鸡设备', '挤奶机', '母猪产床', '化学药', '中兽药', '抗生素', '驱虫', '消毒剂', '疫苗', '阿莫西林', '氟苯尼考']</div><div class="line"></div><div class="line">'''</div></pre></td></tr></table></figure>
<p>说明几点：</p>
<ol>
<li>在使用 <code>meta</code> 传递数据的时候，要使用 <code>copy.deepcopy</code> 深复制，详情查阅 <a href="http://blog.csdn.net/bestbzw/article/details/52894883" target="_blank" rel="external">【scrapy】item传递出错</a></li>
<li>关于搜索词。因为淘宝对搜索结果只会返回100页，所以我们这里增加索引词，来获得更多数据。可以在代码末尾发现更多关键词，有1000多个，这个list是从这里获得 <a href="https://www.taobao.com/markets/tbhome/market-list" target="_blank" rel="external">淘宝首页行业市场</a></li>
</ol>
<h2 id="反爬处理"><a href="#反爬处理" class="headerlink" title="反爬处理"></a>反爬处理</h2><p>呃呃，有意思的是，自己跑了一会获得了几万条数据，并没有任何的异常，没有在 <code>middlewares.py</code> 加代理，连 <code>UA</code> 都是固定的，好奇怪。</p>
<p><strong>可能长得帅的人品都比较好吧。</strong></p>
<h1 id="展示数据"><a href="#展示数据" class="headerlink" title="展示数据"></a>展示数据</h1><p>mongo 中的数据</p>
<p><img src="https://i.imgur.com/UBmhKoG.png" alt=""> </p>
<p>数据并不直观，所以我们选择展示出来，做一个小小的聚合搜索</p>
<p>同样看看主文件 <code>server.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></div><div class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, session, g, redirect, url_for, \</div><div class="line">    abort, render_template, flash</div><div class="line"><span class="keyword">from</span> bson <span class="keyword">import</span> json_util</div><div class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line">conn = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</div><div class="line">db = conn[<span class="string">'taobao'</span>]</div><div class="line">goods_coll = db[<span class="string">'search'</span>]</div><div class="line">cate_coll = db[<span class="string">'categories'</span>]</div><div class="line">app = Flask(__name__)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toJson</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="keyword">return</span> json.dumps(</div><div class="line">               data,</div><div class="line">               default=json_util.default,</div><div class="line">               ensure_ascii=<span class="keyword">False</span></div><div class="line">           )</div><div class="line"></div><div class="line"><span class="meta">@app.errorhandler(404)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">page_not_found</span><span class="params">(e)</span>:</span></div><div class="line">    <span class="keyword">return</span> render_template(<span class="string">'404.html'</span>), <span class="number">404</span></div><div class="line"></div><div class="line"><span class="meta">@app.route('/', methods=['GET'])</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">if</span> request.method == <span class="string">'GET'</span>:</div><div class="line">        total = goods_coll.count()</div><div class="line">        <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>, total=total)</div><div class="line">    <span class="comment">#if request.form['key']:</span></div><div class="line">    <span class="comment">#    key = request.form['key']</span></div><div class="line">    <span class="comment">#    return redirect(url_for('get_goods', key=key, page=1))</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="meta">@app.route('/search', methods=['GET'])</span></div><div class="line"><span class="meta">@app.route('/search/&lt;item&gt;', methods=['GET'])</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_goods</span><span class="params">(item=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> request.method == <span class="string">'GET'</span>:</div><div class="line">        page = request.args.get(<span class="string">'page'</span>, <span class="number">1</span>, type=int)</div><div class="line">        limit = request.args.get(<span class="string">'limit'</span>, <span class="number">30</span>, type=int)</div><div class="line">        p = (page - <span class="number">1</span>) * limit</div><div class="line">        offset = request.args.get(<span class="string">'offset'</span>, p, type=int)</div><div class="line">        catid = request.args.get(<span class="string">'catid'</span>, <span class="keyword">None</span>, type=str)</div><div class="line">        jsons = request.args.get(<span class="string">'json'</span>, <span class="string">'off'</span>)</div><div class="line">        keyword = request.args.get(<span class="string">'key'</span>, <span class="string">''</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> keyword:</div><div class="line">            keyword = item</div><div class="line"></div><div class="line">        <span class="keyword">if</span> catid:</div><div class="line">            cursor = goods_coll.find(&#123;<span class="string">'categories.catid'</span>: catid&#125;)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            cursor = goods_coll.find(&#123;<span class="string">'title'</span>: &#123;<span class="string">'$regex'</span>: keyword&#125; &#125;)</div><div class="line">        <span class="comment">#total = cursor.count()</span></div><div class="line">        <span class="comment">#flash('已查询到 %d 个结果.'%total)</span></div><div class="line">        results = cursor.skip(offset).limit(limit)</div><div class="line">        resultList = []</div><div class="line">        <span class="keyword">for</span> result <span class="keyword">in</span> results:</div><div class="line">            resultList.append(result)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> jsons == <span class="string">'off'</span>:</div><div class="line">            <span class="keyword">return</span> render_template(<span class="string">'search.html'</span>, entries=resultList)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> toJson(resultList)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="comment">#app.run(host='0.0.0.0')</span></div><div class="line">    app.run(debug=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>这里是借鉴了别人的项目，地址在这 <a href="https://github.com/1dot75cm/taobaobao" target="_blank" rel="external">1dot75cm/taobaobao</a></p>
<p>您可以自定义在 <code>\templates</code>目录下修改对应的html文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">λ ls</div><div class="line">404.html  categories.html  index.html  layout.html  search.html</div></pre></td></tr></table></figure>
<p>直接运行  <code>server.py</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">λ piython server.py</div><div class="line"> * Restarting with <span class="built_in">stat</span></div><div class="line"> * Debugger is active!</div><div class="line"> * Debugger PIN: 503-651-032</div><div class="line"> * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)</div></pre></td></tr></table></figure>
<p>打开 <a href="http://127.0.0.1:5000/" target="_blank" rel="external">http://127.0.0.1:5000/</a> 即可查看结果</p>
<h1 id="To-Do"><a href="#To-Do" class="headerlink" title="To Do"></a>To Do</h1><ul>
<li>数据去重</li>
<li>价格及销量可视化展示</li>
<li>速度太慢，应使用分布式</li>
<li>评论采集</li>
<li>加强爬虫的反爬措施</li>
</ul>
<p>Github ： <a href="https://github.com/zhangslob/Taobao_duoshou" target="_blank" rel="external">zhangslob/Taobao_duoshou</a> <strong>万水千山总是情，给个star行不行</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://github.com/1dot75cm/taobaobao" target="_blank" rel="external">1dot75cm/taobaobao</a></li>
<li><a href="http://blog.csdn.net/bestbzw/article/details/52894883" target="_blank" rel="external">【scrapy】item传递出错</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/IzOzuZz.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
      <category term="Flask" scheme="https://zhangslob.github.io/tags/Flask/"/>
    
      <category term="淘宝" scheme="https://zhangslob.github.io/tags/%E6%B7%98%E5%AE%9D/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy学习实例（三）采集批量网页</title>
    <link href="https://zhangslob.github.io/2017/12/12/Scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E4%B8%89%EF%BC%89%E9%87%87%E9%9B%86%E6%89%B9%E9%87%8F%E7%BD%91%E9%A1%B5/"/>
    <id>https://zhangslob.github.io/2017/12/12/Scrapy学习实例（三）采集批量网页/</id>
    <published>2017-12-12T13:11:17.000Z</published>
    <updated>2017-12-19T14:39:05.546Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十九篇原创文章
</code></pre><p><img src="https://i.imgur.com/ALWvu6F.png" alt=""></p>
<a id="more"></a>
<p>先来首火影压压惊  (｡・`ω´･)</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=784594&auto=1&height=66"></iframe>

<p>最开始接触 <code>Rules</code>是在Scrapy的文档上看到的，但是并看读懂这是什么意思。接下来看别人的案例，有使用到Rules，便花了很多时间去了解。</p>
<blockquote>
<p>解释：<br>Rule是在定义抽取链接的规则，上面的两条规则分别对应列表页的各个分页页面和详情页，关键点在于通过<code>restrict_xpath</code>来限定只从页面特定的部分来抽取接下来将要爬取的链接。 </p>
</blockquote>
<p>其实用我的话来说就是，一个是可以便捷的进行翻页操作，二是可以采集二级页面，相当于打开获得详情页内容。所以若使用了 <code>Rules</code>，可以便捷的帮助我们采集批量网页。</p>
<h1 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h1><p><a href="http://python.usyiyi.cn/documents/scrapy_12/topics/spiders.html#crawlspider-example" target="_blank" rel="external">CrawlSpider示例</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">'example.com'</span></div><div class="line">    allowed_domains = [<span class="string">'example.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</div><div class="line"></div><div class="line">    rules = (</div><div class="line">        <span class="comment"># Extract links matching 'category.php' (but not matching 'subsection.php')</span></div><div class="line">        <span class="comment"># and follow links from them (since no callback means follow=True by default).</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</div><div class="line"></div><div class="line">        <span class="comment"># Extract links matching 'item.php' and parse them with the spider's method parse_item</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></div><div class="line">        self.logger.info(<span class="string">'Hi, this is an item page! %s'</span>, response.url)</div><div class="line">        item = scrapy.Item()</div><div class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</div><div class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</div><div class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<blockquote>
<p>该spider将从example.com的首页开始爬取，获取category以及item的链接并对后者使用 parse_item 方法。 对于每个item response，将使用XPath从HTML中提取一些数据，并使用它填充Item。</p>
</blockquote>
<h1 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h1><p>为了更好的理解，我们来看看实际案例中<code>Rules</code>如何使用</p>
<h2 id="豆瓣应用"><a href="#豆瓣应用" class="headerlink" title="豆瓣应用"></a>豆瓣应用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">rules = [Rule(LinkExtractor(allow=(<span class="string">r'https://movie.douban.com/top250\?start=\d+.*'</span>))),</div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">r'https://movie.douban.com/subject/\d+'</span>)),</div><div class="line">            callback=<span class="string">'parse_item'</span>, follow=<span class="keyword">False</span>)</div><div class="line">]</div></pre></td></tr></table></figure>
<p>如果接触过<code>django</code>，那么可以发现这个规则与<code>django</code>的路由系统十分相似（django都已经忘完了 -_-！），其实这里使用的正则匹配。</p>
<p>使用 <code>r&#39;https://movie.douban.com/top250\?start=\d+.*&#39;</code>来匹配翻页链接，如：</p>
<ul>
<li><a href="https://movie.douban.com/top250?start=25&amp;filter=" target="_blank" rel="external">https://movie.douban.com/top250?start=25&amp;filter=</a></li>
<li><a href="https://movie.douban.com/top250?start=50&amp;filter=" target="_blank" rel="external">https://movie.douban.com/top250?start=50&amp;filter=</a></li>
</ul>
<p>使用<code>https://movie.douban.com/subject/\d+</code>来匹配具体电影的链接，如：</p>
<ul>
<li><a href="https://movie.douban.com/subject/1292052/" target="_blank" rel="external">https://movie.douban.com/subject/1292052/</a></li>
<li><a href="https://movie.douban.com/subject/1291546/" target="_blank" rel="external">https://movie.douban.com/subject/1291546/</a></li>
</ul>
<h2 id="链家应用"><a href="#链家应用" class="headerlink" title="链家应用"></a>链家应用</h2><p>爬虫的通常需要在一个网页里面爬去其他的链接，然后一层一层往下爬，scrapy提供了LinkExtractor类用于对网页链接的提取，使用LinkExtractor需要使用<code>CrawlSpider</code>爬虫类中，<code>CrawlSpider</code>与<code>Spider</code>相比主要是多了<code>rules</code>，可以添加一些规则，先看下面这个例子，爬取链家网的链接</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LianjiaSpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">"lianjia"</span></div><div class="line"></div><div class="line">    allowed_domains = [<span class="string">"lianjia.com"</span>]</div><div class="line"></div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://bj.lianjia.com/ershoufang/"</span></div><div class="line">    ]</div><div class="line"></div><div class="line">    rules = [</div><div class="line">        <span class="comment"># 匹配正则表达式,处理下一页</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">r'http://bj.lianjia.com/ershoufang/pg\s+$'</span>,)), callback=<span class="string">'parse_item'</span>),</div><div class="line"></div><div class="line">        <span class="comment"># 匹配正则表达式,结果加到url列表中,设置请求预处理函数</span></div><div class="line">        <span class="comment"># Rule(FangLinkExtractor(allow=('http://www.lianjia.com/client/', )), follow=True, process_request='add_cookie')</span></div><div class="line">    ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="comment"># 这里与之前的parse方法一样，处理</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>同样的，使用<code>r&#39;http://bj.lianjia.com/ershoufang/pg\s+$&#39;</code>来匹配下一页链接，如：</p>
<ul>
<li><a href="https://bj.lianjia.com/ershoufang/pg2/" target="_blank" rel="external">https://bj.lianjia.com/ershoufang/pg2/</a></li>
<li><a href="https://bj.lianjia.com/ershoufang/pg3/" target="_blank" rel="external">https://bj.lianjia.com/ershoufang/pg3/</a></li>
</ul>
<p>还可以使用 <code>r&#39;https://bj.lianjia.com/ershoufang/\d+.html&#39;</code>来匹配详情页链接，如：</p>
<ul>
<li><a href="https://bj.lianjia.com/ershoufang/101102126888.html" target="_blank" rel="external">https://bj.lianjia.com/ershoufang/101102126888.html</a></li>
<li><a href="https://bj.lianjia.com/ershoufang/101100845676.html" target="_blank" rel="external">https://bj.lianjia.com/ershoufang/101100845676.html</a></li>
</ul>
<h1 id="学习参数"><a href="#学习参数" class="headerlink" title="学习参数"></a>学习参数</h1><h2 id="Rule对象"><a href="#Rule对象" class="headerlink" title="Rule对象"></a>Rule对象</h2><p>Role对象有下面参数</p>
<ul>
<li><code>link_extractor</code>：链接提取规则</li>
<li><code>callback</code>：link_extractor提取的链接的请求结果的回调</li>
<li><code>cb_kwargs</code>：附加参数，可以在回调函数中获取到</li>
<li><code>follow</code>：表示提取的链接请求完成后是否还要应用当前规则（boolean），如果为False则不会对提取出来的网页进行进一步提取，默认为False</li>
<li><code>process_links</code>：处理所有的链接的回调，用于处理从response提取的links，通常用于过滤（参数为link列表）</li>
<li><code>process_request</code>：链接请求预处理（添加header或cookie等）</li>
</ul>
<h2 id="LinkExtractor"><a href="#LinkExtractor" class="headerlink" title="LinkExtractor"></a>LinkExtractor</h2><p>LinkExtractor常用的参数有：</p>
<ul>
<li><code>allow</code>：提取满足正则表达式的链接</li>
<li><code>deny</code>：排除正则表达式匹配的链接（优先级高于allow）</li>
<li><code>allow_domains</code>：允许的域名（可以是str或list）</li>
<li><code>deny_domains</code>：排除的域名（可以是str或list）</li>
<li><code>restrict_xpaths</code>：提取满足XPath选择条件的链接（可以是str或list）</li>
<li><code>restrict_css</code>：提取满足css选择条件的链接（可以是str或list）</li>
<li><code>tags</code>：提取指定标签下的链接，默认从a和area中提取（可以是str或list）</li>
<li><code>attrs</code>：提取满足拥有属性的链接，默认为href（类型为list）</li>
<li><code>unique</code>：链接是否去重（类型为boolean）</li>
<li><code>process_value</code>：值处理函数（优先级大于allow）</li>
</ul>
<p>关于LinkExtractor的详细参数介绍见<a href="https://doc.scrapy.org/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml" target="_blank" rel="external">官网</a></p>
<blockquote>
<p>注意：在编写抓取Spider规则时，避免使用<code>parse</code>作为回调，因为<code>CrawlSpider</code>使用<code>parse</code>方法自己实现其逻辑。因此，如果你覆盖<code>parse</code>方法，爬行<code>Spider</code>将不再工作。</p>
</blockquote>
<p>最后说一个自己犯过的低级错误，我用Scrapy有个习惯，创建一个项目之后，直接<code>cd</code>目录，然后使用<code>genspider</code>命令，然后。。</p>
<figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">D:<span class="symbol">\B</span>ackup<span class="symbol">\桌</span>面</div><div class="line">λ scrapy startproject example</div><div class="line">New Scrapy project 'example', using template directory 'c:<span class="symbol">\\</span>users<span class="symbol">\\</span>administrator<span class="symbol">\\</span>appdata<span class="symbol">\\</span>local<span class="symbol">\\</span>programs<span class="symbol">\\</span>python<span class="symbol">\\</span>python36<span class="symbol">\\</span>lib<span class="symbol">\\</span>site-packages<span class="symbol">\\</span>scrapy<span class="symbol">\\</span>templates<span class="symbol">\\</span>project', created in:</div><div class="line">    D:<span class="symbol">\B</span>ackup<span class="symbol">\桌</span>面<span class="symbol">\e</span>xample</div><div class="line"></div><div class="line">You can start your first spider with:</div><div class="line">    cd example</div><div class="line">    scrapy genspider example example.com</div><div class="line"></div><div class="line">D:<span class="symbol">\B</span>ackup<span class="symbol">\桌</span>面</div><div class="line">λ cd example</div><div class="line"></div><div class="line">D:<span class="symbol">\B</span>ackup<span class="symbol">\桌</span>面<span class="symbol">\e</span>xample</div><div class="line">λ scrapy genspider em example.com</div><div class="line">Created spider 'em' using template 'basic' in module:</div><div class="line">  example.spiders.em</div></pre></td></tr></table></figure>
<p>然后我的<code>em.py</code>就变成了这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'em'</span></div><div class="line">    allowed_domains = [<span class="string">'example.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://example.com/'</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>注意，这个时候是不能使用<code>Rules</code>方法的，因为object不对，应该是</p>
<p><code>class EmSpider(CrawlSpider)</code></p>
<p>而不是<code>class EmSpider(scrapy.Spider):</code></p>
<p>共勉！！！</p>
<p>下一节应该会讲到Scrapy中各个组件的作用，以及这张神图</p>
<p><img src="https://i.imgur.com/PhD0NhL.png" alt=""></p>
<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><ul>
<li><a href="http://python.usyiyi.cn/documents/scrapy_12/topics/spiders.html#crawlspider-example" target="_blank" rel="external">CrawlSpider示例</a></li>
<li><a href="https://segmentfault.com/a/1190000007073049#articleHeader14" target="_blank" rel="external">scrapy学习笔记</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十九篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ALWvu6F.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
      <category term="Rules" scheme="https://zhangslob.github.io/tags/Rules/"/>
    
  </entry>
  
  <entry>
    <title>统计学学习笔记（二）数据整理与展示</title>
    <link href="https://zhangslob.github.io/2017/12/06/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E6%95%B0%E6%8D%AE%E6%95%B4%E7%90%86%E4%B8%8E%E5%B1%95%E7%A4%BA/"/>
    <id>https://zhangslob.github.io/2017/12/06/统计学学习笔记（二）数据整理与展示/</id>
    <published>2017-12-06T14:19:18.000Z</published>
    <updated>2017-12-06T15:00:29.414Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十八篇原创文章
</code></pre><p><img src="https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=3270851487,1770960951&amp;fm=27&amp;gp=0.jpg" alt=""></p>
<a id="more"></a>
<p>这是统计学的第二篇笔记，主要记录了如何整理数据与展示数据，书本上是这样说，但是我觉得现在人们会更多的叫做数据清洗与数据可视化。命名无所谓，掌握方法就好。</p>
<p>下面是正文。</p>
<blockquote>
<p>可以接着这里看哦 <a href="https://zhangslob.github.io/2017/11/30/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/">统计学学习笔记（一）</a></p>
</blockquote>
<hr>
<h1 id="数据整理与展示"><a href="#数据整理与展示" class="headerlink" title="数据整理与展示"></a>数据整理与展示</h1><h3 id="3-1-数据的预处理"><a href="#3-1-数据的预处理" class="headerlink" title="3.1 数据的预处理"></a>3.1 数据的预处理</h3><h4 id="3-1-1-数据审核"><a href="#3-1-1-数据审核" class="headerlink" title="3.1.1 数据审核"></a>3.1.1 数据审核</h4><ul>
<li>概念：检查数据是否有错误</li>
<li>对于通过调查取得的原始数据，主要从完整性和准确性两个方面去审核</li>
<li>对于通过其他渠道获得的二手数据，主要审核数据的适用性和时效性</li>
</ul>
<h4 id="3-1-2-数据筛选"><a href="#3-1-2-数据筛选" class="headerlink" title="3.1.2 数据筛选"></a>3.1.2 数据筛选</h4><ul>
<li>删除某些不符合要求的数据和有明显错误的数据</li>
<li>将符合某种特定条件的数据筛选出来，而将不符合特定条件的数据予以剔除</li>
<li>Excel举例</li>
</ul>
<h4 id="3-1-3-数据排序"><a href="#3-1-3-数据排序" class="headerlink" title="3.1.3 数据排序"></a>3.1.3 数据排序</h4><ul>
<li>概念：按一定顺序将数据排列，以便于研究者通过数据发现一些明显的特征或趋势，找到解决问题的线索</li>
<li>通过数据类型选择排序方式：字母型数据、汉字型数据、数值型数据</li>
</ul>
<blockquote>
<p>这个好像也叫做数据清洗</p>
</blockquote>
<h3 id="3-2-品质数据的整理与展示"><a href="#3-2-品质数据的整理与展示" class="headerlink" title="3.2 品质数据的整理与展示"></a>3.2 品质数据的整理与展示</h3><h4 id="3-2-1-频数与频数分布"><a href="#3-2-1-频数与频数分布" class="headerlink" title="3.2.1 频数与频数分布"></a>3.2.1 频数与频数分布</h4><ul>
<li>落在某一特定类别中的数据个数，成为频数</li>
<li>把各个类别及落在其中的相应频数全部列出，并用表格形式表现出来，成为频数分布</li>
</ul>
<blockquote>
<p>好像不止表格吧</p>
</blockquote>
<ul>
<li>一个总体（或样本）中各个部分的数据与全部数据之比，成为比例</li>
<li>将比例乘以100得到的数值，成为百分比或百分数用%表示</li>
<li>总体（或样本）中各不同类别数值之间的比值，成为比率</li>
</ul>
<h4 id="3-2-2-品质数据的展示"><a href="#3-2-2-品质数据的展示" class="headerlink" title="3.2.2 品质数据的展示"></a>3.2.2 品质数据的展示</h4><ol>
<li>条形图</li>
<li>饼图</li>
<li>环形图</li>
</ol>
<h3 id="3-3-数值型数据的整理与展示"><a href="#3-3-数值型数据的整理与展示" class="headerlink" title="3.3 数值型数据的整理与展示"></a>3.3 数值型数据的整理与展示</h3><h4 id="3-3-1-数据分组"><a href="#3-3-1-数据分组" class="headerlink" title="3.3.1 数据分组"></a>3.3.1 数据分组</h4><ul>
<li>根据统计分析的需要，将原始数据按照某种标准划分成不同的组别，成为数据分组</li>
<li>在组距分组中，一个组的最小值称为下限，一个组的最大值称为上限</li>
<li>每一组的上限和下限之间的中间值称为组中值</li>
</ul>
<h4 id="3-3-2-数值型数据的图示"><a href="#3-3-2-数值型数据的图示" class="headerlink" title="3.3.2 数值型数据的图示"></a>3.3.2 数值型数据的图示</h4><ul>
<li>分组数据看分布：直方图（histogram）</li>
<li>未分组数据看分布：茎叶图和箱型图</li>
<li>多变量数据的展示：雷达图</li>
</ul>
<h3 id="3-4-使用图表的注意事项"><a href="#3-4-使用图表的注意事项" class="headerlink" title="3.4 使用图表的注意事项"></a>3.4 使用图表的注意事项</h3><h4 id="优秀图表特征："><a href="#优秀图表特征：" class="headerlink" title="优秀图表特征："></a>优秀图表特征：</h4><ol>
<li>显示数据</li>
<li>让读者把注意力放在图表内容上</li>
<li>避免歪曲</li>
<li>强调数据之间的比较</li>
<li>服务于一个明确的目的</li>
<li>有对图形的描述统计和文字说明</li>
</ol>
<h4 id="优秀图形应当："><a href="#优秀图形应当：" class="headerlink" title="优秀图形应当："></a>优秀图形应当：</h4><ol>
<li>精心设计，有助于洞察问题的实质</li>
<li>使复杂的观点得到简明、确切、高效的阐述</li>
<li>能在最短的时间内、以最少的笔墨给读者提供大量的信息</li>
<li>是多维的</li>
<li>表述数据的真实情况</li>
</ol>
<h1 id="品质数据"><a href="#品质数据" class="headerlink" title="品质数据"></a>品质数据</h1><p>您应该会和我一样提问，什么是品质数据？</p>
<p>品质数据:对产品或商品进行各种化学、物理、力学等试验后所得出的数据。</p>
<ol>
<li>品质数据：对产品或商品进行各种化学、物理、力学等试验后所得出的数据。</li>
<li>品质型数据：按品质标志分组所得到数据，包括分类数据和顺序数据，他们在整理和图形展示上的方法大体相同。</li>
</ol>
<blockquote>
<p>本文中提到的品质数据应该是后者</p>
</blockquote>
<h1 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h1><p>这里重点说下数据图表的选择。</p>
<p>就我自己的工作中，比较常用的就是直方图、折线图与饼图，词云图（如果算的话）。教材中也说了很多没用过的图，这个东西还是要根据自己的具体业务来操作。</p>
<p>这里推荐一个很好的网站，<a href="https://antv.alipay.com/zh-cn/vis/chart/tag-compare.html" target="_blank" rel="external">图表使用</a></p>
<p><img src="https://i.imgur.com/gxGglUB.png" alt=""></p>
<p>这个网站有多好，你一看便知，不多解释。</p>
<h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><p>有人说：一个分析项目基本八成时间在洗数据。那么什么是清洗数据。</p>
<blockquote>
<p>数据清洗是指发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性，处理无效值和缺失值等。与问卷审核不同，录入后的数据清理一般是由计算机而不是人工完成。</p>
</blockquote>
<p><strong>数据清洗的方法 </strong></p>
<ol>
<li>解决不完整数据（ 即值缺失）的方法</li>
<li>错误值的检测及解决方法</li>
<li>重复记录的检测及消除方法</li>
<li>不一致性（ 数据源内部及数据源之间）的检测及解决方法</li>
<li>转换构造</li>
<li>数据压缩</li>
</ol>
<p>老规矩，还是放一点代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</div><div class="line"><span class="keyword">import</span> matplotlib.path <span class="keyword">as</span> path</div><div class="line"></div><div class="line">fig, ax = plt.subplots()</div><div class="line"></div><div class="line"><span class="comment"># Fixing random state for reproducibility</span></div><div class="line">np.random.seed(<span class="number">19680801</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># histogram our data with numpy</span></div><div class="line"></div><div class="line">data = np.random.randn(<span class="number">1000</span>)</div><div class="line">n, bins = np.histogram(data, <span class="number">50</span>)</div><div class="line"></div><div class="line"><span class="comment"># get the corners of the rectangles for the histogram</span></div><div class="line">left = np.array(bins[:<span class="number">-1</span>])</div><div class="line">right = np.array(bins[<span class="number">1</span>:])</div><div class="line">bottom = np.zeros(len(left))</div><div class="line">top = bottom + n</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># we need a (numrects x numsides x 2) numpy array for the path helper</span></div><div class="line"><span class="comment"># function to build a compound path</span></div><div class="line">XY = np.array([[left, left, right, right], [bottom, top, top, bottom]]).T</div><div class="line"></div><div class="line"><span class="comment"># get the Path object</span></div><div class="line">barpath = path.Path.make_compound_path_from_polys(XY)</div><div class="line"></div><div class="line"><span class="comment"># make a patch out of it</span></div><div class="line">patch = patches.PathPatch(barpath)</div><div class="line">ax.add_patch(patch)</div><div class="line"></div><div class="line"><span class="comment"># update the view limits</span></div><div class="line">ax.set_xlim(left[<span class="number">0</span>], right[<span class="number">-1</span>])</div><div class="line">ax.set_ylim(bottom.min(), top.max())</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://matplotlib.org/_images/sphx_glr_histogram_path_001.png" alt="直方图绘制"></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://wiki.mbalib.com/wiki/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97" target="_blank" rel="external">数据清洗</a></li>
<li><a href="https://www.zhihu.com/question/22077960" target="_blank" rel="external">数据挖掘中常用的数据清洗方法有哪些？</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十八篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=3270851487,1770960951&amp;amp;fm=27&amp;amp;gp=0.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="统计学" scheme="https://zhangslob.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
    
      <category term="统计学" scheme="https://zhangslob.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
      <category term="数据" scheme="https://zhangslob.github.io/tags/%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy学习实例（二）采集无限滚动页面</title>
    <link href="https://zhangslob.github.io/2017/12/03/Scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E4%BA%8C%EF%BC%89%E9%87%87%E9%9B%86%E6%97%A0%E9%99%90%E6%BB%9A%E5%8A%A8%E9%A1%B5%E9%9D%A2/"/>
    <id>https://zhangslob.github.io/2017/12/03/Scrapy学习实例（二）采集无限滚动页面/</id>
    <published>2017-12-03T11:48:53.000Z</published>
    <updated>2017-12-06T14:34:14.165Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十七篇原创文章
</code></pre><p><img src="https://i.imgur.com/ON4MMPT.jpg" alt=""><br><a id="more"></a></p>
<p>上一篇写的是<a href="https://zhangslob.github.io/2017/11/29/Scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E4%B8%80%EF%BC%89/">采集虎嗅网首页的新闻数据</a>，有朋友对我说，采集多页试试看。后来研究下，虎嗅网首页是POST加载，<code>Form Data</code>中携带参数，所以只需要带上一个循环就好了。这是我最初的想法，先让我们看看Scrapy中<br>如何采集无限滚动页面？</p>
<p>先举个栗子，采集网站是<a href="http://spidyquotes.herokuapp.com/scroll" target="_blank" rel="external">quotes</a></p>
<p><img src="https://i.imgur.com/jztJDKa.png" alt=""></p>
<h1 id="分析网页"><a href="#分析网页" class="headerlink" title="分析网页"></a>分析网页</h1><p><img src="https://i.imgur.com/aaLuLEx.png" alt=""></p>
<p>下拉时，会发现更多新的请求，观察这些请求，返回的都是json数据，也就是我们所需的，再看看他们的不同，也就是参数的改变，完整链接是：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">http:<span class="regexp">//</span>spidyquotes.herokuapp.com<span class="regexp">/api/</span>quotes?page=<span class="number">2</span></div><div class="line">http:<span class="regexp">//</span>spidyquotes.herokuapp.com<span class="regexp">/api/</span>quotes?page=<span class="number">3</span></div><div class="line">http:<span class="regexp">//</span>spidyquotes.herokuapp.com<span class="regexp">/api/</span>quotes?page=<span class="number">4</span></div></pre></td></tr></table></figure>
<p>这就很清晰了。</p>
<p><img src="https://i.imgur.com/PT2kb8b.png" alt=""></p>
<p>返回的是json，我们需要解析，然后提取数据，那我们如何知道最多有多少条json呢，文件已经告诉我们了：</p>
<p><code>has_next:true</code></p>
<h1 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h1><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scrapy startproject <span class="selector-tag">quote</span></div><div class="line"></div><div class="line">cd <span class="selector-tag">quote</span></div><div class="line"></div><div class="line">scrapy genspider spiderquote http:<span class="comment">//spidyquotes.herokuapp.com/scroll</span></div></pre></td></tr></table></figure>
<h1 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h1><p>查看网站，采集text、author和tags这三个</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuoteItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    text = scrapy.Field()</div><div class="line">    author = scrapy.Field()</div><div class="line">    tag = scrapy.Field()</div></pre></td></tr></table></figure>
<h1 id="编写spider"><a href="#编写spider" class="headerlink" title="编写spider"></a>编写spider</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpiderquoteSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'spiderquote'</span></div><div class="line">    quotes_base_url = <span class="string">'http://spidyquotes.herokuapp.com/api/quotes?page=%s'</span></div><div class="line">    start_urls = [quotes_base_url % <span class="number">1</span>]</div><div class="line">    download_delay = <span class="number">1.5</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        data = json.loads(response.body)</div><div class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data.get(<span class="string">'quotes'</span>, []):</div><div class="line">            <span class="keyword">yield</span> &#123;</div><div class="line">                <span class="string">'text'</span>: item.get(<span class="string">'text'</span>),</div><div class="line">                <span class="string">'author'</span>: item.get(<span class="string">'author'</span>, &#123;&#125;).get(<span class="string">'name'</span>),</div><div class="line">                <span class="string">'tags'</span>: item.get(<span class="string">'tags'</span>),</div><div class="line">            &#125;</div><div class="line">        <span class="keyword">if</span> data[<span class="string">'has_next'</span>]:</div><div class="line">            next_page = data[<span class="string">'page'</span>] + <span class="number">1</span></div><div class="line">            <span class="keyword">yield</span> scrapy.Request(self.quotes_base_url % next_page)</div></pre></td></tr></table></figure>
<p>运行爬虫，然后就可以看到结果了。</p>
<h1 id="应用到虎嗅网"><a href="#应用到虎嗅网" class="headerlink" title="应用到虎嗅网"></a>应用到虎嗅网</h1><p>那么如何应用到虎嗅网呢？首先还是要去分析网页。</p>
<p><img src="https://i.imgur.com/x7YoUYW.png" alt=""></p>
<p>虎嗅网的参数有3个：</p>
<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">huxiu_hash_code:</span><span class="number">13</span>a3a353c52d424e1e263dda4d594e59</div><div class="line"><span class="symbol">page:</span><span class="number">3</span></div><div class="line"><span class="symbol">last_dateline:</span><span class="number">1512026700</span></div></pre></td></tr></table></figure>
<p>我们知道<code>page</code>就是翻页页码，<code>huxiu_hash_code</code>是一个不变的字符，<code>last_dateline</code>看起来像unix时间戳，验证确实如此。这个时间戳有必要带上吗，我想验证试试看。</p>
<p><img src="https://i.imgur.com/fF0ivj5.png" alt=""></p>
<p>在postman中测试，不带上<code>last_dateline</code>也是可以返回数据，并且这个json中已经告诉我们一共有多少页：</p>
<p><code>&quot;total_page&quot;: 1654</code></p>
<p>在主函数中我们可以依葫芦画瓢</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> huxiu.items <span class="keyword">import</span> HuxiuItem</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'HuXiu'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        url = <span class="string">'https://www.huxiu.com/v2_action/article_list'</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</div><div class="line">        <span class="comment"># FormRequest 是Scrapy发送POST请求的方法</span></div><div class="line">            <span class="keyword">yield</span> scrapy.FormRequest(</div><div class="line">                url = url,</div><div class="line">                formdata = &#123;<span class="string">"huxiu_hash_code"</span> : <span class="string">"13a3a353c52d424e1e263dda4d594e59"</span>, <span class="string">"page"</span> : str(i)&#125;,</div><div class="line">                callback = self.parse</div><div class="line">            )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = HuxiuItem()</div><div class="line">        data = json.loads(response.text)</div><div class="line">        s = etree.HTML(data[<span class="string">'data'</span>])</div><div class="line"></div><div class="line">        item[<span class="string">'title'</span>] = s.xpath(<span class="string">'//a[@class="transition msubstr-row2"]/text()'</span>)</div><div class="line">        item[<span class="string">'link'</span>] = s.xpath(<span class="string">'//a[@class="transition msubstr-row2"]/@href'</span>)</div><div class="line">        item[<span class="string">'author'</span>] = s.xpath(<span class="string">'//span[@class="author-name"]/text()'</span>)</div><div class="line">        item[<span class="string">'introduction'</span>] = s.xpath(<span class="string">'//div[@class="mob-sub"]/text()'</span>)</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> item</div></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/cntFATT.png" alt=""></p>
<p>输出的数据有点难看，是一段一段的。。</p>
<p>因为<code>data[&#39;data&#39;]</code>是一段html文件，所以这里选择的是xpath，不清楚这里是否直接使用Scrapy的xpath解析工具，如果可以，欢迎在评论中告诉我。</p>
<h1 id="本篇收获"><a href="#本篇收获" class="headerlink" title="本篇收获"></a>本篇收获</h1><ol>
<li>Scrapy采集动态网站：分析网页</li>
<li>使用Scrapy模拟post请求方法，<a href="https://doc.scrapy.org/en/latest/topics/request-response.html" target="_blank" rel="external">文档在这</a></li>
<li>刘亦菲好漂亮</li>
</ol>
<h1 id="待做事宜"><a href="#待做事宜" class="headerlink" title="待做事宜"></a>待做事宜</h1><ol>
<li>完善文件保存与解析</li>
<li>全站抓取大概用了3分钟，速度有点慢</li>
</ol>
<blockquote>
<p>若想评论，先翻长城</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十七篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ON4MMPT.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
      <category term="虎嗅" scheme="https://zhangslob.github.io/tags/%E8%99%8E%E5%97%85/"/>
    
  </entry>
  
  <entry>
    <title>统计学学习笔记（一）</title>
    <link href="https://zhangslob.github.io/2017/11/30/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://zhangslob.github.io/2017/11/30/统计学学习笔记（一）/</id>
    <published>2017-11-30T15:45:14.000Z</published>
    <updated>2017-12-01T14:48:31.526Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十六篇原创文章
</code></pre><p><img src="https://i.imgur.com/gmL4oSK.jpg" alt=""></p>
<a id="more"></a>
<p>这是学习统计学的第一篇笔记，以后尽量都放在这里吧。</p>
<p>发现使用hexo发文章的快捷键：</p>
<p><code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></p>
<p>下面是正文：</p>
<hr>
<h1 id="1、统计中的基本概念"><a href="#1、统计中的基本概念" class="headerlink" title="1、统计中的基本概念"></a>1、统计中的基本概念</h1><ol>
<li><strong>总体和样本</strong>。总体：所研究的全部个体；样本：总体中的一部分</li>
<li><strong>参数和统计量</strong>。参数：用来描述<strong>总体</strong>特征的概括性数字度量；统计量：用来描述<strong>样本</strong>特征的概括性数字度量。</li>
<li><strong>变量</strong>。变量、分类变量（事物类别的一个名称）、顺序变量（事物有序类别的一个名称）、数值型变量、离散型变量（只能取可数值的变量）、连续性变量。</li>
</ol>
<h1 id="2、数据的收集"><a href="#2、数据的收集" class="headerlink" title="2、数据的收集"></a>2、数据的收集</h1><h3 id="1、数据的间接来源"><a href="#1、数据的间接来源" class="headerlink" title="1、数据的间接来源"></a>1、数据的间接来源</h3><p><strong>二手数据</strong>：公开出版的或公开报道的数据。</p>
<h3 id="2、数据的直接来源"><a href="#2、数据的直接来源" class="headerlink" title="2、数据的直接来源"></a>2、数据的直接来源</h3><h4 id="（1）统计调查方式"><a href="#（1）统计调查方式" class="headerlink" title="（1）统计调查方式"></a>（1）统计调查方式</h4><ol>
<li>抽样调查：经济性、时效性强、适应面广、准确性高。</li>
<li>普查：一次性或周期性、规定调查时间、数据比较准确、范围比较狭窄。</li>
</ol>
<h4 id="（2）数据的收集方法"><a href="#（2）数据的收集方法" class="headerlink" title="（2）数据的收集方法"></a>（2）数据的收集方法</h4><ol>
<li>询问调查：访问调查、邮寄调查、电话调查、计算机辅助调查、座谈会、个别深度访问。</li>
<li>观察与实验：观察法、实验法。</li>
</ol>
<blockquote>
<p>竟然没网络爬虫，嘤嘤嘤</p>
</blockquote>
<h3 id="3、调查设计"><a href="#3、调查设计" class="headerlink" title="3、调查设计"></a>3、调查设计</h3><h4 id="（1）调查方案设计"><a href="#（1）调查方案设计" class="headerlink" title="（1）调查方案设计"></a>（1）调查方案设计</h4><ol>
<li>调查目的</li>
<li>调查对象和调查单位</li>
<li>调查项目和调查表</li>
</ol>
<h4 id="（2）调查问卷设计"><a href="#（2）调查问卷设计" class="headerlink" title="（2）调查问卷设计"></a>（2）调查问卷设计</h4><h5 id="a-调查问卷的基本结构："><a href="#a-调查问卷的基本结构：" class="headerlink" title="a.调查问卷的基本结构："></a>a.调查问卷的基本结构：</h5><ul>
<li>开头部分（问候语、填表说明、问卷编号 ）</li>
<li>甄别部分：过滤——筛选掉不需要的部分——针对特定人群</li>
<li>主体部分：调查的全部问题</li>
<li>背景部分：被调查者的背景资料</li>
</ul>
<h5 id="b-提问项目的设计："><a href="#b-提问项目的设计：" class="headerlink" title="b.提问项目的设计："></a>b.提问项目的设计：</h5><ul>
<li>提问的内容尽可能短</li>
<li>用词要确切、通俗</li>
<li>一个项目只包含一项内容</li>
<li>避免诱导性提问</li>
<li>避免否定式提问</li>
<li>避免敏感性问题</li>
</ul>
<h5 id="c-回答项目的设计"><a href="#c-回答项目的设计" class="headerlink" title="c.回答项目的设计"></a>c.回答项目的设计</h5><ul>
<li>开放性问题：灵活；整理资料困难</li>
<li>封闭性问题：两项选择法、多项选择法（单项选择型、多项选择型、限制选择型）</li>
<li>顺序选择法：按顺序排列</li>
<li>评定尺度法：和NPS有点像哦</li>
<li>双向列联法：表格表现</li>
</ul>
<h5 id="d-问题顺序的设计"><a href="#d-问题顺序的设计" class="headerlink" title="d.问题顺序的设计"></a>d.问题顺序的设计</h5><ul>
<li>问题的安排硬具有逻辑性</li>
<li>问题的顺序应先难后易</li>
<li>能引起被调查者兴趣的问题放在最前面</li>
<li>开放性问题放在后面</li>
</ul>
<h4 id="（3）统计数据的质量"><a href="#（3）统计数据的质量" class="headerlink" title="（3）统计数据的质量"></a>（3）统计数据的质量</h4><h5 id="a-统计数据的误差"><a href="#a-统计数据的误差" class="headerlink" title="a.统计数据的误差"></a>a.统计数据的误差</h5><h5 id="b-统计数据的误差"><a href="#b-统计数据的误差" class="headerlink" title="b.统计数据的误差"></a>b.统计数据的误差</h5><ul>
<li>精度</li>
<li>准确性</li>
<li>关联性</li>
<li>及时性</li>
<li>一致性</li>
<li><p>最低成本</p>
<hr>
</li>
</ul>
<p>这是统计学基础 第三版 (贾俊平)的记录，看了前两章，感觉受益匪浅，尤其是问卷的设计，比较系统、完整，可以应用在以后的工作中。</p>
<blockquote>
<p>书名：统计学基础 第三版</p>
<p>作者：贾俊平</p>
<p>出版社：中国人民大学出版社</p>
</blockquote>
<h1 id="最后说一说为什么要学统计学"><a href="#最后说一说为什么要学统计学" class="headerlink" title="最后说一说为什么要学统计学"></a>最后说一说为什么要学统计学</h1><p>最直接原因是<strong>工资高</strong>。可以去拉勾上看看“数据分析”、“数据挖掘”、“数据科学家”等职位，他们对学历的要求基本上都会有“统计学”。</p>
<p>对于我这种文科生来说，学习统计学是必经之路。<strong>敲门砖啊！</strong></p>
<p>最后记录下最近学习的数据科学的流程：</p>
<ol>
<li>业务理解</li>
<li>分析方法</li>
<li>数据要求</li>
<li>收集数据</li>
<li>数据理解</li>
<li>数据准备</li>
<li>建模（use and test）</li>
<li>模型评估</li>
<li>部署与反馈</li>
</ol>
<p>自己判断，缺少的是业务理解，对相关的业务知识了解太少；分析方法知道的太少了，接下来会着重学习一些常见的算法；数据准备也是一个大坑，不过好在自己有一些Python基础；建模才是最难的，慢慢来吧。</p>
<p><strong>最近几天需要个自己定一个学习任务，内容主要包括：统计学基础、常见算法、pandas处理数据及可视化、业务理解、Scrapy框架学习、前端（没错，学点前端很有必要）</strong></p>
<p>欢迎加我微信，一起来学习，嘤嘤嘤</p>
<blockquote>
<p>下面是常见的分析方法</p>
</blockquote>
<p><img src="https://i.imgur.com/poRqKVo.png" alt=""></p>
<p><img src="https://i.imgur.com/FLfwzDU.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十六篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/gmL4oSK.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="统计学" scheme="https://zhangslob.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
    
      <category term="统计学" scheme="https://zhangslob.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
      <category term="数据" scheme="https://zhangslob.github.io/tags/%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy学习实例（一）</title>
    <link href="https://zhangslob.github.io/2017/11/29/Scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://zhangslob.github.io/2017/11/29/Scrapy学习实例（一）/</id>
    <published>2017-11-29T13:52:32.000Z</published>
    <updated>2017-12-03T13:36:18.610Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十五篇原创文章
</code></pre><p><img src="https://i.imgur.com/n1TnhMv.png" alt=""><br><a id="more"></a></p>
<p>Hello，我又回来啦。以后就在这发文章吧，记录自己的学习历程。</p>
<blockquote>
<p>举头卖竹鼠，低头嘤嘤嘤。</p>
</blockquote>
<p>我会记录自己对Scrapy的学历经历，更重要的是理解。下面就开始吧，首先当然是创建一个项目啦！</p>
<p>我选择爬取<a href="https://www.huxiu.com/" target="_blank" rel="external">虎嗅网</a>首页的新闻列表。</p>
<h1 id="1、创建项目"><a href="#1、创建项目" class="headerlink" title="1、创建项目"></a>1、创建项目</h1><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">F:\Python\huxiu&gt;scrapy startproject huxiu</div><div class="line">New Scrapy project 'huxiu', using template directory 'c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scrapy\\templates\\pro</div><div class="line">ject', created in:</div><div class="line">    F:\Python\huxiu\huxiu</div><div class="line"></div><div class="line">You can start your first spider with:</div><div class="line">    cd huxiu</div><div class="line">    scrapy genspider example example.com</div><div class="line"></div><div class="line">F:\Python\huxiu&gt;cd huxiu</div><div class="line"></div><div class="line">F:\Python\huxiu\huxiu&gt;scrapy genspider huxiu huxiu.com</div><div class="line">Cannot create a spider with the same name as your project</div><div class="line"></div><div class="line">F:\Python\huxiu\huxiu&gt;scrapy genspider HuXiu huxiu.com</div><div class="line">Created spider 'HuXiu' using template 'basic' in module:</div><div class="line">  huxiu.spiders.HuXiu</div></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/MrrcuLg.png" alt=""></p>
<blockquote>
<p>记住爬虫和项目命名不一样</p>
</blockquote>
<h1 id="2、定义Item"><a href="#2、定义Item" class="headerlink" title="2、定义Item"></a>2、定义Item</h1><p>在<code>item.py</code>中创建<code>scrapy.Item</code>类，并定义它的类型为<code>scrapy.Field</code>的属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    title = scrapy.Field() <span class="comment">#标题</span></div><div class="line">    link = scrapy.Field() <span class="comment">#链接</span></div><div class="line">    author = scrapy.Field() <span class="comment">#作者</span></div><div class="line">    introduction = scrapy.Field() <span class="comment">#简介</span></div><div class="line">    time = scrapy.Field() <span class="comment">#时间</span></div></pre></td></tr></table></figure>
<h1 id="3、编写Spider"><a href="#3、编写Spider" class="headerlink" title="3、编写Spider"></a>3、编写Spider</h1><p><img src="https://i.imgur.com/uK5EU0x.png" alt=""></p>
<blockquote>
<p>一目了然</p>
</blockquote>
<p>在<code>huxiu/spider/HuXiu.py</code>中编写代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> huxiu.items <span class="keyword">import</span> HuxiuItem</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'HuXiu'</span></div><div class="line">    allowed_domains = [<span class="string">'huxiu.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://huxiu.com/'</span>]</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</div><div class="line">            item = HuxiuItem()</div><div class="line">            item[<span class="string">'title'</span>] = s.xpath(<span class="string">'h2/a/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'link'</span>] = s.xpath(<span class="string">'h2/a/@href'</span>)[<span class="number">0</span>].extract()</div><div class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</div><div class="line">            item[<span class="string">'author'</span>] = s.xpath(<span class="string">'div/a/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'introduction'</span>] = s.xpath(<span class="string">'div[2]/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'time'</span>] = s.xpath(<span class="string">'div/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            print(item)</div></pre></td></tr></table></figure>
<p>在终端输入命令</p>
<pre><code>scrapy crawl HuXiu
</code></pre><p>部分输出</p>
<p><img src="https://i.imgur.com/MqWJ2vP.png" alt=""></p>
<h1 id="4、深度爬取"><a href="#4、深度爬取" class="headerlink" title="4、深度爬取"></a>4、深度爬取</h1><p>哈哈，这里借用造数的命名了。其实就是爬取新闻详情页。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> huxiu.items <span class="keyword">import</span> HuxiuItem</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'HuXiu'</span></div><div class="line">    allowed_domains = [<span class="string">'huxiu.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://huxiu.com/'</span>]</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</div><div class="line">            item = HuxiuItem()</div><div class="line">            item[<span class="string">'title'</span>] = s.xpath(<span class="string">'h2/a/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'link'</span>] = s.xpath(<span class="string">'h2/a/@href'</span>)[<span class="number">0</span>].extract()</div><div class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</div><div class="line">            item[<span class="string">'author'</span>] = s.xpath(<span class="string">'div/a/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'introduction'</span>] = s.xpath(<span class="string">'div[2]/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'time'</span>] = s.xpath(<span class="string">'div/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            <span class="comment">#print(item)</span></div><div class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_article)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_article</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = HuxiuItem()</div><div class="line">        detail = response.xpath(<span class="string">'//div[@class="article-wrap"]'</span>)</div><div class="line">        item[<span class="string">'title'</span>] = detail.xpath(<span class="string">'h1/text()'</span>)[<span class="number">0</span>].extract().strip()</div><div class="line">        item[<span class="string">'link'</span>] = response.url</div><div class="line">        item[<span class="string">'author'</span>] = detail.xpath(<span class="string">'div[@class="article-author"]/span/a/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">        item[<span class="string">'time'</span>] = detail.xpath(<span class="string">'div[@class="article-author"]/div[@class="column-link-box"]/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">        print(item)</div><div class="line">        word = detail.xpath(<span class="string">'div[5]'</span>)</div><div class="line">        print(word[<span class="number">0</span>].xpath(<span class="string">'string(.)'</span>).extract()[<span class="number">0</span>])</div><div class="line">        <span class="keyword">yield</span> item</div></pre></td></tr></table></figure></p>
<p>输出结果</p>
<p><img src="https://i.imgur.com/JJoTvtU.png" alt=""></p>
<p>说明一点，<code>如何使用xpath获得多个标签下的文本</code>，这里参考了<a href="http://blog.csdn.net/MrLevo520/article/details/53158050" target="_blank" rel="external">解决：xpath取出指定多标签内所有文字text</a>，把文章详细内容打印出来，但是会遇到一些错误，可以使用<code>goose</code>来试试看。</p>
<h1 id="Python-Goose-Article-Extractor"><a href="#Python-Goose-Article-Extractor" class="headerlink" title="Python-Goose - Article Extractor"></a><a href="https://github.com/grangier/python-goose" target="_blank" rel="external">Python-Goose - Article Extractor</a></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> goose <span class="keyword">import</span> Goose</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> goose.text <span class="keyword">import</span> StopWordsChinese</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>url  = <span class="string">'http://www.bbc.co.uk/zhongwen/simp/chinese_news/2012/12/121210_hongkong_politics.shtml'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>g = Goose(&#123;<span class="string">'stopwords_class'</span>: StopWordsChinese&#125;)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>article = g.extract(url=url)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> article.cleaned_text[:<span class="number">150</span>]</div><div class="line">香港行政长官梁振英在各方压力下就其大宅的违章建筑（僭建）问题到立法会接受质询，并向香港民众道歉。</div><div class="line"></div><div class="line">梁振英在星期二（<span class="number">12</span>月<span class="number">10</span>日）的答问大会开始之际在其演说中道歉，但强调他在违章建筑问题上没有隐瞒的意图和动机。</div><div class="line"></div><div class="line">一些亲北京阵营议员欢迎梁振英道歉，且认为应能获得香港民众接受，但这些议员也质问梁振英有</div></pre></td></tr></table></figure>
<h1 id="参考文章："><a href="#参考文章：" class="headerlink" title="参考文章："></a>参考文章：</h1><ul>
<li><a href="https://www.xncoding.com/2016/03/10/scrapy-02.html" target="_blank" rel="external">Scrapy笔记02- 完整示例</a></li>
<li><a href="http://blog.csdn.net/MrLevo520/article/details/53158050" target="_blank" rel="external">解决：xpath取出指定多标签内所有文字text</a></li>
</ul>
<blockquote>
<p>若想评论，先翻长城</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十五篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/n1TnhMv.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
      <category term="虎嗅" scheme="https://zhangslob.github.io/tags/%E8%99%8E%E5%97%85/"/>
    
  </entry>
  
  <entry>
    <title>爬虫三步走（二）解析源码</title>
    <link href="https://zhangslob.github.io/2017/05/26/%E7%88%AC%E8%99%AB%E4%B8%89%E6%AD%A5%E8%B5%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E8%A7%A3%E6%9E%90%E6%BA%90%E7%A0%81/"/>
    <id>https://zhangslob.github.io/2017/05/26/爬虫三步走（二）解析源码/</id>
    <published>2017-05-26T12:15:20.000Z</published>
    <updated>2017-05-26T12:16:58.084Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十四篇原创文章
</code></pre><p><img src="http://i.imgur.com/Dw2WeLb.png" alt=""></p>
<p>爬虫三步走：获取源码、解析源码、数据储存</p>
<a id="more"></a>
<p>上一期讲了如何获取网页源码的方法，这一期说一说怎么从其中获得我们需要的和数据。</p>
<p>解析网页的方法很多，最常见的就是BeautifulSoup和正则了，其他的像xpath、PyQuery等等，其中我觉得最好用的就是xpath了，xpath真的超级简单好用，学了之后再也不想取用<code>美丽汤</code>了。下面介绍xpath的使用方法。<br><img src="http://i.imgur.com/tVgBAeY.jpg" alt=""></p>
<p>首先需要安装lxml，windows下安装lxml是个大坑，知乎上有人给出了解决方法<a href="https://www.zhihu.com/question/30047496" target="_blank" rel="external">Python LXML模块死活安装不了怎么办？</a></p>
<p>详细的用法可以参考<a href="https://zhuanlan.zhihu.com/p/25572729" target="_blank" rel="external">爬虫入门到精通-网页的解析（xpath）</a></p>
<p>在这里我们尝试使用xpath来迅速获取数据。</p>
<p>例如想要获熊猫直播<a href="http://www.huya.com/g/lol" target="_blank" rel="external">虎牙直播</a>下主播的ID</p>
<p><img src="http://i.imgur.com/HEGXkuM.jpg" alt=""></p>
<pre><code>import requests
from lxml import etree

url = &apos;http://www.huya.com/g/lol&apos;
headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
res = requests.get(url,headers=headers).text
s = etree.HTML(res)
print(s.xpath(&apos;//i[@class=&quot;nick&quot;]/text()&apos;))
</code></pre><p>输出：</p>
<p><img src="http://i.imgur.com/czo9c1M.png" alt=""></p>
<p>下面一步步讲解为什么这样做。</p>
<pre><code>import requests
from lxml import etree
</code></pre><p>首先是导入模块，<code>requests</code>很常见，但是xpath需要    <code>from lxml import etree</code>，你肯点想问为什么这样写，回答是“我也不知道”，就像是约定俗成的东西一样。</p>
<pre><code>url = &apos;http://www.huya.com/g/lol&apos;
headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
res = requests.get(url,headers=headers).text
</code></pre><p>这三步就是平常获取源码的过程，很简单。</p>
<pre><code>s = etree.HTML(res)
</code></pre><p>给一个html，返回xml结构，为什么这样写？？答案和上面一样。最重要的就是下面的这一步：</p>
<pre><code>s.xpath(&apos;//i[@class=&quot;nick&quot;]/text()&apos;)
</code></pre><p><img src="http://i.imgur.com/QP54jeA.png" alt=""></p>
<p>按下F12看到“爱拍-古手羽”在<code>i</code>标签下，接着我们右键打开“查看网页源代码”，搜索“爱拍-古手羽”</p>
<p><img src="http://i.imgur.com/BkKsKgH.png" alt=""></p>
<p>确实找到了“爱拍-古手羽”就在<code>i</code>标签下，那我们就把他提出来吧！</p>
<p><code>s.xpath(&#39;//i[@class=&quot;nick&quot;]/text()&#39;)</code></p>
<p>这个段代码意思是，找到class为“nick”的<code>i</code>标签，返回其中的文本信息，当然你也可以返回<code>i</code>标签中的<code>title</code>，写法如下：</p>
<p><code>s.xpath(&#39;//i[@class=&quot;nick&quot;]/@title&#39;)</code></p>
<p><code>text()</code>返回的是文本信息，<code>@title</code>则是标签里面的具体属性的值，例如我想知道观众人数</p>
<pre><code>import requests
from lxml import etree

url = &apos;http://www.huya.com/g/lol&apos;
headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
res = requests.get(url,headers=headers).text
s = etree.HTML(res)
print(s.xpath(&apos;//i[@class=&quot;js-num&quot;]/text()&apos;))
</code></pre><p>只需在原来基础上修改一个属性，<code>i</code>标签class为“js-num”里面的值</p>
<p><img src="http://i.imgur.com/jTKeBqW.png" alt=""></p>
<pre><code>print(s.xpath(&apos;//i[@class=&quot;js-num&quot;]/text()&apos;))
</code></pre><p>返回结果是：</p>
<p><img src="http://i.imgur.com/nuuT658.png" alt=""></p>
<p>说明：在运行代码中，发现虎牙反爬虫做得挺好的，瞬间就识别爬虫身份并封了IP，所以我换了IP去访问，至于如何设置代理，在我的上一篇文章中有说到，去看看吧。</p>
<p>在实际操作中，你可能会遇到更加复杂的情况，所以一定记得去看看详细的教程。<a href="https://zhuanlan.zhihu.com/p/25572729" target="_blank" rel="external">爬虫入门到精通-网页的解析（xpath）</a></p>
<p>小广告：喜欢爬虫、数据的可以关注一下我的微信公众号（<strong>zhangslob</strong>），多多交流。</p>
<p><img src="http://i.imgur.com/PZMOvTP.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十四篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/Dw2WeLb.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;爬虫三步走：获取源码、解析源码、数据储存&lt;/p&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python入门" scheme="https://zhangslob.github.io/tags/Python%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>爬虫三步走（一）获取源码</title>
    <link href="https://zhangslob.github.io/2017/05/25/%E7%88%AC%E8%99%AB%E4%B8%89%E6%AD%A5%E8%B5%B0%EF%BC%88%E4%B8%80%EF%BC%89%E8%8E%B7%E5%8F%96%E6%BA%90%E7%A0%81/"/>
    <id>https://zhangslob.github.io/2017/05/25/爬虫三步走（一）获取源码/</id>
    <published>2017-05-25T07:21:06.000Z</published>
    <updated>2017-11-30T15:21:41.231Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十三篇原创文章
</code></pre><p><img src="http://i.imgur.com/Dw2WeLb.png" alt=""></p>
<p>爬虫三步走：获取源码、解析源码、数据储存</p>
<a id="more"></a>
<p>举个例子，爬一爬知乎日报的相关数据 <a href="http://daily.zhihu.com/" target="_blank" rel="external">知乎日报 - 每天 3 次，每次 7 分钟</a></p>
<h1 id="1、获取源码"><a href="#1、获取源码" class="headerlink" title="1、获取源码"></a>1、获取源码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line">	</div><div class="line">url = <span class="string">'http://daily.zhihu.com/'</span></div><div class="line">res = requests.get(url).text</div><div class="line">print(res)</div></pre></td></tr></table></figure>
<p>个人喜欢requests，直接访问，发现返回500错误</p>
<pre><code>C:\Python35\python.exe F:/PyCharm/爬虫/daily.py
&lt;html&gt;&lt;body&gt;&lt;h1&gt;500 Server Error&lt;/h1&gt;
An internal server error occured.
&lt;/body&gt;&lt;/html&gt;


Process finished with exit code 0
</code></pre><p>根据经验判断，是知乎禁止爬虫，需要加上一些伪装，让我们看看加上浏览器伪装效果</p>
<pre><code>import requests

headers = {&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
url = &apos;http://daily.zhihu.com/&apos;
res = requests.get(url,headers=headers).text
print(res)
</code></pre><p>看看结果，已经返回我们需要的数据</p>
<pre><code>C:\Python35\python.exe F:/PyCharm/爬虫/daily.py
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-CN&quot;&gt;&lt;head&gt;&lt;title&gt;知乎日报 - 每天 3 次，每次 7 分钟&lt;/title&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot;&gt;&lt;meta name=&quot;description&quot; content=&quot;在中国，资讯类移动应用的人均阅读时长是 5 分钟，而在知乎日报，这个数字是 21。以独有的方式为你提供最高质、最深度、最有收获的阅读体验。&quot;&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;/css/base.auto.css&quot;&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;/css/new_home_v3.auto.css&quot;&gt;&lt;script src=&quot;/js/jquery.1.9.1.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/js/new_index_v3/home.js&quot;&gt;&lt;/script&gt;&lt;link rel=&quot;shortcut icon&quot; href=&quot;/favicon.ico&quot; type=&quot;image/x-icon&quot;&gt;&lt;base target=&quot;_blank&quot;&gt;&lt;style&gt;h1,h2,h3 {padding: 0;margin:0}&lt;/style&gt;&lt;base target=&quot;_blank&quot;&gt;&lt;/head&gt;&lt;body class=&quot;home&quot;&gt;&lt;a href=&quot;javascript:;&quot; title=&quot;回到顶部&quot; class=&quot;back-to-top&quot;&gt;&lt;/a&gt;&lt;div class=&quot;header navbar-fixed-top&quot;&gt;&lt;div class=&quot;container-fixed-width clearfix&quot;&gt;&lt;div class=&quot;top-nav-link&quot;&gt;&lt;a href=&quot;javascript:;&quot; data-offset=&quot;470&quot;&gt;&lt;span&gt;浏览内容&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;javascript:;&quot; data-offset=&quot;0&quot; class=&quot;active&quot;&gt;&lt;span&gt;App 下载&lt;/span&gt;&lt;/a&gt;&lt;/div&gt;&lt;h1 class=&quot;logo&quot;&gt;&lt;a href=&quot;http://daily.zhihu.com/&quot; title=&quot;知乎日报&quot; class=&quot;link-logo&quot;&gt;知乎日报&lt;/a&gt;&lt;/h1&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;download&quot;&gt;
...
</code></pre><p>但是这种写法是否可以应用到所有的网站，答案是“不”</p>
<p><img src="http://i.imgur.com/RKDWmZY.png" alt=""></p>
<h1 id="2、代理设置"><a href="#2、代理设置" class="headerlink" title="2、代理设置"></a>2、代理设置</h1><p>有时候同一个IP去爬取同一网站上的内容，久了之后就会被该网站服务器屏蔽。解决方法就是更换IP。这个时候，在对方网站上，显示的不是我们真实地IP地址，而是代理服务器的IP地址。</p>
<p><a href="http://www.xicidaili.com/nn/" target="_blank" rel="external">国内高匿免费HTTP代理IP_国内高匿</a> 西刺代理提供了很多可用的国内IP，可以直接拿来使用。</p>
<p>那么如何在爬虫里加入代理呢，看看requests的官方文档怎么说。<a href="http://docs.python-requests.org/zh_CN/latest/user/advanced.html#proxies" target="_blank" rel="external">高级用法 - Requests 2.10.0 文档</a></p>
<p>如果需要使用代理，你可以通过为任意请求方法提供 proxies 参数来配置单个请求:</p>
<pre><code>import requests

proxies = {
  &quot;http&quot;: &quot;http://10.10.1.10:3128&quot;,
  &quot;https&quot;: &quot;http://10.10.1.10:1080&quot;,
}

requests.get(&quot;http://example.org&quot;, proxies=proxies)
</code></pre><p>用法很简单，加入proxies参数即可 </p>
<pre><code>import requests

proxies = {
  &quot;http&quot;: &quot;http://121.201.24.248：8088&quot;,
  &quot;https&quot;: &quot;http://36.249.194.52：8118&quot;,
}
headers = {&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
url = &apos;http://daily.zhihu.com/&apos;
res = requests.get(url,headers=headers，proxies=proxies).text
print(len(res))
</code></pre><p>为了便于测试，只打印出返回数据的长度</p>
<pre><code>C:\Python35\python.exe F:/PyCharm/爬虫/daily.py
10830

Process finished with exit code 0
</code></pre><p>发现代理服务器成功爬取知乎日报的信息，内容是10830，故意把代理IP写错一位数，看看结果</p>
<pre><code>import requests

proxies = {
  &quot;http&quot;: &quot;http://121.201.24.248：8088&quot;,
  &quot;https&quot;: &quot;http://36.249.194.52: 222&quot;,
}
headers = {&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
url = &apos;http://daily.zhihu.com/&apos;
res = requests.get(url,headers=headers,proxies=proxies).text
print(len(res))
</code></pre><p>我们把”https”: “<a href="http://36.249.194.52：8118&quot;修改为&quot;https" target="_blank" rel="external">http://36.249.194.52：8118&quot;修改为&quot;https</a>“: “<a href="http://36.249.194.52" target="_blank" rel="external">http://36.249.194.52</a>: 222”，此时返回的结果如下，发现不能获取网页数据。所以，在使用代理服务器爬去网站时，如果出现异常，要考虑代理IP是否失效了。 当然你也可以写一个爬虫，实时抓取最新的代理IP用来爬取。</p>
<pre><code>Traceback (most recent call last):
  File &quot;F:/PyCharm/爬虫/daily.py&quot;, line 9, in &lt;module&gt;
    res = requests.get(url,headers=headers,proxies=proxies).text
  File &quot;C:\Python35\lib\site-packages\requests\api.py&quot;, line 70, in get
    return request(&apos;get&apos;, url, params=params, **kwargs)
  File &quot;C:\Python35\lib\site-packages\requests\api.py&quot;, line 56, in request
    return session.request(method=method, url=url, **kwargs)
  File &quot;C:\Python35\lib\site-packages\requests\sessions.py&quot;, line 488, in request
    resp = self.send(prep, **send_kwargs)
  File &quot;C:\Python35\lib\site-packages\requests\sessions.py&quot;, line 609, in send
    r = adapter.send(request, **kwargs)
  File &quot;C:\Python35\lib\site-packages\requests\adapters.py&quot;, line 485, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host=&apos;121.201.24.248：8088&apos;, port=80): Max retries exceeded with url: http://daily.zhihu.com/ (Caused by ProxyError(&apos;Cannot connect to proxy.&apos;, NewConnectionError(&apos;&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x0000000003860DA0&gt;: Failed to establish a new connection: [Errno 11004] getaddrinfo failed&apos;,)))
</code></pre><h1 id="3、模拟登录"><a href="#3、模拟登录" class="headerlink" title="3、模拟登录"></a>3、模拟登录</h1><p>有些网站是需要登录才能看到信息的，例如知乎，直接用requests获取知乎首页信息，返回数据是需要你登录的，只有登录了才能看到数据。</p>
<pre><code>&lt;button type=&quot;button&quot; class=&quot;signin-switch-button&quot;&gt;手机验证码登录&lt;/button&gt;

&lt;a class=&quot;unable-login&quot; href=&quot;#&quot;&gt;无法登录？&lt;/a&gt;
&lt;/div&gt;

&lt;div class=&quot;social-signup-wrapper&quot; data-za-module=&quot;SNSSignIn&quot;&gt;
&lt;span class=&quot;name js-toggle-sns-buttons&quot;&gt;社交帐号登录&lt;/span&gt;

&lt;div class=&quot;sns-buttons&quot;&gt;
&lt;a title=&quot;微信登录&quot; class=&quot;js-bindwechat&quot; href=&quot;#&quot;&gt;&lt;i class=&quot;sprite-index-icon-wechat&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a title=&quot;微博登录&quot; class=&quot;js-bindweibo&quot; href=&quot;#&quot;&gt;&lt;i class=&quot;sprite-index-icon-weibo&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a title=&quot;QQ 登录&quot; class=&quot;js-bindqq&quot; href=&quot;#&quot;&gt;&lt;i class=&quot;sprite-index-icon-qq&quot;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;/div&gt;
</code></pre><p>再次回到官方文档<a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html#cookie" target="_blank" rel="external">快速上手 - Requests 2.10.0 文档</a></p>
<pre><code>如果某个响应中包含一些 cookie，你可以快速访问它们：

&gt;&gt;&gt; url = &apos;http://example.com/some/cookie/setting/url&apos;
&gt;&gt;&gt; r = requests.get(url)

&gt;&gt;&gt; r.cookies[&apos;example_cookie_name&apos;]
&apos;example_cookie_value&apos;
要想发送你的cookies到服务器，可以使用 cookies 参数：

&gt;&gt;&gt; url = &apos;http://httpbin.org/cookies&apos;
&gt;&gt;&gt; cookies = dict(cookies_are=&apos;working&apos;)

&gt;&gt;&gt; r = requests.get(url, cookies=cookies)
&gt;&gt;&gt; r.text
&apos;{&quot;cookies&quot;: {&quot;cookies_are&quot;: &quot;working&quot;}}&apos;
</code></pre><p>具体的分析过程可以参考xchaoinfo所写的文章和视频，讲解十分清晰<a href="https://zhuanlan.zhihu.com/p/25633789" target="_blank" rel="external">Python 模拟登录哪些事儿 - 知乎专栏</a> 下面是代码</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import os, time
import re
# import http.cookiejar as cookielib

# 构造 Request headers
agent = &apos;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36&apos;
headers = {
    &quot;Host&quot;: &quot;www.zhihu.com&quot;,
    &quot;Referer&quot;: &quot;https://www.zhihu.com/&quot;,
    &apos;User-Agent&apos;: agent
}

######### 构造用于网络请求的session
session = requests.Session()
# session.cookies = cookielib.LWPCookieJar(filename=&apos;zhihucookie&apos;)
# try:
#     session.cookies.load(ignore_discard=True)
# except:
#     print(&apos;cookie 文件未能加载&apos;)

############ 获取xsrf_token
homeurl = &apos;https://www.zhihu.com&apos;
homeresponse = session.get(url=homeurl, headers=headers)
homesoup = BeautifulSoup(homeresponse.text, &apos;html.parser&apos;)
xsrfinput = homesoup.find(&apos;input&apos;, {&apos;name&apos;: &apos;_xsrf&apos;})
xsrf_token = xsrfinput[&apos;value&apos;]
print(&quot;获取到的xsrf_token为： &quot;, xsrf_token)

########## 获取验证码文件
randomtime = str(int(time.time() * 1000))
captchaurl = &apos;https://www.zhihu.com/captcha.gif?r=&apos;+\
             randomtime+&quot;&amp;type=login&quot;
captcharesponse = session.get(url=captchaurl, headers=headers)
with open(&apos;checkcode.gif&apos;, &apos;wb&apos;) as f:
    f.write(captcharesponse.content)
    f.close()
# os.startfile(&apos;checkcode.gif&apos;)
captcha = input(&apos;请输入验证码：&apos;)
print(captcha)

########### 开始登陆
headers[&apos;X-Xsrftoken&apos;] = xsrf_token
headers[&apos;X-Requested-With&apos;] = &apos;XMLHttpRequest&apos;
loginurl = &apos;https://www.zhihu.com/login/email&apos;
postdata = {
    &apos;_xsrf&apos;: xsrf_token,
    &apos;email&apos;: &apos;邮箱@qq.com&apos;,
    &apos;password&apos;: &apos;密码&apos;
}
loginresponse = session.post(url=loginurl, headers=headers, data=postdata)
print(&apos;服务器端返回响应码：&apos;, loginresponse.status_code)
print(loginresponse.json())
# 验证码问题输入导致失败: 猜测这个问题是由于session中对于验证码的请求过期导致
if loginresponse.json()[&apos;r&apos;]==1:
    # 重新输入验证码，再次运行代码则正常。也就是说可以再第一次不输入验证码，或者输入一个错误的验证码，只有第二次才是有效的
    randomtime = str(int(time.time() * 1000))
    captchaurl = &apos;https://www.zhihu.com/captcha.gif?r=&apos; + \
                 randomtime + &quot;&amp;type=login&quot;
    captcharesponse = session.get(url=captchaurl, headers=headers)
    with open(&apos;checkcode.gif&apos;, &apos;wb&apos;) as f:
        f.write(captcharesponse.content)
        f.close()
    os.startfile(&apos;checkcode.gif&apos;)
    captcha = input(&apos;请输入验证码：&apos;)
    print(captcha)

    postdata[&apos;captcha&apos;] = captcha
    loginresponse = session.post(url=loginurl, headers=headers, data=postdata)
    print(&apos;服务器端返回响应码：&apos;, loginresponse.status_code)
    print(loginresponse.json())




##########################保存登陆后的cookie信息
# session.cookies.save()
############################判断是否登录成功
profileurl = &apos;https://www.zhihu.com/settings/profile&apos;
profileresponse = session.get(url=profileurl, headers=headers)
print(&apos;profile页面响应码：&apos;, profileresponse.status_code)
profilesoup = BeautifulSoup(profileresponse.text, &apos;html.parser&apos;)
div = profilesoup.find(&apos;div&apos;, {&apos;id&apos;: &apos;rename-section&apos;})
print(div)
</code></pre><p>好了关于爬虫的第一步，获取源码这一节讲了很多，其实大多数网站加上User-Agent和代理IP就可以正常爬取。下一节会讲讲如何利用xpath来解析网页，获取我们想要的数据。</p>
<p>喜欢爬虫、数据的可以关注一下我的微信公众号，多多交流。</p>
<p><img src="http://i.imgur.com/0oZDwG4.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十三篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/Dw2WeLb.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;爬虫三步走：获取源码、解析源码、数据储存&lt;/p&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python入门" scheme="https://zhangslob.github.io/tags/Python%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>知乎上妹子都爱取啥名？</title>
    <link href="https://zhangslob.github.io/2017/05/20/%E7%9F%A5%E4%B9%8E%E4%B8%8A%E5%A6%B9%E5%AD%90%E9%83%BD%E7%88%B1%E5%8F%96%E5%95%A5%E5%90%8D%EF%BC%9F/"/>
    <id>https://zhangslob.github.io/2017/05/20/知乎上妹子都爱取啥名？/</id>
    <published>2017-05-20T09:45:48.000Z</published>
    <updated>2017-05-20T09:55:27.930Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十二篇原创文章
</code></pre><p><img src="http://i.imgur.com/g4LZEo8.png" alt=""></p>
<p>闲来无事上知乎，看到好多妹子，于是抓取一波。<br><a id="more"></a></p>
<blockquote>
<p>目标网址：<a href="https://www.zhihu.com/collection/78172986" target="_blank" rel="external">轮子哥带我逛知乎 - 收藏夹 - 知乎</a></p>
<p>目标：抓取妹子的ID</p>
</blockquote>
<h1 id="1、抓取"><a href="#1、抓取" class="headerlink" title="1、抓取"></a>1、抓取</h1><p>这次并没有使用Python做爬虫，使用工具来，速度更快。<a href="http://zaoshu.io/" target="_blank" rel="external">造数 - 新一代智能云爬虫</a></p>
<p><img src="http://i.imgur.com/ImOBVfA.png" alt=""></p>
<p>不到十分钟就完成了，遗失了部分数据，但是无所谓啦。保存下来，分析分析。</p>
<h1 id="2、分析"><a href="#2、分析" class="headerlink" title="2、分析"></a>2、分析</h1><p><img src="http://i.imgur.com/rTvn8QE.png" alt=""></p>
<p>使用pandas操作文件</p>
<pre><code>import pandas as pd
fp = pd.read_excel(&apos;D:\Backup\桌面\lunzige.xlsx&apos;)

fp
</code></pre><p><img src="http://i.imgur.com/pS8ZbGw.png" alt=""></p>
<pre><code>name = fp[&apos;name&apos;].tolist()
li1 = list(set(name))
li1

[&apos;阿蕾&apos;,
 &apos;杨面&apos;,
 &apos;陈10&apos;,
 &apos;杨顺顺&apos;,
 &apos;霧橤&apos;,
 &apos;真顺顺真&apos;,
 &apos;谢椿明&apos;,
 &apos;刀刀&apos;,
 &apos;水枪大帝&apos;,
 &apos;倾浅&apos;,
 &apos;Listening&apos;,
 &apos;小火龙&apos;,
 &apos;包子琛&apos;,
 &apos;杨笋笋&apos;,
 &apos;蜉蝣&apos;,
 &apos;十元&apos;,
 &apos;靡靡之音&apos;,
 &apos;Real机智张&apos;,
 &apos;陈梓小童鞋&apos;,
 &apos;花甲&apos;,
 &apos;窗里窗外&apos;,
 &apos;刘梓乔&apos;,
 &apos;璇璇97&apos;,
 &apos;Olivia菊香小姐姐&apos;,
 &apos;牛奶小夏目&apos;,
 &apos;周依宁&apos;,
 &apos;万阿咸&apos;,
 &apos;一蓑烟雨任平生&apos;,
 &apos;来都来了&apos;,
 &apos;就像周一&apos;,
 &apos;Mc蛋蛋&apos;,
 &apos;秉剑侯&apos;,
 &apos;李大梦Lee&apos;,
 &apos;Diss锐雯&apos;,
 &apos;雨音眞白&apos;,
 &apos;半仙幺幺&apos;,
 &apos;Natsuki是只蠢兔纸&apos;,
 &apos;夏冰莹&apos;,
 &apos;guuweihai&apos;,
 &apos;阿舞&apos;,
 &apos;肖柚妮&apos;,
 &apos;墨脱要开&apos;,
 &apos;芷珞&apos;,
 &apos;舒西婷&apos;,
 &apos;Childe0Q&apos;,
 &apos;被压扁的海螺&apos;,
 &apos;snow arc&apos;,
 &apos;灰灰灰灰灰plus&apos;,
 &apos;小兔子菲呀&apos;,
 &apos;士多啤梨羊咩咩&apos;,
 &apos;李小可可&apos;,
 &apos;谁来拽我的尾巴&apos;,
 &apos;飞鸽之舞&apos;,
 &apos;小美&apos;,
 &apos;樱雪绫sama&apos;,
 &apos;zshiyao&apos;,
 &apos;王漠里&apos;,
 &apos;Slivan&apos;,
 &apos;喵小虾&apos;,
 &apos;SUSAN苏&apos;,
 &apos;上官兰颜&apos;,
 &apos;这个杀手不太冷&apos;,
 &apos;看朱成碧纷思君&apos;,
 &apos;情绪&apos;,
 &apos;我系小忌廉&apos;,
 &apos;一只兔&apos;,
 &apos;June&apos;,
 &apos;我就想改名而已&apos;,
 &apos;温柔的大猫Leo&apos;,
 &apos;猫芙琳&apos;,
 &apos;以太&apos;,
 &apos;博丽魔理沙&apos;,
 &apos;洛丽塔&apos;,
 &apos;羽小团&apos;,
 &apos;娄良&apos;,
 &apos;Rosi&apos;,
 &apos;叶以北&apos;,
 &apos;吃不胖的小猫&apos;,
 &apos;Lina&apos;,
 &apos;ingrid&apos;,
 &apos;itttttx&apos;,
 &apos;胡杨&apos;,
 &apos;孙阿童&apos;,
 &apos;林美珍&apos;,
 &apos;赫蘿Taiga&apos;,
 &apos;宫曼曼&apos;,
 &apos;Yoonyicc&apos;,
 &apos;ZW711&apos;,
 &apos;笙箫&apos;,
 &apos;KIKI.Liu&apos;,
 &apos;另一只袜子&apos;,
 &apos;荒野大嫖客&apos;,
 &apos;少女诗&apos;,
 &apos;芸豆豆豆豆&apos;,
 &apos;璐璐噜&apos;,
 &apos;棹歌&apos;,
 &apos;梦里有只独角兽&apos;,
 &apos;Oo澄子oO&apos;,
 &apos;雷梅苔丝&apos;,
 &apos;CherryZhao&apos;,
 &apos;李萬一&apos;,
 &apos;琴脂&apos;,
 &apos;鹿斑比&apos;,
 &apos;Chris姬-云烟&apos;,
 &apos;hyoram&apos;,
 &apos;蔗蔗蔗&apos;,
 &apos;柚子Ruby&apos;,
 &apos;Sheena&apos;,
 &apos;孟德尔&apos;,
 &apos;kaka小师妹&apos;,
 &apos;桢视明&apos;,
 &apos;大豆苗&apos;,
 &apos;少女开膛手&apos;,
 &apos;陈诗茗&apos;]
</code></pre><p>那么，下一步就是对名字进行分词了，jieba分词，你值得拥有。<a href="https://github.com/fxsjy/jieba" target="_blank" rel="external">fxsjy/jieba</a></p>
<pre><code>li2 = &apos;&apos;.join(li1)
li2

&apos;阿蕾杨面陈10杨顺顺霧橤真顺顺真谢椿明刀刀水枪大帝倾浅Listening小火龙包子琛杨笋笋蜉蝣十元靡靡之音Real机智张陈梓小童鞋花甲窗里窗外刘梓乔璇璇97Olivia菊香小姐姐牛奶小夏目周依宁万阿咸一蓑烟雨任平生来都来了就像周一Mc蛋蛋秉剑侯李大梦LeeDiss锐雯雨音眞白半仙幺幺Natsuki是只蠢兔纸夏冰莹guuweihai阿舞肖柚妮墨脱要开芷珞舒西婷Childe0Q被压扁的海螺snow arc灰灰灰灰灰plus小兔子菲呀士多啤梨羊咩咩李小可可谁来拽我的尾巴飞鸽之舞小美樱雪绫samazshiyao王漠里Slivan喵小虾SUSAN苏上官兰颜这个杀手不太冷看朱成碧纷思君情绪我系小忌廉一只兔June我就想改名而已温柔的大猫Leo猫芙琳以太博丽魔理沙洛丽塔羽小团娄良Rosi叶以北吃不胖的小猫Linaingriditttttx胡杨孙阿童林美珍赫蘿Taiga宫曼曼YoonyiccZW711笙箫KIKI.Liu另一只袜子荒野大嫖客少女诗芸豆豆豆豆璐璐噜棹歌梦里有只独角兽Oo澄子oO雷梅苔丝CherryZhao李萬一琴脂鹿斑比Chris姬-云烟hyoram蔗蔗蔗柚子RubySheena孟德尔kaka小师妹桢视明大豆苗少女开膛手陈诗茗&apos;
</code></pre><p>有何感想？？</p>
<p><img src="http://i.imgur.com/F7G5etA.jpg" alt=""></p>
<p>下一步就是分词制作图云了</p>
<pre><code>import jieba
seg_list = jieba.cut(li2)
word = &quot;/&quot;.join(seg_list)
print(&quot;Full Mode: &quot; + &quot;/ &quot;.join(seg_list)) 

Building prefix dict from the default dictionary ...
Dumping model to file cache C:\Users\ADMINI~1\AppData\Local\Temp\jieba.cache
Loading model cost 1.148 seconds.
Prefix dict has been built succesfully.
Full Mode: 阿蕾/ 杨/ 面陈/ 10/ 杨/ 顺顺/ 霧/ 橤/ 真/ 顺顺/ 真/ 谢椿明/ 刀刀/ 水枪/ 大帝/ 倾浅/ Listening/ 小/ 火龙/ 包子/ 琛/ 杨笋/ 笋/ 蜉蝣/ 十元/ 靡靡之音/ Real/ 机智/ 张/ 陈梓/ 小/ 童鞋/ 花甲/ 窗里/ 窗外/ 刘梓乔/ 璇/ 璇/ 97Olivia/ 菊香/ 小姐姐/ 牛奶/ 小夏目/ 周依宁/ 万/ 阿/ 咸一/ 蓑/ 烟雨任/ 平生/ 来/ 都/ 来/ 了/ 就/ 像/ 周一/ Mc/ 蛋蛋/ 秉剑侯/ 李大梦/ LeeDiss/ 锐雯雨/ 音眞白/ 半仙/ 幺/ 幺/ Natsuki/ 是/ 只/ 蠢/ 兔纸/ 夏/ 冰莹/ guuweihai/ 阿舞/ 肖柚妮/ 墨脱/ 要/ 开芷/ 珞/ 舒西婷/ Childe0Q/ 被/ 压扁/ 的/ 海螺/ snow/  / arc/ 灰灰/ 灰灰/ 灰/ plus/ 小兔子/ 菲/ 呀/ 士多啤梨/ 羊/ 咩/ 咩/ 李小/ 可可/ 谁/ 来/ 拽/ 我/ 的/ 尾巴/ 飞鸽/ 之舞/ 小美/ 樱雪/ 绫/ samazshiyao/ 王漠/ 里/ Slivan/ 喵/ 小虾/ SUSAN/ 苏/ 上官/ 兰颜/ 这个/ 杀手/ 不/ 太冷/ 看朱成碧/ 纷思君/ 情绪/ 我系/ 小忌廉/ 一只/ 兔/ June/ 我/ 就/ 想/ 改名/ 而已/ 温柔/ 的/ 大猫/ Leo/ 猫/ 芙琳/ 以太/ 博丽/ 魔理沙/ 洛丽塔/ 羽小团/ 娄良/ Rosi/ 叶/ 以北/ 吃不胖/ 的/ 小猫/ Linaingriditttttx/ 胡杨/ 孙阿童/ 林美珍/ 赫蘿/ Taiga/ 宫曼曼/ YoonyiccZW711/ 笙箫/ KIKI/ ./ Liu/ 另一只/ 袜子/ 荒野/ 大/ 嫖客/ 少女/ 诗/ 芸豆/ 豆豆/ 豆璐璐噜/ 棹/ 歌梦里/ 有/ 只/ 独角兽/ Oo/ 澄子/ oO/ 雷梅/ 苔丝/ CherryZhao/ 李萬/ 一琴脂/ 鹿斑/ 比/ Chris/ 姬/ -/ 云烟/ hyoram/ 蔗蔗蔗/ 柚子/ RubySheena/ 孟德尔/ kaka/ 小/ 师妹/ 桢视/ 明大/ 豆苗/ 少女/ 开膛手/ 陈诗/ 茗
</code></pre><p>下一步绘制图云，用jupyter遇到了很多坑。。</p>
<pre><code># -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator

# 直接从文件读取数据

text = &apos;&apos;&apos;阿蕾/杨/面陈/10/杨/顺顺/霧/橤/真/顺顺/真/谢椿明/刀刀/水枪/大帝/倾浅/Listening/小/火龙/包子/琛/杨笋/笋/蜉蝣/十元/靡靡之音/Real/机智/张/陈梓/小/童鞋/花甲/窗里/窗外/刘梓乔/璇/璇/97Olivia/菊香/小姐姐/牛奶/小夏目/周依宁/万/阿/咸一/蓑/烟雨任/平生/来/都/来/了/就/像/周一/Mc/蛋蛋/秉剑侯/李大梦/LeeDiss/锐雯雨/音眞白/半仙/幺/幺/Natsuki/是/只/蠢/兔纸/夏/冰莹/guuweihai/阿舞/肖柚妮/墨脱/要/开芷/珞/舒西婷/Childe0Q/被/压扁/的/海螺/snow/ /arc/灰灰/灰灰/灰/plus/小兔子/菲/呀/士多啤梨/羊/咩/咩/李小/可可/谁/来/拽/我/的/尾巴/飞鸽/之舞/小美/樱雪/绫/samazshiyao/王漠/里/Slivan/喵/小虾/SUSAN/苏/上官/兰颜/这个/杀手/不/太冷/看朱成碧/纷思君/情绪/我系/小忌廉/一只/兔/June/我/就/想/改名/而已/温柔/的/大猫/Leo/猫/芙琳/以太/博丽/魔理沙/洛丽塔/羽小团/娄良/Rosi/叶/以北/吃不胖/的/小猫/Linaingriditttttx/胡杨/孙阿童/林美珍/赫蘿/Taiga/宫曼曼/YoonyiccZW711/笙箫/KIKI/./Liu/另一只/袜子/荒野/大/嫖客/少女/诗/芸豆/豆豆/豆璐璐噜/棹/歌梦里/有/只/独角兽/Oo/澄子/oO/雷梅/苔丝/CherryZhao/李萬/一琴脂/鹿斑/比/Chris/姬/-/云烟/hyoram/蔗蔗蔗/柚子/RubySheena/孟德尔/kaka/小/师妹/桢视/明大/豆苗/少女/开膛手/陈诗/茗&apos;&apos;&apos;

backgroud_Image = plt.imread(&apos;girl.jpg&apos;)
wc = WordCloud( background_color = &apos;white&apos;,    # 设置背景颜色
                mask = backgroud_Image,        # 设置背景图片
                max_words = 2000,            # 设置最大现实的字数
                stopwords = STOPWORDS,        # 设置停用词
                font_path = &apos;C:/Users/Windows/fonts/msyh.ttf&apos;,# 设置字体格式，如不设置显示不了中文
                max_font_size = 300,            # 设置字体最大值
                random_state = 50,            # 设置有多少种随机生成状态，即有多少种配色方案
                )
wc.generate(text)
image_colors = ImageColorGenerator(backgroud_Image)
#wc.recolor(color_func = image_colors)
plt.imshow(wc)
plt.axis(&apos;off&apos;)
plt.show()
</code></pre><p>来看看图云吧</p>
<p><img src="http://i.imgur.com/tXgcb6g.png" alt=""></p>
<p><img src="http://i.imgur.com/CuFdg9z.png" alt=""></p>
<p>其实这个并没有什么卵用，知识自己无聊时玩玩的。下面才是我想要的</p>
<pre><code>0    陈诗茗    https://www.zhihu.com/people/chen-shi-ming-69
1    李大梦Lee    https://www.zhihu.com/people/li-da-meng-58-44
2    snow arc    https://www.zhihu.com/people/xiaoxueli
3    夏冰莹    https://www.zhihu.com/people/xia-bingying
4    Sheena    https://www.zhihu.com/people/zhang-chu-yun-84
5    喵小虾    https://www.zhihu.com/people/maoxiaoxia233
6    李大梦Lee    https://www.zhihu.com/people/li-da-meng-58-44
7    李大梦Lee    https://www.zhihu.com/people/li-da-meng-58-44
8    以太    https://www.zhihu.com/people/elapse08
9    zshiyao    https://www.zhihu.com/people/duo-rou-wan-zi-89
10    SUSAN苏    https://www.zhihu.com/people/susansu-66
11    温柔的大猫Leo    https://www.zhihu.com/people/li-yue-90-56
12    琴脂    https://www.zhihu.com/people/qin-zhi-49
13    王漠里    https://www.zhihu.com/people/wang-mo-li-66
14    花甲    https://www.zhihu.com/people/hua-jia-1-71
15    雷梅苔丝    https://www.zhihu.com/people/lei-mei-tai-si-15
16    Olivia菊香小姐姐    https://www.zhihu.com/people/olivia-60-10
17    芷珞    https://www.zhihu.com/people/zhi-luo-90-6
18    Mc蛋蛋    https://www.zhihu.com/people/lee2egg
19    少女诗    https://www.zhihu.com/people/shao-nu-shi-75
20    ingrid    https://www.zhihu.com/people/da-da-yao-guai
21    博丽魔理沙    https://www.zhihu.com/people/nan-xiao-niao-94-5
22    赫蘿Taiga    https://www.zhihu.com/people/he-luo-taiga
23    kaka小师妹    https://www.zhihu.com/people/kakasis
24    芸豆豆豆豆    https://www.zhihu.com/people/yun-dou-dou-dou-dou
25    林美珍    https://www.zhihu.com/people/lin-mei-zhen
26    喵小虾    https://www.zhihu.com/people/maoxiaoxia233
27    这个杀手不太冷    https://www.zhihu.com/people/wei-jun-jie-9
28    喵小虾    https://www.zhihu.com/people/maoxiaoxia233
29    Rosi    https://www.zhihu.com/people/rosi-91
...    ...    ...
111    洛丽塔    https://www.zhihu.com/people/hua-hua-gu-niang-5
112    洛丽塔    https://www.zhihu.com/people/hua-hua-gu-niang-5
113    洛丽塔    https://www.zhihu.com/people/hua-hua-gu-niang-5
114    洛丽塔    https://www.zhihu.com/people/hua-hua-gu-niang-5
115    Diss锐雯    https://www.zhihu.com/people/DSRiven
116    水枪大帝    https://www.zhihu.com/people/shuiqiangge
117    樱雪绫sama    https://www.zhihu.com/people/lin-xuan-ting-1
118    李小可可    https://www.zhihu.com/people/li-gao-xing-2
119    士多啤梨羊咩咩    https://www.zhihu.com/people/shi-duo-pi-li-yan...
120    李萬一    https://www.zhihu.com/people/moire
121    万阿咸    https://www.zhihu.com/people/wan-a-xian-58
122    笙箫    https://www.zhihu.com/people/sheng-xiao-36
123    谢椿明    https://www.zhihu.com/people/xie-chun-ming-16
124    孙阿童    https://www.zhihu.com/people/sun-a-tong
125    宫曼曼    https://www.zhihu.com/people/gong-nuo-6
126    荒野大嫖客    https://www.zhihu.com/people/ji-da-fa-37
127    我就想改名而已    https://www.zhihu.com/people/wowjessica
128    就像周一    https://www.zhihu.com/people/yin-qing-chu-kai
129    胡杨    https://www.zhihu.com/people/hu-yang-49-22
130    杨笋笋    https://www.zhihu.com/people/yang-sun-sun-98
131    蜉蝣    https://www.zhihu.com/people/yuan-xia-66
132    羽小团    https://www.zhihu.com/people/xiao-yu-bao-er
133    杨笋笋    https://www.zhihu.com/people/yang-sun-sun-98
134    Lina    https://www.zhihu.com/people/li-nuo-84-28
135    另一只袜子    https://www.zhihu.com/people/151231
136    刘梓乔    https://www.zhihu.com/people/liu-zi-qiao-42
137    guuweihai    https://www.zhihu.com/people/guuweihai
138    陈10    https://www.zhihu.com/people/chen-10-80
139    ZW711    https://www.zhihu.com/people/zw711
140    看朱成碧纷思君    https://www.zhihu.com/people/kan-zhu-cheng-bi-...
</code></pre><p>下周二晚上8点，我会在趣直播聊一聊Python爬虫，如果你感兴趣，欢迎你来参加。</p>
<p><a href="http://m.quzhiboapp.com/?#!/intro/522?liveId=522" target="_blank" rel="external">趣直播 - 知识直播平台</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十二篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/g4LZEo8.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;闲来无事上知乎，看到好多妹子，于是抓取一波。&lt;br&gt;
    
    </summary>
    
    
      <category term="知乎" scheme="https://zhangslob.github.io/tags/%E7%9F%A5%E4%B9%8E/"/>
    
      <category term="妹子" scheme="https://zhangslob.github.io/tags/%E5%A6%B9%E5%AD%90/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode_4.Median of Two Sorted Arrays</title>
    <link href="https://zhangslob.github.io/2017/05/19/Leetcode-4-Median-of-Two-Sorted-Arrays/"/>
    <id>https://zhangslob.github.io/2017/05/19/Leetcode-4-Median-of-Two-Sorted-Arrays/</id>
    <published>2017-05-19T14:03:18.147Z</published>
    <updated>2017-05-19T15:07:32.244Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十一篇原创文章
</code></pre><p><img src="http://i.imgur.com/67W64sr.png" alt=""></p>
<p>参考<a href="https://my.oschina.net/u/1994258/blog/303757" target="_blank" rel="external">Median of Two Sorted Arrays</a><br><a id="more"></a></p>
<h1 id="1、题目"><a href="#1、题目" class="headerlink" title="1、题目"></a>1、题目</h1><p>There are two sorted arrays nums1 and nums2 of size m and n respectively.</p>
<p>Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)).</p>
<p>Example 1:</p>
<pre><code>nums1 = [1, 3]
nums2 = [2]

The median is 2.0
</code></pre><p>Example 2:</p>
<pre><code>nums1 = [1, 2]
nums2 = [3, 4]

The median is (2 + 3)/2 = 2.5
</code></pre><h1 id="2、思路"><a href="#2、思路" class="headerlink" title="2、思路"></a>2、思路</h1><p>蛋疼的说，我有看不懂题目了，尤其是<code>O(log (m+n))</code>，啥玩意。没办法，去网上搜索，看看前辈们的想法。</p>
<p>翻译如下：</p>
<blockquote>
<p>给你两个排序数组，容量为m的数组A，容量为n的数组B。求出两个数组的中位数（啥玩意？），硬性要求时间复杂度O(log (m+n)).</p>
<p>1：太汗颜了，median到底是个啥，查一下：</p>
<p>中位数是在一组数据中居于中间的数(特别注意的地方是：这组数据之前已经经过升序排列！！！)，即在这组数据中，有一半的数据比它大，有一半的数据比它小。如果这组数据包含偶数个数字，中值是位于中间的两个数的平均值。</p>
<p>2：好吧，中位数是这么个玩意，那么理论上首先我们需要先将两个数组合为一，再求这个新合并的数组的中位数。</p>
<p>3：但是，已经限定死了时间复杂度为log（m+n），原来LeetCode的题目也思路不开放嘛。</p>
<p>4：问题可以转化成两个有序序列找第num大的数，由于时间复杂度已经限定死了，只能采用类似二分的思想，每个步骤去掉一半数据元素。</p>
</blockquote>
<p>出现了一个词语：<strong>时间复杂度</strong>，这是个啥？</p>
<p><img src="http://i.imgur.com/NTEILO2.jpg" alt=""></p>
<p>完全不懂，换个思路来吧，不去看题目了，直接看第四点：</p>
<blockquote>
<p>问题可以转化成两个有序序列找第num大的数，由于时间复杂度已经限定死了，只能采用类似二分的思想，每个步骤去掉一半数据元素。</p>
</blockquote>
<p>二分，又是二分，赶紧去复习下。</p>
<blockquote>
<p>二分查找就是将查找的键和子数组的中间键作比较，如果被查找的键小于中间键，就在左子数组继续查找；如果大于中间键，就在右子数组中查找，否则中间键就是要找的元素。</p>
</blockquote>
<p>这个好像还可以看得懂，嘿嘿。我还发现了Python源代码（百度这样说的）：</p>
<pre><code>def bin_search(data_list, val):    
    low = 0                         # 最小数下标    
    high = len(data_list) - 1       # 最大数下标    
    while low &lt;= high:        
        mid = (low + high) // 2     # 中间数下标        
        if data_list[mid] == val:   # 如果中间数下标等于val, 返回            
            return mid        
        elif data_list[mid] &gt; val:  # 如果val在中间数左边, 移动high下标            
            high = mid - 1        
        else:                       # 如果val在中间数右边, 移动low下标            
            low = mid + 1    
     return # val不存在, 返回None
ret = bin_search(list(range(1, 10)), 3)
print(ret)
</code></pre><p>大概明白他的意思了。</p>
<h1 id="3、解法"><a href="#3、解法" class="headerlink" title="3、解法"></a>3、解法</h1><p>很多解法都提到：<code>如果我们可以在两个数列中求出第K小的元素，便可以解决该问题</code></p>
<blockquote>
<p>解题思路：这道题要求两个已经排好序的数列的中位数。中位数的定义：如果数列有偶数个数，那么中位数为中间两个数的平均值；如果数列有奇数个数，那么中位数为中间的那个数。比如{1，2，3，4，5}的中位数为3。{1，2，3，4，5，6}的中位数为（3+4）/ 2 = 3.5。那么这题最直接的思路就是将两个数列合并在一起，然后排序，然后找到中位数就行了。可是这样最快也要O((m+n)log(m+n))的时间复杂度，而题目要求O(log(m+n))的时间复杂度。这道题其实考察的是二分查找，是《算法导论》的一道课后习题，难度还是比较大的。</p>
<p>首先我们来看如何找到两个数列的第k小个数，即程序中getKth(A, B , k)函数的实现。用一个例子来说明这个问题：A = {1，3，5，7}；B = {2，4，6，8，9，10}；如果要求第7个小的数，A数列的元素个数为4，B数列的元素个数为6；k/2 = 7/2 = 3，而A中的第3个数A[2]=5；B中的第3个数B[2]=6；而A[2]&lt;B[2]；则A[0]，A[1]，A[2]中必然不可能有第7个小的数。因为A[2]&lt;B[2]，所以比A[2]小的数最多可能为A[0], A[1], B[0], B[1]这四个数，也就是说A[2]最多可能是第5个大的数，由于我们要求的是getKth(A, B, 7)；现在就变成了求getKth(A’, B, 4)；即A’ = {7}；B不变，求这两个数列的第4个小的数，因为A[0]，A[1]，A[2]中没有解，所以我们直接删掉它们就可以了。这个可以使用递归来实现。</p>
</blockquote>
<pre><code>class Solution:
    # @return a float
    # @line20 must multiply 0.5 for return a float else it will return an int
    def getKth(self, A, B, k):
        lenA = len(A); lenB = len(B)
        if lenA &gt; lenB: return self.getKth(B, A, k)
        if lenA == 0: return B[k - 1]
        if k == 1: return min(A[0], B[0])
        pa = min(k/2, lenA); pb = k - pa
        if A[pa - 1] &lt;= B[pb - 1]:
            return self.getKth(A[pa:], B, pb)
        else:
            return self.getKth(A, B[pb:], pa)

    def findMedianSortedArrays(self, A, B):
        lenA = len(A); lenB = len(B)
        if (lenA + lenB) % 2 == 1: 
            return self.getKth(A, B, (lenA + lenB)/2 + 1)
        else:
            return (self.getKth(A, B, (lenA + lenB)/2) + self.getKth(A, B, (lenA + lenB)/2 + 1)) * 0.5
</code></pre><p>在我提交了代码之后，发现超过50.24 % </p>
<p><img src="http://i.imgur.com/COEyBaw.png" alt=""></p>
<p>我找出最快的解法，来学习下：</p>
<pre><code>class Solution(object):
    def findMedianSortedArrays(self, a, b):
        &quot;&quot;&quot;
        :type nums1: List[int]
        :type nums2: List[int]
        :rtype: float
        &quot;&quot;&quot;
        c = a+b
        c.sort()
        m = len(c) / 2 
        mm = len(c) % 2
        if mm &gt; 0 :
            return c[m]
        return (c[m-1]+c[m])/2.# + (c[m-1]+c[m])%2
</code></pre><p>第二名的：</p>
<pre><code>class Solution(object):
    def findMedianSortedArrays(self, nums1, nums2):
        &quot;&quot;&quot;
        :type nums1: List[int]
        :type nums2: List[int]
        :rtype: float
        &quot;&quot;&quot;
        nums3 = nums1 + nums2
        nums3.sort()
        l = len(nums3)
        if l%2 == 1:
            return nums3[l/2]
        else:
            return (float(nums3[l/2]) + float(nums3[l/2-1]))/2
</code></pre><p><img src="http://i.imgur.com/rcobwfC.jpg" alt=""></p>
<p>每做一题，都会被打击好多次，但是算法是一定要学习的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十一篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/67W64sr.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;参考&lt;a href=&quot;https://my.oschina.net/u/1994258/blog/303757&quot;&gt;Median of Two Sorted Arrays&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Leetcode_2. Add Two Numbers</title>
    <link href="https://zhangslob.github.io/2017/05/14/Leetcode-2-Add-Two-Numbers/"/>
    <id>https://zhangslob.github.io/2017/05/14/Leetcode-2-Add-Two-Numbers/</id>
    <published>2017-05-14T08:42:56.000Z</published>
    <updated>2017-05-14T11:54:42.330Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十篇原创文章
</code></pre><p><img src="http://i.imgur.com/yjs0pvB.jpg" alt=""></p>
<p>好久没写博客了，定个小目标，2天一篇，哈哈。<br><a id="more"></a></p>
<h1 id="1、题目"><a href="#1、题目" class="headerlink" title="1、题目"></a>1、题目</h1><p>You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.</p>
<p>You may assume the two numbers do not contain any leading zero, except the number 0 itself.</p>
<pre><code>Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)
Output: 7 -&gt; 0 -&gt; 8
</code></pre><h1 id="2、思路"><a href="#2、思路" class="headerlink" title="2、思路"></a>2、思路</h1><p>其实我并看不懂题目说得啥意思，如果说<code>243+564</code>，结果是807才是，但是很明显题目不会这么简单。怎么办？</p>
<p><img src="http://i.imgur.com/qu9NlE1.jpg" alt=""></p>
<p>好像只能去看看别人的理解了，发现：</p>
<pre><code>1.因为存储是反过来的，即数字342存成2-&gt;4-&gt;3，所以要注意进位是向后的；
2.链表l1或l2为空时，直接返回，这是边界条件，省掉多余的操作；
3.链表l1和l2长度可能不同，因此要注意处理某个链表剩余的高位；
4.2个数相加，可能会产生最高位的进位，因此要注意在完成以上1－3的操作后，判断进位是否为0，不为0则需要增加结点存储最高位的进位。
</code></pre><p>给个链接<a href="http://blog.csdn.net/zhouworld16/article/details/14045855" target="_blank" rel="external">http://blog.csdn.net/zhouworld16/article/details/14045855</a></p>
<p>原来是倒着相加，<code>342+465=807</code>，结果倒序，正好是708，题目应该是这样理解的吧，哈哈。</p>
<p><img src="http://i.imgur.com/9t7Zryr.jpg" alt=""></p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><pre><code>本题的思路很简单，按照小学数学中学习的加法原理从末尾到首位，对每一位对齐相加即可。技巧在于如何处理不同长度的数字，以及进位和最高位的判断。这里对于不同长度的数字，我们通过将较短的数字补0来保证每一位都能相加。递归写法的思路比较直接，即判断该轮递归中两个ListNode是否为null。

全部为null时，返回进位值
有一个为null时，返回不为null的那个ListNode和进位相加的值
都不为null时，返回 两个ListNode和进位相加的值
</code></pre><p>来源一位大佬<a href="https://segmentfault.com/a/1190000002986101" target="_blank" rel="external">https://segmentfault.com/a/1190000002986101</a></p>
<p>原来是小学内容<code>小学数学中学习的加法</code>，使用递归写法。</p>
<p><strong>递归，就是在运行的过程中调用自己。</strong><br><img src="http://i.imgur.com/fsDv3Pw.jpg" alt=""><br>什么鬼啊，自己使用自己吗？</p>
<pre><code>目前我找到的对递归最恰当的比喻，就是查词典。
我们使用的词典，本身就是递归，为了解释一个词，需要使用更多的词。
当你查一个词，发现这个词的解释中某个词仍然不懂，于是你开始查这第二个词，可惜，第二个词里仍然有不懂的词，于是查第三个词，这样查下去，直到有一个词的解释是你完全能看懂的，那么递归走到了尽头，然后你开始后退，逐个明白之前查过的每一个词，最终，你明白了最开始那个词的意思。。。
</code></pre><p>好像明白了一点点~</p>
<h1 id="3、解法-（Python）"><a href="#3、解法-（Python）" class="headerlink" title="3、解法 #（Python）"></a>3、解法 #（Python）</h1><p><strong>版本1</strong></p>
<pre><code>class Solution:
    def addTwoNumbers(self, l1, l2):
        addends = l1, l2
        dummy = end = ListNode(0)
        carry = 0
        while addends or carry:
            carry += sum(a.val for a in addends)
            addends = [a.next for a in addends if a.next]
            end.next = end = ListNode(carry % 10)
            carry /= 10
        return dummy.next
</code></pre><p><strong>版本2</strong></p>
<pre><code>class Solution:
    # @return a ListNode
    def addTwoNumbers(self, l1, l2):
        carry = 0
        sum = ListNode(0)
        s = sum
        while l1 is not None or l2 is not None or carry:
            s.val = carry
            if l1:
                s.val += l1.val
                l1 = l1.next
            if l2:
                s.val += l2.val
                l2 = l2.next
            carry = s.val / 10
            s.val = s.val % 10
            if l1 or l2 or carry:
                s.next = ListNode(0)
                s = s.next
        return sum
</code></pre><p>好吧，承认版本1看不懂。至于版本2 <code>ListNode</code> 是一个元祖，然后。。。<code>s.val</code>是什么意思啊？又不懂。回去看看，原来是已经定义了<code>ListNode</code></p>
<pre><code># Definition for singly-linked list.
# class ListNode(object):
#     def __init__(self, x):
#         self.val = x
#         self.next = None
</code></pre><p>也就是说有2个类，<code>Solution</code> 和 <code>ListNode</code></p>
<hr>
<p>今天先这样了，我需要去学习一下什么是“类”，也就是Python的<strong>面向对象编程</strong></p>
<p><img src="http://i.imgur.com/xldgxuX.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/yjs0pvB.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;好久没写博客了，定个小目标，2天一篇，哈哈。&lt;br&gt;
    
    </summary>
    
    
      <category term="leetcode" scheme="https://zhangslob.github.io/tags/leetcode/"/>
    
      <category term="刷题" scheme="https://zhangslob.github.io/tags/%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>使用Python计算文章中的字词频率丨学习笔记和反思</title>
    <link href="https://zhangslob.github.io/2017/03/28/%E4%BD%BF%E7%94%A8Python%E8%AE%A1%E7%AE%97%E6%96%87%E7%AB%A0%E4%B8%AD%E7%9A%84%E5%AD%97%E8%AF%8D%E9%A2%91%E7%8E%87%E4%B8%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%92%8C%E5%8F%8D%E6%80%9D/"/>
    <id>https://zhangslob.github.io/2017/03/28/使用Python计算文章中的字词频率丨学习笔记和反思/</id>
    <published>2017-03-28T10:58:18.000Z</published>
    <updated>2017-03-28T11:18:39.332Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第九篇原创文章
</code></pre><p><img src="http://i.imgur.com/zd3tHnD.png" alt=""><br><a id="more"></a></p>
<blockquote>
<p>来源：天善智能-商业智能和大数据在线社区，用心创造价值<a href="https://edu.hellobi.com/course/159/play/lesson/2531" target="_blank" rel="external">https://edu.hellobi.com/course/159/play/lesson/2531</a></p>
<p>丘祐玮<a href="https://ask.hellobi.com/people/DavidChiu" target="_blank" rel="external">https://ask.hellobi.com/people/DavidChiu</a>人人都爱数据科学家！Python数据科学精华实战课程</p>
</blockquote>
<p>环境：Anaconda3</p>
<p>建议使用Anaconda，下载源文件后再阅读本文：<a href="https://github.com/zhangslob/DanmuFenxi" target="_blank" rel="external">https://github.com/zhangslob/DanmuFenxi</a></p>
<p>选择经典演讲稿，奥巴马2009年9月8日开学演讲。。<a href="https://wenku.baidu.com/view/ad77bc1caf45b307e8719758.html" target="_blank" rel="external">https://wenku.baidu.com/view/ad77bc1caf45b307e8719758.html</a></p>
<blockquote>
<p>THE PRESIDENT:</p>
<p> Hello, everybody! Thank you. Thank you. Thank you, everybody. All right, everybody go ahead and have a seat. How is everybody doing today? (Applause.) How about Tim Spicer? (Applause.) I am here with students at Wakefield High School in Arlington, Virginia. And we’ve got students tuning in from all across America, from kindergarten through 12th grade. And I am just so glad that all could join us today. And I want to thank Wakefield for being such an outstanding host. Give yourselves a big round of applause. (Applause.)<br>…</p>
</blockquote>
<h1 id="1、World-Count-Version-1"><a href="#1、World-Count-Version-1" class="headerlink" title="1、World Count(Version 1)"></a>1、World Count(Version 1)</h1><p>把数据命名为speech_text，首先需要对英文进行分词。英文中主要是空格，使用split()函数</p>
<pre><code># coding: utf-8

# In[1]:

speech_text=&apos;&apos;&apos;#长文本使用&apos;&apos;&apos;..&apos;&apos;&apos;
THE PRESIDENT:

 Hello, everybody! Thank you. Thank you. Thank you, everybody. All right, everybody go ahead and have a seat. How is everybody doing today? (Applause.) How about Tim Spicer? (Applause.) I am here with students at Wakefield High School in Arlington, Virginia. And we&apos;ve got students tuning in from all across America, from kindergarten through 12th grade. And I am just so glad that all could join us today. And I want to thank Wakefield for being such an outstanding host. Give yourselves a big round of applause. (Applause.)
 ...#省略文字
&apos;&apos;&apos;
# In[2]:

speech=speech_text.split()


# In[3]:

speech
</code></pre><p><img src="http://i.imgur.com/hU6Dvhc.png" alt=""></p>
<p>下一步，计算speech中词语出现的次数</p>
<pre><code># In[4]:

dic={}
for word in speech:
    if word not in dic:
        dic[word] = 1
    else:
        dic[word] = dic[word] + 1


# In[5]:

dic
</code></pre><p><img src="http://i.imgur.com/MyFbn3d.png" alt=""></p>
<p>通过 items() 函数以列表返回可遍历的(键, 值) 元组数组。</p>
<p><img src="http://i.imgur.com/41PnERl.png" alt=""></p>
<p>下一步，对词语进行排序</p>
<pre><code># In[7]:

import operator
swd=sorted(dic.items(),key=operator.itemgetter(1),reverse=True)#从大到小排序


# In[9]:

swd
</code></pre><p><img src="http://i.imgur.com/NakQWW0.png" alt=""></p>
<p>发现其中“to”、“the”等单词是常见词，借用nltk我们可以把这些词语去掉</p>
<pre><code>from nltk.corpus import stopwords
stop_words = stopwords.words(&apos;English&apos;)
</code></pre><p>虽说Anaconda已经安装了NLTK，但是我自己操作时stopwords貌似没有，出错请参考<a href="https://www.douban.com/note/534906136/" target="_blank" rel="external">https://www.douban.com/note/534906136/</a></p>
<p><img src="http://i.imgur.com/jOK5aa1.png" alt=""></p>
<p>看看英文中的去停词，下一步，遍历，打印出不含有去停词</p>
<pre><code>for k,v in swd2:
    if k not in stop_words:
        print(k,v)
</code></pre><p><img src="http://i.imgur.com/C5ER07f.png" alt=""></p>
<p>发现出现了很多“–”，回去原文中观察，发现确实有很多，</p>
<p>那么问题来了，为什么出现这么多“–”。萌新求解！</p>
<h1 id="2、World-Count-Version-2"><a href="#2、World-Count-Version-2" class="headerlink" title="2、World Count(Version 2)"></a>2、World Count(Version 2)</h1><pre><code>from collections import Counter
c=Counter(speech2)
</code></pre><p>使用Python 的collections模块更简洁，详细见<a href="http://www.jb51.net/article/48771.htm" target="_blank" rel="external">http://www.jb51.net/article/48771.htm</a></p>
<p><img src="http://i.imgur.com/6uZzcVV.png" alt=""></p>
<p>同样可以使用stop_word，还可以使用most_common打印出前几个</p>
<pre><code>for sw in stop_words:
    del c[sw]
</code></pre><p><img src="http://i.imgur.com/uV3TTwA.png" alt=""></p>
<h1 id="3、反思"><a href="#3、反思" class="headerlink" title="3、反思"></a>3、反思</h1><p>上一篇文章<a href="https://zhuanlan.zhihu.com/p/25983014" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25983014</a>写的比较粗糙，很多人要求把“观众” “礼物”筛选出来，那我来试试。</p>
<pre><code>stop = [&apos;！&apos;,&apos;*&apos;,&apos;观众&apos;,&apos;礼物&apos;,&apos;:&apos;,&apos;？&apos;,&apos;。&apos;,&apos;，&apos;,&apos;~&apos;,&apos;1&apos;]
</code></pre><p>去停词只有这些、可以根据实际情况添删。</p>
<p>看来观众很喜欢说“xx学院发来贺电~~”</p>
<p><img src="http://i.imgur.com/DKOE9Q5.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第九篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/zd3tHnD.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="学习笔记" scheme="https://zhangslob.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="文字处理" scheme="https://zhangslob.github.io/tags/%E6%96%87%E5%AD%97%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>昨天看球时，球迷都说了啥——弹幕抓取与分析</title>
    <link href="https://zhangslob.github.io/2017/03/24/%E6%98%A8%E5%A4%A9%E7%9C%8B%E7%90%83%E6%97%B6%EF%BC%8C%E7%90%83%E8%BF%B7%E9%83%BD%E8%AF%B4%E4%BA%86%E5%95%A5%E2%80%94%E2%80%94%E5%BC%B9%E5%B9%95%E6%8A%93%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    <id>https://zhangslob.github.io/2017/03/24/昨天看球时，球迷都说了啥——弹幕抓取与分析/</id>
    <published>2017-03-24T12:41:58.000Z</published>
    <updated>2017-03-24T12:51:54.996Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第八篇原创文章
</code></pre><p><img src="http://i.imgur.com/m7Aarx4.jpg" alt=""><br><a id="more"></a></p>
<p>数据来源：<a href="http://star.longzhu.com/teamchina" target="_blank" rel="external">http://star.longzhu.com/teamchina</a></p>
<p>本次弹幕记录（开始时间: 2017-03-23-19:43:34，结束21:29:33)，共记录20788条数据。</p>
<p>使用OBS弹幕助手记录<a href="http://www.obsapp.com/apps/obsdanmu/" target="_blank" rel="external">http://www.obsapp.com/apps/obsdanmu/</a></p>
<p><img src="http://i.imgur.com/fk9G55o.png" alt=""></p>
<h1 id="1、分析"><a href="#1、分析" class="headerlink" title="1、分析"></a>1、分析</h1><p><img src="http://i.imgur.com/dcdvVir.png" alt=""></p>
<p>文件中含有时间记录，观众ID和送礼记录，其次是弹幕内容，所以决定对前两列内容不分析。</p>
<p>首先需要对文本分词，这里采用jieba分词 <a href="https://github.com/fxsjy/jieba/" target="_blank" rel="external">https://github.com/fxsjy/jieba/</a></p>
<p>去除空格，使用<code>strip()</code>函数， 去掉换行符”\n” </p>
<pre><code>line = line.strip(&apos;\n&apos;)
</code></pre><p>把分析结果写入新的文档’text.txt’，Python join() 方法用于将序列中的元素以指定的字符连接生成一个新的字符串</p>
<pre><code>text = &apos;&apos;
with open(&apos;danmu.txt&apos;,encoding=&apos;utf-8&apos;) as fin:
    for line in fin.readlines():
         line = line.strip(&apos;\n&apos;)
         text += &apos;/&apos;.join(jieba.cut(line))
         text += &apos; &apos;
fout = open(&apos;text.txt&apos;,&apos;wb&apos;)#以二进制写模式写入
pickle.dump(text,fout)
fout.close()
</code></pre><p>这样就完成了分词过程，结果如下：</p>
<p><img src="http://i.imgur.com/Yxki3q6.png" alt=""></p>
<h1 id="2、绘制图云"><a href="#2、绘制图云" class="headerlink" title="2、绘制图云"></a>2、绘制图云</h1><pre><code># 直接从文件读取数据
fr = open(&apos;text.txt&apos;,&apos;rb&apos;)
text = pickle.load(fr)
</code></pre><p>使用word_cloud，具体用法<a href="https://github.com/amueller/word_cloud" target="_blank" rel="external">https://github.com/amueller/word_cloud</a></p>
<pre><code>backgroud_Image = plt.imread(&apos;girl.jpg&apos;)
wc = WordCloud( background_color = &apos;white&apos;,    # 设置背景颜色
                mask = backgroud_Image,        # 设置背景图片
                max_words = 2000,            # 设置最大现实的字数
                stopwords = STOPWORDS,        # 设置停用词
                font_path = &apos;C:/Users/Windows/fonts/msyh.ttf&apos;,# 设置字体格式，如不设置显示不了中文
                max_font_size = 300,            # 设置字体最大值
                random_state = 50,            # 设置有多少种随机生成状态，即有多少种配色方案
                )
</code></pre><p>使用matplotlib绘图<a href="http://matplotlib.org/2.0.0/index.html" target="_blank" rel="external">http://matplotlib.org/2.0.0/index.html</a></p>
<pre><code>wc.generate(text)
image_colors = ImageColorGenerator(backgroud_Image)
#wc.recolor(color_func = image_colors)
plt.imshow(wc)
plt.axis(&apos;off&apos;)
plt.show()
</code></pre><p>OK，这样就完成了，附上结果</p>
<p><img src="http://i.imgur.com/JE54yKZ.png" alt=""><br>有没有你发过的弹幕呢？</p>
<p>可自形修改数据，得到更好看图片。</p>
<p>能力有限，分析很少，如果你想进行更深入分析，请找我要文件。</p>
<p>对英雄联盟感兴趣的小伙伴可以看看这篇，对游戏直播弹幕的分析。</p>
<p><a href="https://zhangslob.github.io/2017/03/24/%E5%88%A9%E7%94%A8Python%E5%AF%B9%E7%9B%B4%E6%92%AD%E5%BC%B9%E5%B9%95%E7%9A%84%E5%88%86%E6%9E%90/">https://zhangslob.github.io/2017/03/24/%E5%88%A9%E7%94%A8Python%E5%AF%B9%E7%9B%B4%E6%92%AD%E5%BC%B9%E5%B9%95%E7%9A%84%E5%88%86%E6%9E%90/</a></p>
<blockquote>
<p>github：<a href="https://github.com/zhangslob/DanmuFenxi" target="_blank" rel="external">https://github.com/zhangslob/DanmuFenxi</a></p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第八篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/m7Aarx4.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="足球" scheme="https://zhangslob.github.io/tags/%E8%B6%B3%E7%90%83/"/>
    
      <category term="数据分析" scheme="https://zhangslob.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>利用Python对直播弹幕的分析</title>
    <link href="https://zhangslob.github.io/2017/03/24/%E5%88%A9%E7%94%A8Python%E5%AF%B9%E7%9B%B4%E6%92%AD%E5%BC%B9%E5%B9%95%E7%9A%84%E5%88%86%E6%9E%90/"/>
    <id>https://zhangslob.github.io/2017/03/24/利用Python对直播弹幕的分析/</id>
    <published>2017-03-24T12:12:58.000Z</published>
    <updated>2017-03-24T12:39:06.414Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第七篇原创文章
</code></pre><p>弹幕（ barrage），中文流行词语，原意指用大量或少量火炮提供密集炮击。而<br>弹幕，顾名思义是指子弹多而形成的幕布， 大量吐槽评论从屏幕飘过时效果看<br>上去像是飞行射击游戏里的弹幕。<br><img src="http://i.imgur.com/UMR9Lme.jpg" alt=""><br><a id="more"></a></p>
<p>今天就来说说游戏直播中， 弹幕都有哪些。</p>
<h1 id="一、准备"><a href="#一、准备" class="headerlink" title="一、准备"></a>一、准备</h1><p>利用 danmu 弹幕接口对斗鱼主播赏金术士直播间的弹幕进行抓取，抓取时间约<br>2 小时，共计 2534 条弹幕。赏金直播间：<a href="https://www.douyu.com/846805" target="_blank" rel="external"> https://www.douyu.com/846805</a></p>
<p><img src="http://i.imgur.com/cmDvHBM.png" alt=""></p>
<h1 id="二、-分析"><a href="#二、-分析" class="headerlink" title="二、 分析"></a>二、 分析</h1><h2 id="1、-弹幕词云"><a href="#1、-弹幕词云" class="headerlink" title="1、 弹幕词云"></a>1、 弹幕词云</h2><p>词云， 由词汇组成类似云的彩色图形。 使用的是 Python 的模块 wordcloud。 通<br>过 jieba 分词对弹幕中文分词， 使用 wordcloud 对结果构造词云， 最终结果为：</p>
<p><img src="http://i.imgur.com/NlePiqC.png" alt=""></p>
<p>可以看到，高频率词语有：</p>
<p>外甥、无敌、厉害、可以、无限、火力、什么、垃圾、赏金。</p>
<h2 id="2、关键词"><a href="#2、关键词" class="headerlink" title="2、关键词"></a>2、关键词</h2><p>TextRank算法可以用来从文本中提取关键词和摘要</p>
<p><strong>关键词：</strong></p>
<pre><code>不    0.010922664205428556
外甥    0.010484629632807344
玩    0.008177160003721682
无限    0.0058741805660283575
没    0.005665342357801469
说    0.005548941560115147
大    0.005541099430280024
主播    0.005498954448927515
出    0.0054948076800822405
看    0.0051528084835430555
</code></pre><p>可以看出来，作为汉字常用字，‘不’、‘玩’、‘没’、‘说’、‘大’、‘出’、‘看’这7个字出现频率高，这不奇怪。但是，‘外甥’、‘无限’、‘主播’就和英雄联盟主播赏金术士有责很大关系了。</p>
<p>外甥是赏金双排的一位选手、扮演着搞笑、逗乐的角色。‘无限’则是“无限火力”，一个特定模式，颇受玩家喜爱。主播可能是赏金，也有可能说别的主播。</p>
<p><img src="http://i.imgur.com/QAB31Jb.jpg" alt=""></p>
<p><strong>关键短语：</strong></p>
<pre><code>对面不会
垃圾主播
不出
没带
不大
赏金玩
对面德玛
外甥说
大不
外甥大
</code></pre><p><strong>摘要：</strong></p>
<pre><code>7337 0.0006968086282134394 真的有护眼模式666
3059 0.0006968086282134393 护眼模式为什么这么绿
10503 0.00047342729603724247 找儿子，爱好护眼
</code></pre><p>从摘要中看到三条弹幕中，都含有“<strong>护眼</strong>”二字，这是为什么呢？</p>
<p>lol护眼一词，其实主要来源于英雄联盟的直播平台，随着叫的人多了，这个词便火热了起来。起初是有人带节奏，说打护眼斗鱼可以进入护眼模式。一般在直播lol的主播使用的英雄死掉后，界面会呈现暗灰色的，亮度降低有利于防护眼睛，从而就有了lol护眼，当然意思就是嘲讽主播很菜的意思。</p>
<h1 id="三、总结："><a href="#三、总结：" class="headerlink" title="三、总结："></a>三、总结：</h1><p>由于自己所收集的数据过少、而且仅保存了一位主播的弹幕，造成结果不具有通用性，可以通过对各大直播平台的热门主播弹幕的爬取，获得观众的心理变化和网络风气。</p>
<p><strong>以及主播有没有过气一说~</strong></p>
<p>github：<a href="https://github.com/zhangslob/DanmuFenxi" target="_blank" rel="external">https://github.com/zhangslob/DanmuFenxi</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第七篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;弹幕（ barrage），中文流行词语，原意指用大量或少量火炮提供密集炮击。而&lt;br&gt;弹幕，顾名思义是指子弹多而形成的幕布， 大量吐槽评论从屏幕飘过时效果看&lt;br&gt;上去像是飞行射击游戏里的弹幕。&lt;br&gt;&lt;img src=&quot;http://i.imgur.com/UMR9Lme.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="直播" scheme="https://zhangslob.github.io/tags/%E7%9B%B4%E6%92%AD/"/>
    
      <category term="文本分析" scheme="https://zhangslob.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>如何优雅的“轮带逛”初级篇——获取单张图片</title>
    <link href="https://zhangslob.github.io/2017/03/20/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E2%80%9C%E8%BD%AE%E5%B8%A6%E9%80%9B%E2%80%9D%E5%88%9D%E7%BA%A7%E7%AF%87%E2%80%94%E2%80%94%E8%8E%B7%E5%8F%96%E5%8D%95%E5%BC%A0%E5%9B%BE%E7%89%87/"/>
    <id>https://zhangslob.github.io/2017/03/20/如何优雅的“轮带逛”初级篇——获取单张图片/</id>
    <published>2017-03-20T12:35:00.000Z</published>
    <updated>2017-03-20T12:58:51.891Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第六篇原创文章
</code></pre><p><img src="http://i.imgur.com/Oi3WEZu.png" alt=""></p>
<p>轮子哥护体</p>
<a id="more"></a>
<p>首先上收藏夹 <a href="https://www.zhihu.com/collection/78172986?page=1" target="_blank" rel="external">https://www.zhihu.com/collection/78172986?page=1</a></p>
<p>由@vega13创建，内容挺多的。例如，</p>
<p><img src="http://i.imgur.com/EgCezYY.jpg" alt=""></p>
<p><img src="http://i.imgur.com/4fVMbpi.jpg" alt=""></p>
<p>等等，看的老夫脸都红了</p>
<p>写了一个简单爬取图片的程序。记录下过程。手动 @轮子哥</p>
<h1 id="1、分析网页"><a href="#1、分析网页" class="headerlink" title="1、分析网页"></a>1、分析网页</h1><p>收藏夹只收藏了问题的一个答案，初步想法是获取当前页面的图片</p>
<p>因为上一次原因，直接去网页源代码</p>
<p><img src="http://i.imgur.com/3DMEaf7.png" alt=""></p>
<p><img src="http://i.imgur.com/lIQzPy9.png" alt=""></p>
<pre><code>&lt;img src=&quot;https://pic4.zhimg.com/de5ecb16bcb912e99a83f647eb96c5bb_200x112.jpg&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image inline-img zh-lightbox-thumb&quot; data-original=&quot;https://pic4.zhimg.com/de5ecb16bcb912e99a83f647eb96c5bb_r.jpg&quot;&gt;

&lt;img data-rawwidth=&quot;1280&quot; data-rawheight=&quot;1836&quot; src=&quot;https://pic2.zhimg.com/v2-61ba67d910104f99acdb805a3568ab05_200x112.jpg&quot; class=&quot;origin_image inline-img zh-lightbox-thumb&quot; data-original=&quot;https://pic2.zhimg.com/v2-61ba67d910104f99acdb805a3568ab05_r.jpg&quot;&gt;
</code></pre><p>在<code>&lt;img&gt;</code>标签下，<code>src</code>和<code>data-original</code>都含有图片链接，经验证<code>data-original</code>是大图，那就把每个问题的图片链接找到了，接下来就很简单了。</p>
<h1 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h1><p>就18行的代码。简单吧~</p>
<pre><code>import requests,urllib
from lxml import etree

def get_img(url):
    headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;}
    r= requests.get(url,headers=headers).text
    s = etree.HTML(r)
    #print(r)
    link = s.xpath(&apos;//img/@data-original&apos;)
    for i in link:
        print(i)
        name = i.split(&apos;/&apos;)[-1]#图片名称
        urllib.request.urlretrieve(i,name)

if __name__ == &apos;__main__&apos;:
    for i in range(1,43):
        url = &apos;https://www.zhihu.com/collection/78172986?page=&apos; + str(i)
        get_img(url)
</code></pre><p>但是这样存在一个问题</p>
<pre><code>link = s.xpath(&apos;//img/@data-original&apos;)
</code></pre><p>这里的link只有每个回答的第一张图片，更多的图片藏在文本中，除了正则不知道还有没有更好的办法</p>
<p><img src="http://i.imgur.com/L51i5jA.png" alt=""></p>
<p><img src="http://i.imgur.com/uStQpvw.jpg" alt=""></p>
<p>只有200多张图片。结尾有百度云</p>
<p>下载了几分钟出现错误，有一张图片下载不了，知友们有什么好办法吗？</p>
<h1 id="三、“轮带逛”高级篇"><a href="#三、“轮带逛”高级篇" class="headerlink" title="三、“轮带逛”高级篇"></a>三、“轮带逛”高级篇</h1><p>既然有初级篇，肯定还有高级篇。</p>
<p>其实这个收藏夹中每一个问题下面都含有很多回答，收藏夹只是选取其中一个，也就是被轮子哥点赞的那个，那么还有那些没有被点赞的呢？</p>
<p>下一期讲一讲怎么获取所有图片链接。</p>
<p>放一张图片，卡死我程序的</p>
<p><img src="http://i.imgur.com/12b0X70.jpg" alt=""></p>
<p>—————————————最后的小广告—————————————–</p>
<p>有朋友竟然叫我去作一期直播，讲一讲Python。</p>
<p>打算根据自己的经历分享一些经验，主要是关于Python入门的，想听听可以私信我。</p>
<p>时间是周二晚9点~</p>
<hr>
<p><strong>百度云下载</strong></p>
<p>链接：<a href="http://pan.baidu.com/s/1dFOPbUx" target="_blank" rel="external">http://pan.baidu.com/s/1dFOPbUx</a> 密码：abrl</p>
<p><img src="http://i.imgur.com/RF2P6MR.jpg" alt=""></p>
<p><img src="http://i.imgur.com/jFbuAcb.jpg" alt=""></p>
<p><img src="http://i.imgur.com/7pm3j4M.jpg" alt=""></p>
<p><img src="http://i.imgur.com/B80rUbw.jpg" alt=""></p>
<p><img src="http://i.imgur.com/Ra4Lkr1.jpg" alt=""></p>
<p><img src="http://i.imgur.com/VTA6Ll4.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第六篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/Oi3WEZu.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;轮子哥护体&lt;/p&gt;
    
    </summary>
    
    
      <category term="知乎爬虫" scheme="https://zhangslob.github.io/tags/%E7%9F%A5%E4%B9%8E%E7%88%AC%E8%99%AB/"/>
    
      <category term="轮带逛" scheme="https://zhangslob.github.io/tags/%E8%BD%AE%E5%B8%A6%E9%80%9B/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫实战——免费图片 - Pixabay</title>
    <link href="https://zhangslob.github.io/2017/03/19/Python%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%E5%85%8D%E8%B4%B9%E5%9B%BE%E7%89%87-Pixabay/"/>
    <id>https://zhangslob.github.io/2017/03/19/Python爬虫实战——免费图片-Pixabay/</id>
    <published>2017-03-19T09:24:58.000Z</published>
    <updated>2017-03-19T09:52:39.786Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第五篇原创文章
</code></pre><p><img src="http://i.imgur.com/jUTb7Pw.jpg" alt=""></p>
<p>Pixabay，一个挺不错的高清无码图片网站，可以免费下载。</p>
<p><a href="https://pixabay.com/" target="_blank" rel="external">https://pixabay.com/</a></p>
<a id="more"></a>
<p><em>一些介绍</em></p>
<blockquote>
<p>超过 900000 高质量照片、 插图和矢量图形。可免费用于商业用途。没有所需的归属。</p>
<p>Pixabay是一家高质量图片分享网站。最初，该网站由Hans Braxmeier和Simon Steinberger在德国发展起来。2013年2月，网站拥有由影师和其社区的插画家提供的大约7万张免费的照片和矢量图形。该公司于2010年12月在德国乌尔姆成立。</p>
<p>2012年3月，Pixabay开始从一个私人图像搜集网站转变成一个互动的网上社区，该网站支持20种语言。同年5月，网站推出公共应用程序编程接口，从而使第三方用户和网站开发人员搜索其图像数据库。网站还与Flickr，YouTube和维基共享资源。</p>
<p>Pixabay用户无需注册就可以获得免费版权的高质量图像。根据知识共享契约CC0相关的肖像权，用户在该网站通过上传图片就默认放弃图片版权，从而使图片广泛流通。网站允许任何人使用，修改图片 - 即便是在商业应用 - 不要求许可并且未认可。 </p>
<p>Pixabay为了确保高品质图片标准，用户上传的所有图片将由网站工作人员手动审批。大约27％的用户会说英语，20％的用户会说西班牙语，11％的用户会说葡萄牙语，7％的用户会说德语和5％的用户会说法语。其用户主要是博客、图形设计师、作家、记者和广告商。</p>
</blockquote>
<p>今天的目标就是爬取小编精选的图片  <a href="https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=1" target="_blank" rel="external">https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=1</a></p>
<h1 id="一、分析"><a href="#一、分析" class="headerlink" title="一、分析"></a>一、分析</h1><p>我们需要写3个函数</p>
<blockquote>
<p>一个Download(url)，用来下载图片</p>
<p>一个用来获取小编精选一共有的165页FullUrl()</p>
<p>最后用来调用main()</p>
</blockquote>
<p>下面开始一个个写吧~</p>
<p><a href="https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=1" title="小编精选 - 照片" target="_blank" rel="external">https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=1</a></p>
<p>打开网页，F12，查看图片链接所在的标签</p>
<p><img src="http://i.imgur.com/TlQbPlN.jpg" alt=""><br><img src="http://i.imgur.com/D3kTbAR.jpg" alt=""></p>
<p>可以看到图片链接都在<code>&lt;img&gt;</code>标签下，但是我自己发现前几张和后几张的属性是不一样的，提取出<code>&lt;img&gt;</code>中“src”就可以了，使用的是xpath</p>
<pre><code>import requests
from lxml import etree

header = {&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;}
url = &apos;https://pixabay.com/zh/editors_choice/&apos;
r = requests.get(url,headers=header).text
s = etree.HTML(r)
print(s.xpath(&apos;//img/@src&apos;))
</code></pre><p>结果是</p>
<p><img src="http://i.imgur.com/uWgYPa4.png" alt=""></p>
<p>前面都是正确的图片链接，可是后面出现了’/static/img/blank.gif’，这个是什么鬼，查看网页源代码，搜索</p>
<p><img src="http://i.imgur.com/Mxqh6Qu.png" alt=""></p>
<p>可以发现确实有这一段字符串，我自己在这一点上花了很多时间。感谢@李宏杰的帮助，<br><a href="https://www.zhihu.com/question/57188290" title="Python爬虫动态页面抓取问题？ - 爬虫（计算机网络） - 知乎" target="_blank" rel="external">https://www.zhihu.com/question/57188290</a></p>
<blockquote>
<p>浏览器中的代码是JavaScript修改过的, 你直接用requests请求然后打印出来看就会发现</p>
</blockquote>
<pre><code>&lt;div class=&quot;item&quot; data-w=&quot;640&quot; data-h=&quot;426&quot;&gt;
                    &lt;a href=&quot;/zh/%E8%9B%8B%E7%B3%95-%E4%B8%80%E5%9D%97%E8%9B%8B%E7%B3%95-%E9%A3%9F%E8%B0%B1-%E4%B8%80%E7%89%87-%E7%B3%96%E6%9E%9C-%E6%8F%92%E5%9B%BE-%E7%83%98%E7%83%A4-%E7%94%9C%E7%82%B9-%E9%A3%9F%E5%93%81-1971556/&quot;&gt;
                        &lt;img src=&quot;/static/img/blank.gif&quot; data-lazy-srcset=&quot;https://cdn.pixabay.com/photo/2017/01/11/11/33/cake-1971556__340.jpg 1x, https://cdn.pixabay.com/photo/2017/01/11/11/33/cake-1971556__480.jpg 2x&quot; data-lazy=&quot;https://cdn.pixabay.com/photo/2017/01/11/11/33/cake-1971556__340.jpg&quot; alt=&quot;&quot;&gt;

                    &lt;/a&gt;
                    &lt;div&gt;
</code></pre><p>requests返回的数据中可以看到，“data-lazy”总含有我们需要的数据，修改代码</p>
<p><img src="http://i.imgur.com/PgU67mV.png" alt=""></p>
<p>发现现在返回的数据是我们需要的，打开一张图片查看</p>
<p><img src="http://i.imgur.com/caZqMpt.png" alt=""><br><img src="http://i.imgur.com/AE8qAph.png" alt=""></p>
<p>下面的图片要清晰很多，我们只需要把<code>__340</code>换成<code>_960_720</code>即可</p>
<p>小编精选一共有165页，我们需要获取下一页URL</p>
<pre><code>https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=2
https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=3
。。。
</code></pre><p>规律很简单</p>
<pre><code>full_link = []
for i in range(1,165):
    #print(i)
    full_link.append( &apos;https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=&apos;+ str(i))
</code></pre><p>到现在，准备工作做好了，思路可能不是很清楚，请谅解~</p>
<h1 id="二、代码"><a href="#二、代码" class="headerlink" title="二、代码"></a>二、代码</h1><pre><code>import requests
from lxml import etree
import time
import urllib

def Download(url):
    header = {
        &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;}
    r = s.get(url, headers=header).text
    s = etree.HTML(r)
    r = s.xpath(&apos;//img/@data-lazy&apos;)
    for i in r:
        imglist = i.replace(&apos;__340&apos;, &apos;_960_720&apos;)
        name = imglist.split(&apos;/&apos;)[-1]#图片名称
        urllib.request.urlretrieve(imglist,name)
        time.sleep(1)


def FullUrl():
    full_link = []
    for i in range(2,165):
        #print(i)
        full_link.append( &apos;https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=&apos;+ str(i))
        #print(full)
    return full_link

if __name__ == &apos;__main__&apos;:
    urls = FullUrl()
    for url in urls:
        Download(url)
</code></pre><p>爬取图片的工作就完成了，粗略的计算6600张，每一张下载需要5秒钟，一分钟60秒、一小时60分钟，天呐，需要<strong>9个小时</strong>才能爬取全部的图片。想一想还是算了吧，整站爬取还是要使用Scrapy+mongodb。</p>
<pre><code>&gt;&gt;&gt; 165*40
6600
&gt;&gt;&gt; from __future__ import division
&gt;&gt;&gt; 6600*5/60/60
9.166666666666666
</code></pre><p>下载了700多张，108M，也算是留着看看吧。</p>
<p>一会上传到Github上</p>
<h1 id="三、结语"><a href="#三、结语" class="headerlink" title="三、结语"></a>三、结语</h1><p>昨天学习了<strong>崔庆才</strong>老师的爬虫，感觉真的学习到了好多，对Python爬虫提高很有帮助，还有，原来他就是<strong>静觅</strong>，刚开始学习爬虫就在看他的博客，没想到他现在又在出爬虫教程，打算跟着学习。</p>
<p><img src="http://i.imgur.com/SYdEM8X.png" alt=""></p>
<pre><code>分享内容：  

1. 分析知乎Ajax请求及爬取逻辑

2. 用Scrapy实现递归爬取

3. 爬取结果存储到MongoDB   
</code></pre><p>废话不多说，自己看看就知道了。</p>
<p>静觅丨崔庆才的个人博客<a href="http://cuiqingcai.com/" target="_blank" rel="external">http://cuiqingcai.com/</a></p>
<p>微课录播 | 03月17日 爬取知乎所有用户详细信息<a href="https://edu.hellobi.com/course/163" target="_blank" rel="external">https://edu.hellobi.com/course/163</a></p>
<blockquote>
<p>最后的小广告</p>
<p>有朋友竟然叫我去作一期直播，讲一讲Python。</p>
<p>打算根据自己的经历分享一些经验，主要是关于Python入门的，想听听可以私信我。</p>
<p>时间是周二晚9点~</p>
<p><strong>Hello World！ Try to be a Pythoner！</strong></p>
</blockquote>
<p><img src="http://i.imgur.com/6MorRwD.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第五篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/jUTb7Pw.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Pixabay，一个挺不错的高清无码图片网站，可以免费下载。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pixabay.com/&quot;&gt;https://pixabay.com/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="图片下载" scheme="https://zhangslob.github.io/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Python练习第九题，爬取贴吧图片</title>
    <link href="https://zhangslob.github.io/2017/03/14/Python%E7%BB%83%E4%B9%A0%E7%AC%AC%E4%B9%9D%E9%A2%98%EF%BC%8C%E7%88%AC%E5%8F%96%E8%B4%B4%E5%90%A7%E5%9B%BE%E7%89%87/"/>
    <id>https://zhangslob.github.io/2017/03/14/Python练习第九题，爬取贴吧图片/</id>
    <published>2017-03-14T11:56:07.000Z</published>
    <updated>2017-03-14T12:14:01.577Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第四篇原创文章
</code></pre><h1 id="一、问题：用-Python爬取妹子图片"><a href="#一、问题：用-Python爬取妹子图片" class="headerlink" title="一、问题：用 Python爬取妹子图片 :)"></a>一、问题：用 Python爬取妹子图片 :)</h1><p><img src="http://i.imgur.com/MwvaPyU.jpg" alt=""></p>
<p><a href="http://tieba.baidu.com/p/2166231880" target="_blank" rel="external">http://tieba.baidu.com/p/2166231880</a><br><a id="more"></a></p>
<h1 id="二、分析贴吧网页源码"><a href="#二、分析贴吧网页源码" class="headerlink" title="二、分析贴吧网页源码"></a>二、分析贴吧网页源码</h1><p>打开网页<a href="http://tieba.baidu.com/p/2166231880" target="_blank" rel="external">http://tieba.baidu.com/p/2166231880</a>，F12</p>
<p><img src="http://i.imgur.com/FHK0pSS.png" alt=""></p>
<p>发现图片链接都在<code>&lt;img</code>&gt;标签中</p>
<pre><code>&lt;cc&gt;
        &lt;div...&gt;
           &lt;img...&gt;
           &lt;img...&gt;
</code></pre><p>测试发现，<strong>src</strong>中的链接就是图片链接。那么就很简单，只需要把<code>&lt;img&gt;</code>中的src的链接拿出来即可。</p>
<h1 id="三、写代码"><a href="#三、写代码" class="headerlink" title="三、写代码"></a>三、写代码</h1><p>环境：Python3，Pycharm</p>
<p>使用<strong>requests</strong>和<strong>xpath</strong>，最近才学了xpath，发现超级好用，比bs4简洁，有兴趣看看这个<a href="https://zhuanlan.zhihu.com/p/25572729" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25572729</a></p>
<pre><code>import requests
from lxml import etree

url = &apos;http://tieba.baidu.com/p/2166231880&apos;
header = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;}
r = requests.get(url,headers=header).content
s = etree.HTML(r)
print(s.xpath(&apos;//div/img/@src&apos;))
</code></pre><p><img src="http://i.imgur.com/gh8U0qK.png" alt=""></p>
<p>发现链接都已经拿到手，下一步就是下载了~</p>
<p>下载图片的语句：</p>
<pre><code>import urllib.request
path = &apos;......&apos; #下载链接
jpg_link ＝ &apos;......&apos;  #图片链接
request.urlretrieve(jpg_link, path) 
</code></pre><p>加在一起，大功告成。</p>
<p><img src="http://i.imgur.com/aqqGapO.png" alt=""></p>
<h1 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h1><p>经测试，贴吧里面其他网页如：<a href="http://tieba.baidu.com/p/1165861759" target="_blank" rel="external">http://tieba.baidu.com/p/1165861759</a></p>
<p>本代码都可以下载，顺便说一说问题。</p>
<p>1、图片名称使用图片链接中的名称，包含大量数字和字母，可以优化。</p>
<p>2、可以看到，下载文件中包含了一个表情，查看那是用户所发，说明筛选出了问题。</p>
<p>3、帖子数量多，翻页过后，需要在代码中加入获取下一页链接。</p>
<p>除此之外，还有什么问题呢？</p>
<p>源码请见：<a href="https://github.com/zhangslob/TiebaImg" target="_blank" rel="external">https://github.com/zhangslob/TiebaImg</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第四篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&quot;一、问题：用-Python爬取妹子图片&quot;&gt;&lt;a href=&quot;#一、问题：用-Python爬取妹子图片&quot; class=&quot;headerlink&quot; title=&quot;一、问题：用 Python爬取妹子图片 :)&quot;&gt;&lt;/a&gt;一、问题：用 Python爬取妹子图片 :)&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/MwvaPyU.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://tieba.baidu.com/p/2166231880&quot;&gt;http://tieba.baidu.com/p/2166231880&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="福利" scheme="https://zhangslob.github.io/tags/%E7%A6%8F%E5%88%A9/"/>
    
      <category term="美女图片" scheme="https://zhangslob.github.io/tags/%E7%BE%8E%E5%A5%B3%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>Python练习第七题，我要倒过来看</title>
    <link href="https://zhangslob.github.io/2017/03/06/Python%E7%BB%83%E4%B9%A0%E7%AC%AC%E4%B8%83%E9%A2%98%EF%BC%8C%E6%88%91%E8%A6%81%E5%80%92%E8%BF%87%E6%9D%A5%E7%9C%8B/"/>
    <id>https://zhangslob.github.io/2017/03/06/Python练习第七题，我要倒过来看/</id>
    <published>2017-03-06T10:47:46.000Z</published>
    <updated>2017-03-14T12:15:33.703Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第三篇原创文章
</code></pre><p><img src="http://i.imgur.com/oGUF8CL.jpg" alt=""></p>
<h1 id="一、Challenge"><a href="#一、Challenge" class="headerlink" title="一、Challenge"></a>一、Challenge</h1><p>Using the Python language, have the function FirstReverse(str) take the str parameter being passed and return the string in reversed（颠倒的） order. For example: if the input string is “Hello World and Coders” then your program should return the string sredoC dna dlroW olleH.<br><a id="more"></a><br><strong>题目意思是，给定字符串，返回原来的倒序。例如给出的是“Hello World and Coders”，返回“sredoC dna dlroW olleH.”</strong></p>
<p><strong>Sample Test Cases</strong></p>
<p>Input:”coderbyte”</p>
<p>Output:”etybredoc”</p>
<p>Input:”I Love Code”</p>
<p>Output:”edoC evoL I”</p>
<p><strong>Hint</strong></p>
<p>Think of how you can loop through a string or array of characters backwards to produce a new string.</p>
<pre><code>def FirstReverse(str): 

   # code goes here 
   return str

# keep this function call here  
print FirstReverse(raw_input())
</code></pre><p><img src="http://i.imgur.com/PXa187n.jpg" alt=""></p>
<h1 id="二、解法-切片"><a href="#二、解法-切片" class="headerlink" title="二、解法:切片"></a>二、解法:切片</h1><p><strong>环境：Python3.5</strong></p>
<p><em>A simple way to reverse a string would be to create a new string and fill it with the characters from the original string, but backwards. To do this, we need to loop through the original string starting from the end, and every iteration of the loop we move to the previous character in the string. Here is an example: </em></p>
<pre><code>def FirstReverse(str): 

    return str[::-1]

print (FirstReverse(input()))  
</code></pre><p>非常简洁<strong>str[::-1]</strong>就可以完成目标。</p>
<p><img src="http://i.imgur.com/jnRy3Km.jpg" alt=""></p>
<h1 id="三、切片详解"><a href="#三、切片详解" class="headerlink" title="三、切片详解"></a>三、切片详解</h1><p><strong>1、取字符串中第几个字符</strong></p>
<pre><code>&gt;&gt;&gt; &apos;hello&apos;[0]#表示输出字符串中第一个字符
&apos;h&apos;
&gt;&gt;&gt; &apos;hello&apos;[-1]#表示输出字符串中最后一个字符
&apos;o&apos;
</code></pre><p><strong>2、字符串分割</strong></p>
<pre><code>&gt;&gt;&gt; &apos;hello&apos;[1:3]
&apos;el&apos;
</code></pre><p>第一个参数表示原来字符串中的下表<br>第二个参数表示分割后剩下的字符串的第一个字符 在 原来字符串中的下标</p>
<p>注意，<strong>Python从0开始计数</strong></p>
<p><strong>3、几种特殊情况</strong></p>
<pre><code>&gt;&gt;&gt; &apos;hello&apos;[:3]#从第一个字符开始截取，直到最后
&apos;hel&apos;
&gt;&gt;&gt; &apos;hello&apos;[0:]#从第一个字符开始截取，截取到最后
&apos;hello&apos;
&gt;&gt;&gt; &apos;hello&apos;[:]
&apos;hello&apos;
</code></pre><p><strong>4、步长截取</strong></p>
<pre><code>&gt;&gt;&gt; &apos;abcde&apos;[::2]#表示从第一个字符开始截取，间隔2个字符取一个。
&apos;ace&apos;
&gt;&gt;&gt; &apos;abcde&apos;[::-2]
&apos;eca&apos;
&gt;&gt;&gt; &apos;abcde&apos;[::-1]
&apos;edcba&apos;
</code></pre><p>推荐阅读：</p>
<blockquote>
<p>官方文档<br><a href="https://docs.python.org/3/tutorial/introduction.html#strings" target="_blank" rel="external">https://docs.python.org/3/tutorial/introduction.html#strings</a></p>
<p>廖雪峰的教程</p>
<p><a href="http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431756919644a792ee4ead724ef7afab3f7f771b04f5000" title="切片" target="_blank" rel="external">http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431756919644a792ee4ead724ef7afab3f7f771b04f5000</a></p>
</blockquote>
<p>更多解法：</p>
<pre><code>def FirstReverse(str): 

  # reversed(str) turns the string into an iterator object (similar to an array)
 # and reverses the order of the characters
 # then we join it with an empty string producing a final string for us

     return &apos;&apos;.join(reversed(str))

print(FirstReverse(input()))
</code></pre><p>使用了什么语法？评论中见。</p>
<p><img src="http://i.imgur.com/SbWTR0Q.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第三篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/oGUF8CL.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;一、Challenge&quot;&gt;&lt;a href=&quot;#一、Challenge&quot; class=&quot;headerlink&quot; title=&quot;一、Challenge&quot;&gt;&lt;/a&gt;一、Challenge&lt;/h1&gt;&lt;p&gt;Using the Python language, have the function FirstReverse(str) take the str parameter being passed and return the string in reversed（颠倒的） order. For example: if the input string is “Hello World and Coders” then your program should return the string sredoC dna dlroW olleH.&lt;br&gt;
    
    </summary>
    
    
      <category term="Python语法" scheme="https://zhangslob.github.io/tags/Python%E8%AF%AD%E6%B3%95/"/>
    
      <category term="切片介绍" scheme="https://zhangslob.github.io/tags/%E5%88%87%E7%89%87%E4%BB%8B%E7%BB%8D/"/>
    
  </entry>
  
</feed>
