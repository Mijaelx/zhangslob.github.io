<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小歪的博客</title>
  <subtitle>人生苦短，我学Python</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhangslob.github.io/"/>
  <updated>2018-02-26T15:14:48.399Z</updated>
  <id>https://zhangslob.github.io/</id>
  
  <author>
    <name>小歪</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Scrapy源码（2）——爬虫开始的地方</title>
    <link href="https://zhangslob.github.io/2018/02/26/Scrapy%E6%BA%90%E7%A0%81%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94%E7%88%AC%E8%99%AB%E5%BC%80%E5%A7%8B%E7%9A%84%E5%9C%B0%E6%96%B9/"/>
    <id>https://zhangslob.github.io/2018/02/26/Scrapy源码（2）——爬虫开始的地方/</id>
    <published>2018-02-26T14:48:13.000Z</published>
    <updated>2018-02-26T15:14:48.399Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第三十三篇原创文章
</code></pre><p>开始学习<code>Scrapy</code>源码  (๑• . •๑)</p>
<p><img src="https://i.imgur.com/5wP0fKB.png" alt=""></p>
<a id="more"></a>
<h1 id="Scrapy运行命令"><a href="#Scrapy运行命令" class="headerlink" title="Scrapy运行命令"></a>Scrapy运行命令</h1><p>一般来说，运行Scrapy项目的写法有，（这里不考虑从脚本运行Scrapy）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Usage examples:</div><div class="line"></div><div class="line">$ scrapy crawl myspider</div><div class="line">[ ... myspider starts crawling ... ]</div><div class="line"></div><div class="line">$ scrapy runspider myspider.py</div><div class="line">[ ... spider starts crawling ... ]</div></pre></td></tr></table></figure>
<p>但是更好的写法是，新建一个Python文件，如下，（便于调试）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</div><div class="line"></div><div class="line">cmdline.execute(<span class="string">"scrapy crawl myspider"</span>.split())</div></pre></td></tr></table></figure></p>
<p>很容易就发现，Scrapy运行文件是<code>cmdline.py</code>文件里面的<code>execute()</code>函数，下面学习下这个函数在做什么。</p>
<h1 id="分析源码"><a href="#分析源码" class="headerlink" title="分析源码"></a>分析源码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(argv=None, settings=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> argv <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        argv = sys.argv</div><div class="line"></div><div class="line">    <span class="comment"># --- backwards compatibility for scrapy.conf.settings singleton ---</span></div><div class="line">    <span class="keyword">if</span> settings <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">and</span> <span class="string">'scrapy.conf'</span> <span class="keyword">in</span> sys.modules:</div><div class="line">        <span class="keyword">from</span> scrapy <span class="keyword">import</span> conf</div><div class="line">        <span class="keyword">if</span> hasattr(conf, <span class="string">'settings'</span>):</div><div class="line">            settings = conf.settings</div><div class="line">    <span class="comment"># ------------------------------------------------------------------</span></div></pre></td></tr></table></figure>
<p>寻找 <code>scrapy.conf</code>配置文件，argv直接取sys.argv</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> settings <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">    settings = get_project_settings()</div><div class="line">    <span class="comment"># set EDITOR from environment if available</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        editor = os.environ[<span class="string">'EDITOR'</span>]</div><div class="line">    <span class="keyword">except</span> KeyError: <span class="keyword">pass</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        settings[<span class="string">'EDITOR'</span>] = editor</div><div class="line">check_deprecated_settings(settings)</div><div class="line"></div><div class="line"><span class="comment"># --- backwards compatibility for scrapy.conf.settings singleton ---</span></div><div class="line"><span class="keyword">import</span> warnings</div><div class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> ScrapyDeprecationWarning</div><div class="line"><span class="keyword">with</span> warnings.catch_warnings():</div><div class="line">    warnings.simplefilter(<span class="string">"ignore"</span>, ScrapyDeprecationWarning)</div><div class="line">    <span class="keyword">from</span> scrapy <span class="keyword">import</span> conf</div><div class="line">    conf.settings = settings</div><div class="line"><span class="comment"># ------------------------------------------------------------------</span></div></pre></td></tr></table></figure>
<blockquote>
<p>set EDITOR from environment if available</p>
</blockquote>
<p>读取<code>settings</code>设置文件，导入项目，调用<code>get_project_settings()</code>函数，此处为<code>utils</code>文件夹下的<code>project.py</code>文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_project_settings</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">if</span> ENVVAR <span class="keyword">not</span> <span class="keyword">in</span> os.environ:</div><div class="line">        project = os.environ.get(<span class="string">'SCRAPY_PROJECT'</span>, <span class="string">'default'</span>)</div><div class="line">        init_env(project)</div></pre></td></tr></table></figure>
<blockquote>
<p>project.py</p>
</blockquote>
<p><code>init_env()</code> 函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_env</span><span class="params">(project=<span class="string">'default'</span>, set_syspath=True)</span>:</span></div><div class="line">    <span class="string">"""Initialize environment to use command-line tool from inside a project</span></div><div class="line">    dir. This sets the Scrapy settings module and modifies the Python path to</div><div class="line">    be able to locate the project module.</div><div class="line">    """</div><div class="line">    cfg = get_config()</div><div class="line">    <span class="keyword">if</span> cfg.has_option(<span class="string">'settings'</span>, project):</div><div class="line">        os.environ[<span class="string">'SCRAPY_SETTINGS_MODULE'</span>] = cfg.get(<span class="string">'settings'</span>, project)</div><div class="line">    closest = closest_scrapy_cfg()</div><div class="line">    <span class="keyword">if</span> closest:</div><div class="line">        projdir = os.path.dirname(closest)</div><div class="line">        <span class="keyword">if</span> set_syspath <span class="keyword">and</span> projdir <span class="keyword">not</span> <span class="keyword">in</span> sys.path:</div><div class="line">            sys.path.append(projdir)</div></pre></td></tr></table></figure>
<blockquote>
<p>conf.py</p>
</blockquote>
<p>如注释所说，初始化环境,循环递归找到用户项目中的配置文件<code>settings.py</code>,并且将其设置到环境变量<code>Scrapy settings module</code>中。然后修改Python路径，确保能找到项目模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">settings = Settings()</div><div class="line">settings_module_path = os.environ.get(ENVVAR)</div><div class="line"><span class="keyword">if</span> settings_module_path:</div><div class="line">    settings.setmodule(settings_module_path, priority=<span class="string">'project'</span>)</div><div class="line"></div><div class="line"><span class="comment"># <span class="doctag">XXX:</span> remove this hack</span></div><div class="line">pickled_settings = os.environ.get(<span class="string">"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE"</span>)</div><div class="line"><span class="keyword">if</span> pickled_settings:</div><div class="line">    settings.setdict(pickle.loads(pickled_settings), priority=<span class="string">'project'</span>)</div><div class="line"></div><div class="line"><span class="comment"># <span class="doctag">XXX:</span> deprecate and remove this functionality</span></div><div class="line">env_overrides = &#123;k[<span class="number">7</span>:]: v <span class="keyword">for</span> k, v <span class="keyword">in</span> os.environ.items() <span class="keyword">if</span></div><div class="line">                 k.startswith(<span class="string">'SCRAPY_'</span>)&#125;</div><div class="line"><span class="keyword">if</span> env_overrides:</div><div class="line">    settings.setdict(env_overrides, priority=<span class="string">'project'</span>)</div><div class="line"></div><div class="line"><span class="keyword">return</span> settings</div></pre></td></tr></table></figure>
<blockquote>
<p>project.py</p>
</blockquote>
<p>至此，<code>get_project_settings()</code>该函数结束，如函数名字一样，最后返回项目配置，到此为止，接着往下看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">inproject = inside_project()</div><div class="line">cmds = _get_commands_dict(settings, inproject)</div><div class="line">cmdname = _pop_command_name(argv)</div><div class="line">parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \</div><div class="line">    conflict_handler=<span class="string">'resolve'</span>)</div><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> cmdname:</div><div class="line">    _print_commands(settings, inproject)</div><div class="line">    sys.exit(<span class="number">0</span>)</div><div class="line"><span class="keyword">elif</span> cmdname <span class="keyword">not</span> <span class="keyword">in</span> cmds:</div><div class="line">    _print_unknown_command(settings, cmdname, inproject)</div><div class="line">    sys.exit(<span class="number">2</span>)</div></pre></td></tr></table></figure>
<p>导入相应的module爬虫模块（inside_project）</p>
<p>执行环境是否在项目中，主要检查scrapy.cfg配置文件是否存在，读取commands文件夹，把所有的命令类转换为<code>{cmd_name: cmd_instance}</code>的字典</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">cmd = cmds[cmdname]</div><div class="line">parser.usage = <span class="string">"scrapy %s %s"</span> % (cmdname, cmd.syntax())</div><div class="line">parser.description = cmd.long_desc()</div><div class="line">settings.setdict(cmd.default_settings, priority=<span class="string">'command'</span>)</div><div class="line">cmd.settings = settings</div><div class="line">cmd.add_options(parser)</div><div class="line">opts, args = parser.parse_args(args=argv[<span class="number">1</span>:])</div><div class="line">_run_print_help(parser, cmd.process_options, args, opts)</div></pre></td></tr></table></figure>
<p>根据命令名称找到对应的命令实例，设置项目配置和级别为command，添加解析规则，解析命令参数，并交由Scrapy命令实例处理。</p>
<p>最后，看看下面这段代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cmd.crawler_process = CrawlerProcess(settings)</div><div class="line">_run_print_help(parser, _run_command, cmd, args, opts)</div><div class="line">sys.exit(cmd.exitcode)</div></pre></td></tr></table></figure>
<p>初始化<code>CrawlerProcess</code>实例，将对应的命令执行，这里是<code>crawl</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_run_command</span><span class="params">(cmd, args, opts)</span>:</span></div><div class="line">    <span class="keyword">if</span> opts.profile:</div><div class="line">        _run_command_profiled(cmd, args, opts)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        cmd.run(args, opts)</div></pre></td></tr></table></figure>
<p>看到这，想起了文档中的介绍 <a href="https://doc.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script" target="_blank" rel="external">Run Scrapy from a script</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Here’s an example showing how to run a single spider with it.</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerProcess</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    <span class="comment"># Your spider definition</span></div><div class="line">    ...</div><div class="line"></div><div class="line">process = CrawlerProcess(&#123;</div><div class="line">    <span class="string">'USER_AGENT'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'</span></div><div class="line">&#125;)</div><div class="line"></div><div class="line">process.crawl(MySpider)</div><div class="line">process.start() <span class="comment"># the script will block here until the crawling is finished</span></div></pre></td></tr></table></figure>
<p>所以Scrapy爬虫运行都有用使用到<code>CrawlerProcess</code>，想要深入了解可以去看看源码 <a href="https://github.com/scrapy/scrapy/blob/1.5/scrapy/crawler.py#L225" target="_blank" rel="external">scrapy/scrapy/crawler.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">A class to run multiple scrapy crawlers in a process simultaneously.</div><div class="line"></div><div class="line">This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support</div><div class="line">for starting a Twisted `reactor`_ and handling shutdown signals, like the</div><div class="line">keyboard interrupt command Ctrl-C. It also configures top-level logging.</div><div class="line"></div><div class="line">This utility should be a better fit than</div><div class="line">:class:`~scrapy.crawler.CrawlerRunner` if you aren't running another</div><div class="line">Twisted `reactor`_ within your application.</div><div class="line"></div><div class="line">The CrawlerProcess object must be instantiated with a</div><div class="line">:class:`~scrapy.settings.Settings` object.</div><div class="line"></div><div class="line">:param install_root_handler: whether to install root logging handler</div><div class="line">    (default: True)</div><div class="line"></div><div class="line">This class shouldn't be needed (since Scrapy is responsible of using it</div><div class="line">accordingly) unless writing scripts that manually handle the crawling</div><div class="line">process. See :ref:`run-from-script` for an example.</div><div class="line">"""</div></pre></td></tr></table></figure>
<p>最后，附上Scrapy的路径图</p>
<p><img src="https://i.imgur.com/bG560BG.png" alt=""></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>简单来说，有这么几步：</p>
<ol>
<li>读取配置文件，应用到爬虫中</li>
<li>把所有的命令类转换名称与实例字典</li>
<li>初始化<code>CrawlerProcess</code>实例，运行爬虫</li>
</ol>
<p>(看的头疼，好多函数名记不住)</p>
<p>回顾：</p>
<ol>
<li><a href="https://zhangslob.github.io/2018/02/24/Scrapy%E6%BA%90%E7%A0%81%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88/">Scrapy源码（1）——爬虫流程概览</a></li>
<li>Scrapy源码（2）——爬虫开始的地方</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第三十三篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;开始学习&lt;code&gt;Scrapy&lt;/code&gt;源码  (๑• . •๑)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/5wP0fKB.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy源码（1）——爬虫流程概览</title>
    <link href="https://zhangslob.github.io/2018/02/24/Scrapy%E6%BA%90%E7%A0%81%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88/"/>
    <id>https://zhangslob.github.io/2018/02/24/Scrapy源码（1）——爬虫流程概览/</id>
    <published>2018-02-24T13:16:23.000Z</published>
    <updated>2018-02-26T15:15:04.711Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第三十二篇原创文章
</code></pre><p>开始学习<code>Scrapy</code>源码  (๑• . •๑)</p>
<p><img src="https://i.imgur.com/5wP0fKB.png" alt=""></p>
<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>使用 <code>Scrapy</code> 已经有一段时间了，觉得自己有必要对源码好好的学习下了，所以写下记录，希望能加深自己的理解。</p>
<blockquote>
<p>Scrapy | A Fast and Powerful Scraping and Web Crawling Framework</p>
</blockquote>
<p>接下来说到的是最新版本： Scrapy 1.5，暂且把 <code>Spider</code> 称为 <strong>蜘蛛</strong>，而不是爬虫。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Scrapy是一个开源爬虫框架，用于抓取网站并提取有用的结构化数据，如数据挖掘，信息处理或历史档案。</p>
<p>尽管Scrapy最初是为<a href="https://en.wikipedia.org/wiki/Web_scraping" target="_blank" rel="external">网页抓取</a>设计的，但它也可以用于使用API（如<a href="https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html" target="_blank" rel="external">Amazon Associates Web Services</a>）或作为通用网络抓取工具提取数据。</p>
<p>一个最简单的例子，相信大家都写过</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"quotes"</span></div><div class="line">    start_urls = [</div><div class="line">        <span class="string">'http://quotes.toscrape.com/tag/humor/'</span>,</div><div class="line">    ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>):</div><div class="line">            <span class="keyword">yield</span> &#123;</div><div class="line">                <span class="string">'text'</span>: quote.css(<span class="string">'span.text::text'</span>).extract_first(),</div><div class="line">                <span class="string">'author'</span>: quote.xpath(<span class="string">'span/small/text()'</span>).extract_first(),</div><div class="line">            &#125;</div><div class="line"></div><div class="line">        next_page = response.css(<span class="string">'li.next a::attr("href")'</span>).extract_first()</div><div class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">yield</span> response.follow(next_page, self.parse)</div></pre></td></tr></table></figure>
<p>一般来说，创建一个Scrapy项目需要如下流程：</p>
<ol>
<li>使用<code>scrapy startproject spider</code>创建爬虫模板</li>
<li>爬虫类继承<code>scrapy.Spider</code>，重写<code>parse</code>方法和逻辑</li>
<li><code>parse</code>方法中<code>yield</code>或<code>return</code>字典、<code>Request</code>、<code>Item</code></li>
<li>自定义<code>Item</code>、<code>Middlewares</code>、<code>Pipelines</code>等</li>
<li>使用<code>scrapy crawl &lt;spider_name&gt;</code>或新建文件<code>cmdline.execute(&quot;scrapy crawl spider_name&quot;.split())</code>运行（便于调试）</li>
<li>其它</li>
</ol>
<h1 id="架构概述"><a href="#架构概述" class="headerlink" title="架构概述"></a>架构概述</h1><p><img src="https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png" alt=""></p>
<p>这是一张非常经典的图，基本上说到Scrapy都会用到它，来源于<a href="https://doc.scrapy.org/en/latest/topics/architecture.html#data-flow" target="_blank" rel="external">Architecture overview</a></p>
<h2 id="核心组件（Components）"><a href="#核心组件（Components）" class="headerlink" title="核心组件（Components）"></a>核心组件（Components）</h2><ul>
<li><code>Scrapy Engine</code>：<strong>引擎</strong>，负责控制系统所有组件之间的数据流，并在发生某些操作时触发事件；</li>
<li><code>Scheduler</code>：<strong>调度器</strong>，接收来自引擎的请求，并将它们排入队列，以便在引擎请求它们时将它们提供给它们（也提供给引擎）；</li>
<li><code>Downloader</code>：<strong>下载器</strong>，负责从网络上获取网页并将它们返回到引擎，然后引擎将它们返回给蜘蛛/spiders；</li>
<li><code>Spiders</code>：<strong>蜘蛛</strong>，是用户编写的自定义类，用于解析响应并从中提取项目（也称为抓取的项目）或追加其他请求；</li>
<li><code>Item Pipeline</code>：<strong>管道</strong>，负责输出结构化数据，可自定义输出位置，典型的任务包括清理，验证和持久性；</li>
<li><code>Downloader middlewares</code>：<strong>下载中间件</strong>，位于引擎和下载器之间的特定钩子/hooks，当它们从引擎传递到下载器时处理请求，以及从下载器传递到引擎的响应，常用于如下情况：<ul>
<li>在将请求发送到下载器之前处理请求（即在Scrapy将请求发送到网站之前）;</li>
<li>在将其传递给蜘蛛之前改变接收到的响应;</li>
<li>发送新的请求，而不是将接收到的响应传递给蜘蛛;</li>
<li>向蜘蛛传递响应而不需要获取网页;</li>
<li>默默地放下一些请求。</li>
</ul>
</li>
<li><code>Spider middlewares</code>：<strong>Spider中间件</strong>，特定的钩子，位于引擎和蜘蛛之间，能够处理蜘蛛输入（响应）和输出（项目和请求），常用于如下情况：<ul>
<li>spider回调的后处理输出 更改/添加/删除请求或items;</li>
<li>后处理start_requests;</li>
<li>处理蜘蛛异常;</li>
<li>根据响应内容为一些请求调用errback而不是callback。</li>
</ul>
</li>
<li><code>Event-driven networking</code>：<strong>事件驱动的网络</strong>，Scrapy是用Twisted编写的，这是一个流行的事件驱动的Python网络框架。 因此，它使用非阻塞（又称异步）代码来实现并发。</li>
</ul>
<blockquote>
<p>Twisted is an event-driven networking engine written in Python and licensed under the open source ​MIT license. </p>
</blockquote>
<h2 id="数据流（Data-flow）"><a href="#数据流（Data-flow）" class="headerlink" title="数据流（Data flow）"></a>数据流（Data flow）</h2><p>Scrapy中的数据流由执行引擎控制，如下所示：</p>
<ol>
<li>引擎获取最初的请求从蜘蛛抓取（<code>start_urls</code>）。</li>
<li>引擎在调度程序中调度请求，并要求下一个请求进行采集。</li>
<li>调度器将下一个请求返回给引擎。</li>
<li>引擎将请求发送到下载器，通过下载器中间件。</li>
<li>一旦页面完成下载，<code>Downloader</code>会生成一个响应（包含该页面）并将其发送到引擎，并通过<code>Downloader Middlewares</code>。</li>
<li>引擎从<code>Downloader</code>收到响应并将其发送给<code>Spider</code>进行处理，并通过<code>Spider Middleware</code>传递。</li>
<li><code>Spider</code>处理响应，并通过<code>Spider</code>中间件将抓取的项目和新的请求（后续）返回给引擎。</li>
<li>引擎将处理后的项目发送到项目管道，然后将处理后的请求发送到调度程序，并要求可能的下一个请求进行采集。</li>
<li>该过程重复（从第1步开始），直到调度器没有更多请求。</li>
</ol>
<p>找到一张图，便于理解：</p>
<p><img src="https://i.imgur.com/taeQOrA.png" alt=""></p>
<p>第一期差不多就到这了，没有说很多代码，主要是宏观上来观察 <code>Scrapy</code> 的架构，是如何运行。之后会更多的查看Scrapy的源代码，就近是如何采集数据的。</p>
<p>（内心有点小恐慌，不知道会写成什么样子。）</p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>关于如何阅读项目源代码，找到一篇不错的文章，共享：<a href="https://zhijianshusheng.github.io/2017/06/07/2017/6/%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/" target="_blank" rel="external">如何阅读开源项目</a></p>
<p>主要是这几部分：</p>
<ol>
<li>看：静态对代码进行分析，看相关资料，代码逻辑。</li>
<li>跑：将项目在IDE里面跑起来，通过IDE调试参数，加Log等。</li>
<li>查：阅读过程中肯定会遇到不懂的，这时候需要通过搜索引擎来解决你的疑惑。</li>
</ol>
<p>回顾：</p>
<ol>
<li>Scrapy源码（1）——爬虫流程概览</li>
<li>Scrapy源码（2）——爬虫开始的地方</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第三十二篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;开始学习&lt;code&gt;Scrapy&lt;/code&gt;源码  (๑• . •๑)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/5wP0fKB.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>PEP8规则及Pycharm应用</title>
    <link href="https://zhangslob.github.io/2018/02/08/PEP8%E8%A7%84%E5%88%99%E5%8F%8APycharm%E5%BA%94%E7%94%A8/"/>
    <id>https://zhangslob.github.io/2018/02/08/PEP8规则及Pycharm应用/</id>
    <published>2018-02-08T13:06:23.000Z</published>
    <updated>2018-02-08T13:13:47.006Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第三十一篇原创文章
</code></pre><p>学习下 Python增强建议书  (๑• . •๑)</p>
<p><img src="https://i.imgur.com/8HyHkph.png" alt=""></p>
<a id="more"></a>
<h1 id="PEP8"><a href="#PEP8" class="headerlink" title="PEP8"></a>PEP8</h1><blockquote>
<p>PEP是 Python Enhancement Proposal 的缩写，翻译过来就是 Python增强建议书 </p>
</blockquote>
<p>PEP8 是什么呢，简单说就是一种编码规范，是为了让代码“更好看”，更容易被阅读。</p>
<p>具体有这些规范，参考 <a href="https://www.python.org/dev/peps/pep-0008/" target="_blank" rel="external">PEP 8 – Style Guide for Python Code</a></p>
<p>For example</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">Yes: <span class="keyword">import</span> os</div><div class="line">     <span class="keyword">import</span> sys</div><div class="line"></div><div class="line">No:  <span class="keyword">import</span> sys, os</div><div class="line"></div><div class="line"></div><div class="line">Yes:</div><div class="line">i = i + <span class="number">1</span></div><div class="line">submitted += <span class="number">1</span></div><div class="line">x = x*<span class="number">2</span> - <span class="number">1</span></div><div class="line">hypot2 = x*x + y*y</div><div class="line">c = (a+b) * (a-b)</div><div class="line"></div><div class="line"></div><div class="line">No:</div><div class="line">i=i+<span class="number">1</span></div><div class="line">submitted +=<span class="number">1</span></div><div class="line">x = x * <span class="number">2</span> - <span class="number">1</span></div><div class="line">hypot2 = x * x + y * y</div><div class="line">c = (a + b) * (a - b)</div></pre></td></tr></table></figure>
<p>对于初学者（例如我）来说，这些标准太多，在实际coding中可能很难记住所有的。之前招聘爬虫工程师，会PEP8也会加分，所以学习下PEP8还是有帮助的。</p>
<h1 id="Pycharm-设置"><a href="#Pycharm-设置" class="headerlink" title="Pycharm 设置"></a>Pycharm 设置</h1><p>PyCharm 配置 PEP 8 代码提示<br> 直接在右下角调整 Highlighting Level 为 Inspections 就能自动 PEP 8提示 (一般默认就是这个)</p>
<p><img src="https://i.imgur.com/OrCItzF.png" alt=""></p>
<p>For example</p>
<p><img src="https://i.imgur.com/qK1iZVM.png" alt=""></p>
<p>鼠标移到上方会提示： <code>PEP 8: expected 2 blank lines, found 1</code>，我们再增加一个空格就好了，你的代码中有任何不符合 PEP8 规范的地方都会有“<del>~</del>”提示</p>
<p><img src="https://i.imgur.com/kq4POwe.png" alt=""></p>
<p>这是一个困扰了我很久的问题，我一直不知道是哪里出了错，看英文也不懂，最后去Google才知道，创建字典的写法不规范，应该这样写：</p>
<p><code>item = dict()</code></p>
<h1 id="其他工具"><a href="#其他工具" class="headerlink" title="其他工具"></a>其他工具</h1><p>除了Pycharm，还有其他工具可以提示</p>
<ol>
<li>Google 开源的 Python 文件格式化工具：<a href="https://github.com/google/yapf" target="_blank" rel="external">github.com/google/yapf</a></li>
<li>pyflakes, pylint 等工具及各种编辑器的插件</li>
</ol>
<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>好吧，这里才是正文，正确的规范如下：</p>
<h2 id="缩进"><a href="#缩进" class="headerlink" title="缩进"></a>缩进</h2><ul>
<li>不要使用 tab 缩进</li>
<li>使用任何编辑器写 Python，请把一个 tab 展开为 4 个空格</li>
<li>绝对不要混用 tab 和空格，否则容易出现 <code>IndentationError</code></li>
</ul>
<h2 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h2><ul>
<li>在 list, dict, tuple, set, 参数列表的 , 后面加一个空格</li>
<li>在 dict 的 : 后面加一个空格</li>
<li>在注释符号 # 后面加一个空格，但是 <code>#!/usr/bin/python</code> 的 <code>#</code> 后不能有空格</li>
<li>操作符两端加一个空格，如 +, -, *, /, |, &amp;, =</li>
<li>接上一条，在参数列表里的 = 两端不需要空格</li>
<li>括号（(), {}, []）内的两端不需要空格</li>
</ul>
<h2 id="空行"><a href="#空行" class="headerlink" title="空行"></a>空行</h2><ul>
<li>function 和 class 顶上两个空行</li>
<li>class 的 method 之间一个空行</li>
<li>函数内逻辑无关的段落之间空一行，不要过度使用空行</li>
<li>不要把多个语句写在一行，然后用 ; 隔开</li>
<li><code>if/for/while</code> 语句中，即使执行语句只有一句，也要另起一行</li>
</ul>
<h2 id="换行"><a href="#换行" class="headerlink" title="换行"></a>换行</h2><ul>
<li>每一行代码控制在 80 字符以内</li>
<li>使用 \ 或 () 控制换行，举例：<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(first, second, third, fourth, fifth,</span></span></div><div class="line">        sixth, and_some_other_very_long_param):</div><div class="line">    user = User.objects.filter_by(first=first, second=second, third=third) \</div><div class="line">        .skip(<span class="number">100</span>).limit(<span class="number">100</span>) \</div><div class="line">        .all()</div><div class="line"></div><div class="line">text = (<span class="string">'Long strings can be made up '</span></div><div class="line">        <span class="string">'of several shorter strings.'</span>)</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="命名"><a href="#命名" class="headerlink" title="命名"></a>命名</h2><ul>
<li>使用有意义的，英文单词或词组，绝对不要使用汉语拼音</li>
<li>package/module 名中不要出现 -</li>
<li>各种类型的命名规范：</li>
</ul>
<p><img src="https://i.imgur.com/0sVNGR9.png" alt=""></p>
<h2 id="import"><a href="#import" class="headerlink" title="import"></a>import</h2><ul>
<li>所有 import 尽量放在文件开头，在 docstring 下面，其他变量定义的上面</li>
<li>不要使用 from foo imort *</li>
<li>import 需要分组，每组之间一个空行，每个分组内的顺序尽量采用字典序，分组顺序是：<ol>
<li>标准库</li>
<li>第三方库</li>
<li>本项目的 package 和 module</li>
</ol>
</li>
<li>不要使用隐式的相对导入（implicit relative imports），可是使用显示的相对导入（explicit relative imports），如 <code>from ..utils import parse</code>，最好使用全路径导入（absolute imports）</li>
<li>对于不同的 package，一个 import 单独一行，同一个 package/module 下的内容可以写一起：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># bad</span></div><div class="line"><span class="keyword">import</span> sys, os, time</div><div class="line"></div><div class="line"><span class="comment"># good</span></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line"><span class="comment"># ok</span></div><div class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, render_template, jsonify</div></pre></td></tr></table></figure>
<ul>
<li>为了避免可能出现的命名冲突，可以使用 as 或导入上一级命名空间</li>
<li>不要出现循环导入(cyclic import)</li>
</ul>
<h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><ul>
<li>文档字符串 docstring, 是 package, module, class, method, function 级别的注释，可以通过 <strong>doc</strong> 成员访问到，注释内容在一对 “”” 符号之间</li>
<li>function, method 的文档字符串应当描述其功能、输入参数、返回值，如果有复杂的算法和实现，也需要写清楚</li>
<li>不要写错误的注释，不要无谓的注释</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># bad 无谓的注释</span></div><div class="line">x = x + <span class="number">1</span>       <span class="comment"># increase x by 1</span></div><div class="line"></div><div class="line"><span class="comment"># bad 错误的注释</span></div><div class="line">x = x - <span class="number">1</span>       <span class="comment"># increase x by 1</span></div></pre></td></tr></table></figure>
<ul>
<li>优先使用英文写注释，英文不好全部写中文，否则更加看不懂</li>
</ul>
<h2 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h2><ul>
<li>不要轻易使用 try/except</li>
<li>except 后面需要指定捕捉的异常，裸露的 except 会捕捉所有异常，意味着会隐藏潜在的问题</li>
<li>可以有多个 except 语句，捕捉多种异常，分别做异常处理</li>
<li>使用 finally 子句来处理一些收尾操作</li>
<li><p>try/except 里的内容不要太多，只在可能抛出异常的地方使用，如：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># bad</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    user = User()</div><div class="line">    user.name = <span class="string">"leon"</span></div><div class="line">    user.age = int(age) <span class="comment"># 可能抛出异常</span></div><div class="line">    user.created_at = datetime.datetime.utcnow()</div><div class="line"></div><div class="line">    db.session.add(user)</div><div class="line">    db.session.commit() <span class="comment"># 可能抛出异常</span></div><div class="line"><span class="keyword">except</span>:</div><div class="line">    db.session.rollback()</div><div class="line"></div><div class="line"><span class="comment"># better</span></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    age = int(age)</div><div class="line"><span class="keyword">except</span> (TypeError, ValueError):</div><div class="line">    <span class="keyword">return</span> <span class="comment"># 或别的操作</span></div><div class="line"></div><div class="line">user = User()</div><div class="line">user.name = <span class="string">"leon"</span></div><div class="line">user.age = age</div><div class="line">user.created_at = datetime.datetime.utcnow()</div><div class="line">db.session.add(user)</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    db.session.commit()</div><div class="line"><span class="keyword">except</span> sqlalchemy.exc.SQLAlchemyError: <span class="comment"># 或者更具体的异常</span></div><div class="line">    db.session.rollback()</div><div class="line"><span class="keyword">finally</span>:</div><div class="line">    db.session.close()</div></pre></td></tr></table></figure>
</li>
<li><p>从 <code>Exception</code> 而不是 <code>BaseException</code> 继承自定义的异常类</p>
</li>
</ul>
<h2 id="Class（类）"><a href="#Class（类）" class="headerlink" title="Class（类）"></a>Class（类）</h2><ul>
<li>显示的写明父类，如果不是继承自别的类，就继承自 object 类</li>
<li>使用 super 调用父类的方法</li>
<li>支持多继承，即同时有多个父类，建议使用 Mixin</li>
</ul>
<h2 id="编码建议"><a href="#编码建议" class="headerlink" title="编码建议"></a>编码建议</h2><hr>
<h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><ul>
<li>使用字符串的 <code>join</code> 方法拼接字符串</li>
<li>使用字符串类型的方法，而不是 <code>string</code> 模块的方法</li>
<li>使用 <code>startswith</code> 和 <code>endswith</code> 方法比较前缀和后缀</li>
<li>使用 <code>format</code> 方法格式化字符串</li>
</ul>
<h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><ul>
<li>空的 list, str, tuple, set, dict 和 0, 0.0, None 都是 False</li>
<li>使用 if some_list 而不是 if len(some_list) 判断某个 list 是否为空，其他类型同理</li>
<li>使用 is 和 is not 与单例（如 None）进行比较，而不是用 == 和 !=</li>
<li>使用 if a is not None 而不是 if not a is None</li>
<li>用 isinstance 而不是 type 判断类型</li>
<li>不要用 == 和 != 与 True 和 False 比较（除非有特殊情况，如在 sqlalchemy 中可能用到）</li>
<li><p>使用 in 操作：</p>
<ol>
<li><p>用 key in dict 而不是 dict.has_key()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># bad</span></div><div class="line"><span class="keyword">if</span> d.has_key(k):</div><div class="line">    do_something()</div><div class="line"></div><div class="line"><span class="comment"># good</span></div><div class="line"><span class="keyword">if</span> k <span class="keyword">in</span> d:</div><div class="line">    do_something()</div></pre></td></tr></table></figure>
<p>2.用 set 加速 “存在性” 检查，list 的查找是线性的，复杂度 O(n)，set 底层是 hash table, 复杂度 O(1)，但用 set 需要比 list 更多内存空间</p>
</li>
</ol>
</li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>使用列表表达式（<a href="https://www.python.org/dev/peps/pep-0202/" target="_blank" rel="external">list comprehension</a>），字典表达式(<a href="https://www.python.org/dev/peps/pep-0274/" target="_blank" rel="external">dict comprehension</a>, Python 2.7+) 和生成器(generator)</li>
<li>dict 的 get 方法可以指定默认值，但有些时候应该用 [] 操作，使得可以抛出 KeyError</li>
<li>使用 for item in list 迭代 list, for index, item in enumerate(list) 迭代 list 并获取下标</li>
<li>使用内建函数 sorted 和 list.sort 进行排序</li>
<li>适量使用 map, reduce, filter 和 lambda，使用内建的 all, any 处理多个条件的判断</li>
<li>使用 defaultdict (Python 2.5+), Counter(Python 2.7+) 等 “冷门” 但好用的标准库算法和数据结构</li>
<li>使用装饰器(decorator)</li>
<li>使用 with 语句处理上下文</li>
<li>有些时候不要对类型做太过严格的限制，利用 Python 的鸭子类型（Duck Type）特性</li>
<li>使用 logging 记录日志，配置好格式和级别</li>
<li>了解 Python 的 Magic Method：<a href="https://github.com/RafeKettler/magicmethods" target="_blank" rel="external">A Guide to Python’s Magic Methods</a>, <a href="http://pycoders-weekly-chinese.readthedocs.io/en/latest/issue6/a-guide-to-pythons-magic-methods.html" target="_blank" rel="external">Python 魔术方法指南</a></li>
<li>阅读优秀的开源代码，如 <a href="https://github.com/pallets/flask" target="_blank" rel="external">Flask</a> 框架, <a href="https://github.com/requests/requests" target="_blank" rel="external">Requests for Humans</a></li>
<li>不要重复造轮子，查看标准库、PyPi、Github、Google 等使用现有的优秀的解决方案</li>
</ul>
<h1 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h1><p>写程序的好习惯在于理解、坚持，然后就是运用自如！</p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第三十一篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;学习下 Python增强建议书  (๑• . •๑)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/8HyHkph.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://zhangslob.github.io/categories/Python/"/>
    
    
      <category term="PEP8" scheme="https://zhangslob.github.io/tags/PEP8/"/>
    
      <category term="Pycharm" scheme="https://zhangslob.github.io/tags/Pycharm/"/>
    
  </entry>
  
  <entry>
    <title>学点算法之栈的学习与应用</title>
    <link href="https://zhangslob.github.io/2018/02/05/%E5%AD%A6%E7%82%B9%E7%AE%97%E6%B3%95%E4%B9%8B%E6%A0%88%E7%9A%84%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <id>https://zhangslob.github.io/2018/02/05/学点算法之栈的学习与应用/</id>
    <published>2018-02-05T14:08:13.000Z</published>
    <updated>2018-02-05T14:12:58.317Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第三十篇原创文章
</code></pre><p>在学习<code>栈</code>前，脑海中对这个词只有一个印象：<code>客栈</code> (๑• . •๑)</p>
<p><img src="https://i.imgur.com/eCC45z9.jpg" alt=""></p>
<a id="more"></a>
<h1 id="栈是什么"><a href="#栈是什么" class="headerlink" title="栈是什么"></a>栈是什么</h1><p>栈（有时称为“后进先出栈”）是一个项的有序集合，其中添加移除新项总发生在同一端。</p>
<p>这段话初学者是懵逼的，别急，往下看。</p>
<p>对栈的一般操作：</p>
<ul>
<li>Stack() 创建一个空的新栈。 它不需要参数，并返回一个空栈。</li>
<li>push(item)将一个新项添加到栈的顶部。它需要 item 做参数并不返回任何内容。</li>
<li>pop() 从栈中删除顶部项。它不需要参数并返回 item 。栈被修改。</li>
<li>peek() 从栈返回顶部项，但不会删除它。不需要参数。 不修改栈。</li>
<li>isEmpty() 测试栈是否为空。不需要参数，并返回布尔值。</li>
<li>size() 返回栈中的 item 数量。不需要参数，并返回一个整数。</li>
</ul>
<p>例如，s 是已经创建的空栈，下图展示了栈操作序列的结果。栈中，顶部项列在最右边。 </p>
<p><img src="https://i.imgur.com/J3KWYOO.png" alt=""></p>
<blockquote>
<p>自己在心里过一遍就很好理解了</p>
</blockquote>
<h1 id="Python实现栈"><a href="#Python实现栈" class="headerlink" title="Python实现栈"></a>Python实现栈</h1><p>其实看到上面那张图，就想起了Python中 <code>list</code> 的一些用法，append、pop等，下面是使用 Python 来实现栈，也非常简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span>:</span></div><div class="line">     <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">         self.items = []</div><div class="line"></div><div class="line">     <span class="function"><span class="keyword">def</span> <span class="title">isEmpty</span><span class="params">(self)</span>:</span></div><div class="line">         <span class="keyword">return</span> self.items == []</div><div class="line"></div><div class="line">     <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, item)</span>:</span></div><div class="line">         self.items.append(item)</div><div class="line"></div><div class="line">     <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></div><div class="line">         <span class="keyword">return</span> self.items.pop()</div><div class="line"></div><div class="line">     <span class="function"><span class="keyword">def</span> <span class="title">peek</span><span class="params">(self)</span>:</span></div><div class="line">         <span class="keyword">return</span> self.items[len(self.items)<span class="number">-1</span>]</div><div class="line"></div><div class="line">     <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></div><div class="line">         <span class="keyword">return</span> len(self.items)</div></pre></td></tr></table></figure>
<p><a href="https://github.com/bnmnetp/pythonds/blob/master/basic/stack.py" target="_blank" rel="external">pythonds/basic/stack.py</a></p>
<h1 id="栈的应用：简单括号匹配（一）"><a href="#栈的应用：简单括号匹配（一）" class="headerlink" title="栈的应用：简单括号匹配（一）"></a>栈的应用：简单括号匹配（一）</h1><p>有一些正确匹配的括号字符串：</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">(()()()())</div><div class="line"></div><div class="line">(((())))</div><div class="line"></div><div class="line">(()((())()))</div></pre></td></tr></table></figure>
<p>对比那些不匹配的括号：<br><figure class="highlight clojure"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">((((((())</div><div class="line"></div><div class="line">()))</div><div class="line"></div><div class="line">(()()(()</div></pre></td></tr></table></figure></p>
<p>具有挑战的是如何编写一个算法，能够从左到右读取一串符号，并决定符号是否平衡。</p>
<p>为了解决这个问题，我们需要做一个重要的观察。从左到右处理符号时，最近开始符号必须与下一个关闭符号相匹配。此外，处理的第一个开始符号必须等待直到其匹配最后一个符号。结束符号以相反的顺序匹配开始符号。他们从内到外匹配。这是一个可以用栈解决问题的线索。</p>
<p><img src="https://i.imgur.com/SLZpBJ8.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pythonds.basic.stack <span class="keyword">import</span> Stack</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parChecker</span><span class="params">(symbolString)</span>:</span></div><div class="line">    s = Stack()</div><div class="line">    balanced = <span class="keyword">True</span></div><div class="line">    index = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> index &lt; len(symbolString) <span class="keyword">and</span> balanced:</div><div class="line">        symbol = symbolString[index]</div><div class="line">        <span class="keyword">if</span> symbol == <span class="string">"("</span>:</div><div class="line">            s.push(symbol)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">if</span> s.isEmpty():</div><div class="line">                balanced = <span class="keyword">False</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                s.pop()</div><div class="line"></div><div class="line">        index = index + <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> balanced <span class="keyword">and</span> s.isEmpty():</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line">print(parChecker(<span class="string">'((()))'</span>))</div><div class="line">print(parChecker(<span class="string">'(()'</span>))</div></pre></td></tr></table></figure>
<p><em>output</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">True</span></div><div class="line"><span class="keyword">False</span></div></pre></td></tr></table></figure></p>
<p>一旦你认为栈是保存括号的恰当的数据结构，算法是很直接的。</p>
<p>从空栈开始，从左到右处理括号字符串。如果一个符号是一个开始符号，将其作为一个信号，对应的结束符号稍后会出现。另一方面，如果符号是结束符号，弹出栈，只要弹出栈的开始符号可以匹配每个结束符号，则括号保持匹配状态。如果任何时候栈上没有出现符合开始符号的结束符号，则字符串不匹配。最后，当所有符号都被处理后，栈应该是空的。</p>
<p><img src="https://i.imgur.com/M2ZtP7O.png" alt=""></p>
<blockquote>
<p>如果有和我一样不能很好理解的，使用pycharm的debug模式，可以一步步来，看看程序就近在做什么。</p>
</blockquote>
<h1 id="括号配对问题（二）"><a href="#括号配对问题（二）" class="headerlink" title="括号配对问题（二）"></a>括号配对问题（二）</h1><p>来看看第二种匹配问题。Python程序里存在很多括号：如圆括号、方括号和花括号，每种括号都有开括号和闭括号。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">from</span> pythonds.basic.stack <span class="keyword">import</span> Stack</div><div class="line"></div><div class="line">pares = <span class="string">"()[]&#123;&#125;"</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pare_theses</span><span class="params">(text)</span>:</span></div><div class="line">        i, text_len = <span class="number">0</span>, len(text)</div><div class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">            <span class="keyword">while</span> i &lt; text_len <span class="keyword">and</span> text[i] <span class="keyword">not</span> <span class="keyword">in</span> pares:</div><div class="line">                i += <span class="number">1</span></div><div class="line">            <span class="keyword">if</span> i &gt;= text_len:</div><div class="line">                <span class="keyword">return</span></div><div class="line">            <span class="keyword">yield</span> text[i], i</div><div class="line">            i += <span class="number">1</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_pares</span><span class="params">(text)</span>:</span></div><div class="line">    open_pares = <span class="string">"([&#123;"</span></div><div class="line">    opposite = &#123;<span class="string">')'</span>: <span class="string">'('</span>, <span class="string">']'</span>: <span class="string">'['</span>, <span class="string">'&#125;'</span>: <span class="string">'&#123;'</span>&#125; <span class="comment"># 表示配对关系的字典</span></div><div class="line">    s = Stack()</div><div class="line">    <span class="keyword">for</span> pr, i <span class="keyword">in</span> pare_theses(text):</div><div class="line">        <span class="keyword">if</span> pr <span class="keyword">in</span> open_pares:  <span class="comment"># 开括号，压进栈并继续</span></div><div class="line">            s.push(pr)</div><div class="line">        <span class="keyword">elif</span> s.pop() != opposite[pr]:  <span class="comment"># 不匹配就是失败，退出</span></div><div class="line">            print(<span class="string">'Unmatching is found at'</span>, i, <span class="string">'for'</span>, pr)</div><div class="line">            <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">		<span class="comment"># else 是一次括号配对成功，什么也不做，继续</span></div><div class="line">    print(<span class="string">"All paretheses are correctly matched."</span>)</div><div class="line">    <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line"></div><div class="line">check_pares(<span class="string">'([]&#123;&#125;]'</span>)</div><div class="line">check_pares(<span class="string">'([]&#123;&#125;)'</span>)</div></pre></td></tr></table></figure>
<p><em>output</em></p>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Unmatching <span class="keyword">is</span> found <span class="keyword">at</span> <span class="number">5</span> <span class="keyword">for</span> ]</div><div class="line"><span class="keyword">All</span> paretheses are correctly matched.</div></pre></td></tr></table></figure>
<p>生成器（回忆一下）：</p>
<ul>
<li>用 yield 语句产生结果</li>
<li>可以用在需要迭代器的地方</li>
<li>函数结束导致迭代结束</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="http://interactivepython.org/runestone/static/pythonds/BasicDS/TheStackAbstractDataType.html" target="_blank" rel="external">http://interactivepython.org/runestone/static/pythonds/BasicDS/TheStackAbstractDataType.html</a></li>
<li><a href="http://www.math.pku.edu.cn/teachers/qiuzy/ds_python/courseware/index.htm" target="_blank" rel="external">http://www.math.pku.edu.cn/teachers/qiuzy/ds_python/courseware/index.htm</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第三十篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在学习&lt;code&gt;栈&lt;/code&gt;前，脑海中对这个词只有一个印象：&lt;code&gt;客栈&lt;/code&gt; (๑• . •๑)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/eCC45z9.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="https://zhangslob.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://zhangslob.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="栈" scheme="https://zhangslob.github.io/tags/%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>为什么不推荐Selenium写爬虫</title>
    <link href="https://zhangslob.github.io/2018/02/02/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E6%8E%A8%E8%8D%90Selenium%E5%86%99%E7%88%AC%E8%99%AB/"/>
    <id>https://zhangslob.github.io/2018/02/02/为什么不推荐Selenium写爬虫/</id>
    <published>2018-02-02T12:35:02.000Z</published>
    <updated>2018-02-02T12:41:05.500Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十九篇原创文章
</code></pre><p>如果可以使用 <code>Requests</code> 完成的，别用 <code>Selenium</code> (๑• . •๑)</p>
<p><img src="https://raw.githubusercontent.com/requests/requests/master/docs/_static/requests-logo-small.png" alt=""></p>
<a id="more"></a>
<p>最近在群里经常会看到有些朋友说，使用Selenium去采集网站，我看到其实内心是很难受的，哎！为什么要用Selenium呢？</p>
<p>我想说下自己的看法，欢迎各位大佬批评。</p>
<h1 id="观点"><a href="#观点" class="headerlink" title="观点"></a>观点</h1><p>如果可以使用 <code>Requests</code> 完成的，别用 <code>Selenium</code></p>
<h1 id="数据采集的顺序"><a href="#数据采集的顺序" class="headerlink" title="数据采集的顺序"></a>数据采集的顺序</h1><p>接到一个项目或者有一个采集需求时，第一步就是明确自己的需求。经常会遇到半路改需求的事情，真的很难受。</p>
<p>第二步就是去分析这个网站，这个在之前有提到过 <a href="https://zhangslob.github.io/2017/12/23/%E9%87%87%E9%9B%86%E6%96%B9%E6%A1%88%E7%AD%96%E7%95%A5%E4%B9%8BApp%E6%8A%93%E5%8C%85/">采集方案策略之App抓包</a>  :</p>
<blockquote>
<p>首先大的地方，我们想抓取某个数据源，我们要知道大概有哪些路径可以获取到数据源，基本上无外乎三种：<br>PC端网站<br>针对移动设备响应式设计的网站（也就是很多人说的H5, 虽然不一定是H5）；<br>移动App<br>原则是能抓移动App的，最好抓移动App，如果有针对移动设备优化的网站，就抓针对移动设备优化的网站，最后考虑PC网站。因为移动App基本都是API很简单，而移动设备访问优化的网站一般来讲都是结构简单清晰的HTML，而PC网站自然是最复杂的了；针对PC端网站和移动网站的做法一样，分析思路可以一起讲，移动App单独分析。</p>
</blockquote>
<p>这个时候可以借用 <code>postman</code> 来分析请求，参考 <a href="https://zhangslob.github.io/2018/01/31/%E6%88%96%E8%AE%B8%E4%BD%A0%E5%BA%94%E8%AF%A5%E5%AD%A6%E5%AD%A6-postman/">或许你应该学学 postman</a></p>
<p>然后下一步可能就是工程开始，各种配置，以及选择哪种采集方式，一般来说 Scrapy 是最好用、也是最常见的框架。当然你也可以使用 <code>requests + xpath</code> 或者 <code>Selenium</code> 。下面就我自己的看法来说说这三种采集方式。</p>
<h1 id="三种采集差异"><a href="#三种采集差异" class="headerlink" title="三种采集差异"></a>三种采集差异</h1><h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><p>在 <a href="https://scrapy.org/" target="_blank" rel="external">Scrapy 官网 </a> 上是这样写的：</p>
<blockquote>
<p>Scrapy | A Fast and Powerful Scraping and Web Crawling Framework</p>
</blockquote>
<p>关键词是 <code>Fast</code> 和 <code>Powerful</code>，使用过确实感觉如此。我感觉 <code>Scrapy</code> 就是一个全家桶，它把爬虫所需要的大部分东西（为什么不是全部，下面会说到）都集成到这个框架中，如：下载器、中间件、调度器、Spider、调试、数据流等等所有功能全部都在这一个框架中，你所需要做的只是在命令行中输入：<code>scrapy startproject yourproject</code></p>
<p>Scrapy 的缺点也是显而易见的：不支持分布式。scrapy中scheduler是运行在队列中的，而队列是在单机内存中的，服务器上爬虫是无法利用内存的队列做任何处理。但是也有解决办法，参见<a href="https://github.com/rmax/scrapy-redis" target="_blank" rel="external">rmax/scrapy-redis</a></p>
<h2 id="Requests"><a href="#Requests" class="headerlink" title="Requests"></a>Requests</h2><p>来看看 <a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">Requests的文档</a></p>
<blockquote>
<p>Requests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用。</p>
<p>警告：非专业使用其他 HTTP 库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。</p>
</blockquote>
<p>作者真幽默</p>
<p><a href="https://gist.github.com/kennethreitz/973705" target="_blank" rel="external">urllib2 VS requests</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> urllib2</div><div class="line"></div><div class="line">gh_url = <span class="string">'https://api.github.com'</span></div><div class="line"></div><div class="line">req = urllib2.Request(gh_url)</div><div class="line"></div><div class="line">password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()</div><div class="line">password_manager.add_password(<span class="keyword">None</span>, gh_url, <span class="string">'user'</span>, <span class="string">'pass'</span>)</div><div class="line"></div><div class="line">auth_manager = urllib2.HTTPBasicAuthHandler(password_manager)</div><div class="line">opener = urllib2.build_opener(auth_manager)</div><div class="line"></div><div class="line">urllib2.install_opener(opener)</div><div class="line"></div><div class="line">handler = urllib2.urlopen(req)</div><div class="line"></div><div class="line"><span class="keyword">print</span> handler.getcode()</div><div class="line"><span class="keyword">print</span> handler.headers.getheader(<span class="string">'content-type'</span>)</div><div class="line"></div><div class="line"><span class="comment"># ------</span></div><div class="line"><span class="comment"># 200</span></div><div class="line"><span class="comment"># 'application/json'</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">r = requests.get(<span class="string">'https://api.github.com'</span>, auth=(<span class="string">'user'</span>, <span class="string">'pass'</span>))</div><div class="line"></div><div class="line"><span class="keyword">print</span> r.status_code</div><div class="line"><span class="keyword">print</span> r.headers[<span class="string">'content-type'</span>]</div><div class="line"></div><div class="line"><span class="comment"># ------</span></div><div class="line"><span class="comment"># 200</span></div><div class="line"><span class="comment"># 'application/json'</span></div></pre></td></tr></table></figure>
<p>简单对比发现 <code>requests</code> 的好用之处了，刚开始学习爬虫的时候也是从 <code>urllib</code> 开始，当看到 <code>requests</code> 果断抛弃，就像看到 <code>xpath</code> 抛弃 <code>bs4</code> 一样</p>
<p><img src="https://i.imgur.com/QwKXgfH.jpg" alt=""></p>
<p>所以如果你是初学者，那么请毫不犹豫的选择 <code>requests</code> </p>
<h2 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h2><p>最后来到今天的主角 <a href="http://www.seleniumhq.org/" target="_blank" rel="external">Selenium</a>， 首先看看官方怎么说</p>
<blockquote>
<p>What is Selenium?<br>Selenium automates browsers. That’s it! What you do with that power is entirely up to you. Primarily, it is for automating web applications for testing purposes, but is certainly not limited to just that. Boring web-based administration tasks can (and should!) be automated as well.</p>
<p>Selenium has the support of some of the largest browser vendors who have taken (or are taking) steps to make Selenium a native part of their browser. It is also the core technology in countless other browser automation tools, APIs and frameworks.</p>
</blockquote>
<p>重点是：<code>it is for automating web applications for testing purposes, but is certainly not limited to just that</code>，翻译过来就是：<strong>它是用于自动化Web应用程序的测试目的，但肯定不仅限于此</strong>，简单来说，Selenium 是web自动化测试工具集，如果你去Google上搜索 Selenium ，大多结果都是 <code>利用Selenium 自动化web 测试</code>相关内容，比较出名的有<a href="http://www.cnblogs.com/fnng/category/349036.html" target="_blank" rel="external">博客园的虫师</a>，写的两本书也都是关于自动化测试方面的</p>
<p><img src="https://i.imgur.com/6D1Plgm.png" alt=""></p>
<p>至于为啥爬虫要用selenium，我在某些博客上找到有人这样说，我也不知道怎么说</p>
<blockquote>
<p>　对于一般网站来说scrapy、requests、beautifulsoup等都可以爬取，但是有些信息需要执行js才能显现，而且你肉眼所能看到的基本都能爬取下来，在学习中遇到了，就记录下来方便以后查看。</p>
<p>webdrive是selenium中一个函数：</p>
<p>from selenium import webdriver<br>driver = webdriver.Chrome()<br>driver.get(‘网址’)</p>
<p>其中PhantomJS同时可以换成Chrome、Firefox、Ie等等，但是PhantomJS是一个无头的浏览器，运行是不会跳出相应的浏览器，运行相对效率较高。在调试中可以先换成Chrome，方便调试，最后再换成PhantomJS即可。</p>
</blockquote>
<p>下面是吐槽时间，说一说 <code>Selenium</code> 的缺点：</p>
<ol>
<li><strong>速度慢</strong>。每次运行爬虫都打开一个浏览器，如果没有设置，还会加载图片、JS等等一大堆东西；</li>
<li><strong>占用资源太多</strong>。有人说，把<code>Chrome</code>换成无头浏览器<code>PhantomJS</code>，原理都是一样的，都是打开浏览器，而且很多网站会验证参数，如果对方看到你是以<code>PhantomJS</code>去访问，会BAN掉你的请求，然后你又要考虑更换请求头的事情，事情复杂程度不知道多了多少，为啥学Python？因为Python简单啊，如果有更快、更简单的库可以实现同样的功能，为什么不去使用呢？</li>
<li><strong>对网络的要求会更高</strong>。 <code>Selenium</code> 加载了很多可能对您没有价值的补充文件（如css，js和图像文件）。 与仅仅请求您真正需要的资源（使用单独的HTTP请求）相比，这可能会产生更多的流量。</li>
<li><strong>爬取规模不能太大</strong>。你有看到哪家公司用<code>Selenium</code>作为生产环境吗？</li>
<li><strong>难</strong>。学习<code>Selenium</code>的成本太高，只有我一个人觉得<code>Selenium</code>比<code>Requests</code>难一百倍吗？</li>
</ol>
<p>我能想到的就这么多了，欢迎各位大佬补充。所以，如果可以使用 <code>Requests</code> 完成的，别用 <code>Selenium</code>，OK，洗脑完成。</p>
<p>之前面试爬虫工程师有一题就是：如何处理网站的登录系统？ A.浏览器模拟   B.HTTP请求</p>
<p>如果你想做测试工程师，那肯定需要学会 <code>Selenium</code>，公司一个妹子就是测试，现在学了 <code>Selenium</code>，工作轻松了好多。</p>
<p>最后，无耻的来个广告，本公司招聘爬虫工程师，希望和你成为队友!</p>
<p><img src="https://i.imgur.com/LyndQTt.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十九篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果可以使用 &lt;code&gt;Requests&lt;/code&gt; 完成的，别用 &lt;code&gt;Selenium&lt;/code&gt; (๑• . •๑)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/requests/requests/master/docs/_static/requests-logo-small.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="https://zhangslob.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Selenium" scheme="https://zhangslob.github.io/tags/Selenium/"/>
    
  </entry>
  
  <entry>
    <title>或许你应该学学 postman</title>
    <link href="https://zhangslob.github.io/2018/01/31/%E6%88%96%E8%AE%B8%E4%BD%A0%E5%BA%94%E8%AF%A5%E5%AD%A6%E5%AD%A6-postman/"/>
    <id>https://zhangslob.github.io/2018/01/31/或许你应该学学-postman/</id>
    <published>2018-01-31T14:59:36.000Z</published>
    <updated>2018-01-31T15:42:34.859Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十八篇原创文章
</code></pre><p>简单模拟请求的工具 (๑• . •๑)</p>
<p><img src="https://i.imgur.com/7BTDQgq.png" alt=""></p>
<a id="more"></a>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>最简单的方法就是直接在浏览器中复制 <code>Copy as cURL</code> ，然后把数据导入 postman，然后 <code>send</code> ，收工。</p>
<p>我们这里拿 <a href="https://www.zhihu.com/" target="_blank" rel="external">知乎首页</a> 举例</p>
<p><img src="https://i.imgur.com/ZLG75Wm.png" alt=""></p>
<blockquote>
<p>在对应的请求下复制 cURL</p>
</blockquote>
<p>打开 postman ， 点击左上角的 <code>Import</code> ， 选择<code>Paste Raw Text</code> ，最后 <code>Import</code>，点击 <code>send</code>发送请求</p>
<p><img src="https://i.imgur.com/EzUmrVP.png" alt=""></p>
<p>发送请求之后就可以查看了，如下图，标箭头的地方可以打开看更多。比如可以预览web界面，查看 <code>Headers</code> 信息，查看状态，复制代码。</p>
<p><img src="https://i.imgur.com/QbUhi6y.png" alt=""></p>
<p>同时可以打开 <code>Headers</code> ，用来调试，哪些是需要的，哪些不需要</p>
<p><img src="https://i.imgur.com/cJSncW7.png" alt=""></p>
<p>最方便的一点是，可以直接生成对应的编程语言，并复制，例如Python的requests方法：</p>
<p><img src="https://i.imgur.com/LCxVJMe.png" alt=""></p>
<p>好了，到这里 postman 的简单功能就说完了，他的全部功能当然不止这一点，更多的就去看 <a href="https://www.getpostman.com/docs/" target="_blank" rel="external">文档啦</a></p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>在我的使用过程中，发现了 postman 的一些问题，如：导入错误，参数错误，请求失误。</p>
<h2 id="导入错误"><a href="#导入错误" class="headerlink" title="导入错误"></a>导入错误</h2><p>例如知乎这个例子，如果我们复制的是 <code>Copy as cURL (cmd)</code> ，可能你会遇到下面的错误</p>
<p><img src="https://i.imgur.com/dCZ5PAy.png" alt=""></p>
<p>这个时候选用 <code>Copy as cURL (bush)</code> 就好了，具体原因是啥，我也不清楚。我在这里找到了别人的描述</p>
<blockquote>
<p>There is no difference between the two cURL command because there is a difference between ” and ‘.</p>
</blockquote>
<p>Refer : <a href="http://technote.thispage.me/index.php/2016/12/23/use-curl-to-get-the-same-results-as-a-web-browser/" target="_blank" rel="external">Use cURL to get the same results as a web browser</a></p>
<h2 id="参数错误"><a href="#参数错误" class="headerlink" title="参数错误"></a>参数错误</h2><p>举个例子，今天在帮朋友查看 <a href="https://www.crunchbase.com/organization/500-startups/investments/investments_list" target="_blank" rel="external">这个网站</a> 的翻页，复制用postman打开， copy cURL 内容是</p>
<figure class="highlight parser3"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="xml">curl "https://www.crunchbase.com/v4/data/entities/organizations/</span><span class="number">56e40</span><span class="xml">f50</span><span class="number">-97</span><span class="xml">c7</span><span class="number">-2</span><span class="xml">a77</span><span class="number">-255</span><span class="xml">d</span><span class="number">-1</span><span class="xml">d97d5f30646/overrides?field_ids=^%</span><span class="keyword">^5B</span><span class="xml">^%</span><span class="keyword">^22identifier</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">,^%</span><span class="keyword">^22layout_id</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">,^%</span><span class="keyword">^22facet_ids</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">,^%</span><span class="keyword">^22title</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">,^%</span><span class="keyword">^22short_description</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">,^%</span><span class="keyword">^22is_locked</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^5D</span><span class="xml">^&amp;card_ids=^%</span><span class="keyword">^5B</span><span class="xml">^%</span><span class="keyword">^22investments_list</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^5D</span><span class="xml">" -H "cookie: _ga=GA1</span><span class="number">.2</span><span class="xml"></span><span class="number">.35962729</span><span class="xml"></span><span class="number">.1517412509</span><span class="xml">; _gid=GA1</span><span class="number">.2</span><span class="xml"></span><span class="number">.2072770006</span><span class="xml"></span><span class="number">.1517412509</span><span class="xml">; _vdl=</span><span class="number">1</span><span class="xml">; _hp2_ses_props</span><span class="number">.973801186</span><span class="xml">=^%</span><span class="keyword">^7B</span><span class="xml">^%</span><span class="keyword">^22ts</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A1517412512548</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22d</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^22www.crunchbase.com</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22h</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^2Fsearch</span><span class="xml">^%</span><span class="keyword">^2Fprincipal.investors</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^7D</span><span class="xml">; __qca=P0</span><span class="number">-1969245879</span><span class="xml"></span><span class="number">-1517412512628</span><span class="xml">; D_IID=</span><span class="number">1</span><span class="xml">B7344D2</span><span class="number">-1</span><span class="xml">C8F</span><span class="number">-3327</span><span class="xml"></span><span class="number">-8607</span><span class="xml">-D786306444AE; D_UID=</span><span class="number">208</span><span class="xml">F925B</span><span class="number">-3</span><span class="xml">D1C</span><span class="number">-3491</span><span class="xml">-A532-C82375EE187D; D_ZID=</span><span class="number">497</span><span class="xml">DB63C</span><span class="number">-5101</span><span class="xml"></span><span class="number">-3</span><span class="xml">F49-BE35</span><span class="number">-1752</span><span class="xml">A80F8DDA; D_ZUID=D89FCBAA-BF79</span><span class="number">-340</span><span class="xml">C-BF55-B860768D0993; D_HID=</span><span class="number">57</span><span class="xml">B19D5F</span><span class="number">-5069</span><span class="xml"></span><span class="number">-3</span><span class="xml">A82</span><span class="number">-94</span><span class="xml">CB-D42821D1CD10; D_SID=</span><span class="number">123.120</span><span class="xml"></span><span class="number">.141</span><span class="xml"></span><span class="number">.63</span><span class="xml">:bXaeU41PWi5vyYIflFmiShQiK1qwq/nC4G9IljWo+</span><span class="number">6</span><span class="xml">A; AMCVS_6B25357E519160E40A490D44^%</span><span class="keyword">^40AdobeOrg</span><span class="xml">=</span><span class="number">1</span><span class="xml">; wcsid=KZbgLoopx4WnyMOW3F6pZ0H92JEzMrBd; hblid=cfw6lOKzm4FpCUou3F6pZ0H92JE6rBWB; s_cc=true; AMCV_6B25357E519160E40A490D44^%</span><span class="keyword">^40AdobeOrg</span><span class="xml">=</span><span class="number">1099438348</span><span class="xml">^%</span><span class="keyword">^7CMCMID</span><span class="xml">^%</span><span class="keyword">^7C05859477990281579603868663655860142263</span><span class="xml">^%</span><span class="keyword">^7CMCAAMLH-1518017313</span><span class="xml">^%</span><span class="keyword">^7C11</span><span class="xml">^%</span><span class="keyword">^7CMCAAMB-1518017313</span><span class="xml">^%</span><span class="keyword">^7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y</span><span class="xml">^%</span><span class="keyword">^7CMCOPTOUT-1517419713s</span><span class="xml">^%</span><span class="keyword">^7CNONE</span><span class="xml">^%</span><span class="keyword">^7CMCAID</span><span class="xml">^%</span><span class="keyword">^7CNONE</span><span class="xml">^%</span><span class="keyword">^7CMCSYNCSOP</span><span class="xml">^%</span><span class="keyword">^7C411-17570</span><span class="xml">^%</span><span class="keyword">^7CvVersion</span><span class="xml">^%</span><span class="keyword">^7C2.1.0</span><span class="xml">; _okdetect=^%</span><span class="keyword">^7B</span><span class="xml">^%</span><span class="keyword">^22token</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^2215174125149410</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22proto</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^22https</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22host</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^22www.crunchbase.com</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^7D</span><span class="xml">; olfsk=olfsk8562990481377502; _okbk=cd4^%</span><span class="keyword">^3Dtrue</span><span class="xml">^%</span><span class="keyword">^2Cvi5</span><span class="xml">^%</span><span class="keyword">^3D0</span><span class="xml">^%</span><span class="keyword">^2Cvi4</span><span class="xml">^%</span><span class="keyword">^3D1517412515909</span><span class="xml">^%</span><span class="keyword">^2Cvi3</span><span class="xml">^%</span><span class="keyword">^3Dactive</span><span class="xml">^%</span><span class="keyword">^2Cvi2</span><span class="xml">^%</span><span class="keyword">^3Dfalse</span><span class="xml">^%</span><span class="keyword">^2Cvi1</span><span class="xml">^%</span><span class="keyword">^3Dfalse</span><span class="xml">^%</span><span class="keyword">^2Ccd8</span><span class="xml">^%</span><span class="keyword">^3Dchat</span><span class="xml">^%</span><span class="keyword">^2Ccd6</span><span class="xml">^%</span><span class="keyword">^3D0</span><span class="xml">^%</span><span class="keyword">^2Ccd5</span><span class="xml">^%</span><span class="keyword">^3Daway</span><span class="xml">^%</span><span class="keyword">^2Ccd3</span><span class="xml">^%</span><span class="keyword">^3Dfalse</span><span class="xml">^%</span><span class="keyword">^2Ccd2</span><span class="xml">^%</span><span class="keyword">^3D0</span><span class="xml">^%</span><span class="keyword">^2Ccd1</span><span class="xml">^%</span><span class="keyword">^3D0</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">; _ok=</span><span class="number">1554</span><span class="xml"></span><span class="number">-355</span><span class="xml"></span><span class="number">-10</span><span class="xml"></span><span class="number">-6773</span><span class="xml">; _hp2_props</span><span class="number">.973801186</span><span class="xml">=^%</span><span class="keyword">^7B</span><span class="xml">^%</span><span class="keyword">^22Logged</span><span class="xml">^%</span><span class="keyword">^20In</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3Afalse</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22Pro</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3Afalse</span><span class="xml">^%</span><span class="keyword">^7D</span><span class="xml">; _hp2_id</span><span class="number">.973801186</span><span class="xml">=^%</span><span class="keyword">^7B</span><span class="xml">^%</span><span class="keyword">^22userId</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^228805156096536097</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22pageviewId</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^221700148784936413</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22sessionId</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^225929107734453151</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22identity</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3Anull</span><span class="xml">^%</span><span class="keyword">^2C</span><span class="xml">^%</span><span class="keyword">^22trackerVersion</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^3A</span><span class="xml">^%</span><span class="keyword">^223.0</span><span class="xml">^%</span><span class="keyword">^22</span><span class="xml">^%</span><span class="keyword">^7D</span><span class="xml">; _oklv=</span><span class="number">1517412548852</span><span class="xml">^%</span><span class="keyword">^2CKZbgLoopx4WnyMOW3F6pZ0H92JEzMrBd</span><span class="xml">; s_pers=^%</span><span class="keyword">^20s_nrgvo</span><span class="xml">^%</span><span class="keyword">^3DNew</span><span class="xml">^%</span><span class="keyword">^7C1580484574965</span><span class="xml">^%</span><span class="keyword">^3B</span><span class="xml">" -H "origin: https://www.crunchbase.com" -H "accept-encoding: gzip, deflate, br" -H "x-distil-ajax: dfdvfavtsysazfberrtudvwabwe" -H "user-agent: Mozilla/</span><span class="number">5.0</span><span class="xml"> (Windows NT </span><span class="number">6.1</span><span class="xml">; Win64; x64) AppleWebKit/</span><span class="number">537.36</span><span class="xml"> (KHTML, like Gecko) Chrome/</span><span class="number">63.0</span><span class="xml"></span><span class="number">.3239</span><span class="xml"></span><span class="number">.84</span><span class="xml"> Safari/</span><span class="number">537.36</span><span class="xml">" -H "content-type: application/json" -H "accept-language: zh-CN,zh;q=</span><span class="number">0.9</span><span class="xml">,en;q=</span><span class="number">0.8</span><span class="xml">" -H "accept: application/json, text/plain, */*" -H "referer: https://www.crunchbase.com/organization/</span><span class="number">500</span><span class="xml">-startups/investments/investments_list" -H "authority: www.crunchbase.com" -H "x-requested-with: XMLHttpRequest" --data-binary ^"^&#123;^</span></div><div class="line"></div><div class="line">  ^\^"card_lookups^\^": ^[^</div><div class="line"></div><div class="line">    ^&#123;^</div><div class="line"></div><div class="line">      ^\^"card_id^\^": ^\^"investments_list^\^",^</div><div class="line"></div><div class="line">      ^\^"limit^\^": <span class="number">100</span><span class="xml">,^</span></div><div class="line"></div><div class="line">      ^\^"after_id^\^": ^\^"<span class="number">07</span><span class="xml">a9c686</span><span class="number">-4590</span><span class="xml">-fa0f</span><span class="number">-3</span><span class="xml">ac4-fc7b898c0b7a^\^"^</span></div><div class="line"></div><div class="line">    ^&#125;^</div><div class="line"></div><div class="line">  ^]^</div><div class="line"></div><div class="line">^&#125;^" --compressed</div></pre></td></tr></table></figure>
<p>导入之后，<code>send</code>，返回 400 错误。</p>
<p>postman 转义的code是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">url = <span class="string">"https://www.crunchbase.com/v4/data/entities/organizations/56e40f50-97c7-2a77-255d-1d97d5f30646/overrides"</span></div><div class="line"></div><div class="line">querystring = &#123;<span class="string">"field_ids"</span>:<span class="string">"^%^5B^%^22identifier^%^22,^%^22layout_id^%^22,^%^22facet_ids^%^22,^%^22title^%^22,^%^22short_description^%^22,^%^22is_locked^%^22^%^5D^"</span>,<span class="string">"card_ids"</span>:<span class="string">"^%^5B^%^22investments_list^%^22^%^5D"</span>&#125;</div><div class="line"></div><div class="line">payload = <span class="string">"^^&#123;^\n\n  ^\\^card_lookups^^: ^[^\n\n    ^&#123;^\n\n      ^\\^card_id^^: ^\\^investments_list^^,^\n\n      ^\\^limit^^: 100,^\n\n      ^\\^after_id^^: ^\\^07a9c686-4590-fa0f-3ac4-fc7b898c0b7a^^^\n\n    ^&#125;^\n\n  ^]^\n\n^&#125;^"</span></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">'cookie'</span>: <span class="string">"_ga=GA1.2.35962729.1517412509; _gid=GA1.2.2072770006.1517412509; _vdl=1; _hp2_ses_props.973801186=^%^7B^%^22ts^%^22^%^3A1517412512548^%^2C^%^22d^%^22^%^3A^%^22www.crunchbase.com^%^22^%^2C^%^22h^%^22^%^3A^%^22^%^2Fsearch^%^2Fprincipal.investors^%^22^%^7D; __qca=P0-1969245879-1517412512628; D_IID=1B7344D2-1C8F-3327-8607-D786306444AE; D_UID=208F925B-3D1C-3491-A532-C82375EE187D; D_ZID=497DB63C-5101-3F49-BE35-1752A80F8DDA; D_ZUID=D89FCBAA-BF79-340C-BF55-B860768D0993; D_HID=57B19D5F-5069-3A82-94CB-D42821D1CD10; D_SID=123.120.141.63:bXaeU41PWi5vyYIflFmiShQiK1qwq/nC4G9IljWo+6A; AMCVS_6B25357E519160E40A490D44^%^40AdobeOrg=1; wcsid=KZbgLoopx4WnyMOW3F6pZ0H92JEzMrBd; hblid=cfw6lOKzm4FpCUou3F6pZ0H92JE6rBWB; s_cc=true; AMCV_6B25357E519160E40A490D44^%^40AdobeOrg=1099438348^%^7CMCMID^%^7C05859477990281579603868663655860142263^%^7CMCAAMLH-1518017313^%^7C11^%^7CMCAAMB-1518017313^%^7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y^%^7CMCOPTOUT-1517419713s^%^7CNONE^%^7CMCAID^%^7CNONE^%^7CMCSYNCSOP^%^7C411-17570^%^7CvVersion^%^7C2.1.0; _okdetect=^%^7B^%^22token^%^22^%^3A^%^2215174125149410^%^22^%^2C^%^22proto^%^22^%^3A^%^22https^%^3A^%^22^%^2C^%^22host^%^22^%^3A^%^22www.crunchbase.com^%^22^%^7D; olfsk=olfsk8562990481377502; _okbk=cd4^%^3Dtrue^%^2Cvi5^%^3D0^%^2Cvi4^%^3D1517412515909^%^2Cvi3^%^3Dactive^%^2Cvi2^%^3Dfalse^%^2Cvi1^%^3Dfalse^%^2Ccd8^%^3Dchat^%^2Ccd6^%^3D0^%^2Ccd5^%^3Daway^%^2Ccd3^%^3Dfalse^%^2Ccd2^%^3D0^%^2Ccd1^%^3D0^%^2C; _ok=1554-355-10-6773; _hp2_props.973801186=^%^7B^%^22Logged^%^20In^%^22^%^3Afalse^%^2C^%^22Pro^%^22^%^3Afalse^%^7D; _hp2_id.973801186=^%^7B^%^22userId^%^22^%^3A^%^228805156096536097^%^22^%^2C^%^22pageviewId^%^22^%^3A^%^221700148784936413^%^22^%^2C^%^22sessionId^%^22^%^3A^%^225929107734453151^%^22^%^2C^%^22identity^%^22^%^3Anull^%^2C^%^22trackerVersion^%^22^%^3A^%^223.0^%^22^%^7D; _oklv=1517412548852^%^2CKZbgLoopx4WnyMOW3F6pZ0H92JEzMrBd; s_pers=^%^20s_nrgvo^%^3DNew^%^7C1580484574965^%^3B"</span>,</div><div class="line">    <span class="string">'origin'</span>: <span class="string">"https://www.crunchbase.com"</span>,</div><div class="line">    <span class="string">'accept-encoding'</span>: <span class="string">"gzip, deflate, br"</span>,</div><div class="line">    <span class="string">'x-distil-ajax'</span>: <span class="string">"dfdvfavtsysazfberrtudvwabwe"</span>,</div><div class="line">    <span class="string">'user-agent'</span>: <span class="string">"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36"</span>,</div><div class="line">    <span class="string">'content-type'</span>: <span class="string">"application/json"</span>,</div><div class="line">    <span class="string">'accept-language'</span>: <span class="string">"zh-CN,zh;q=0.9,en;q=0.8"</span>,</div><div class="line">    <span class="string">'accept'</span>: <span class="string">"application/json, text/plain, */*"</span>,</div><div class="line">    <span class="string">'referer'</span>: <span class="string">"https://www.crunchbase.com/organization/500-startups/investments/investments_list"</span>,</div><div class="line">    <span class="string">'authority'</span>: <span class="string">"www.crunchbase.com"</span>,</div><div class="line">    <span class="string">'x-requested-with'</span>: <span class="string">"XMLHttpRequest"</span>,</div><div class="line">    <span class="string">'cache-control'</span>: <span class="string">"no-cache"</span>,</div><div class="line">    <span class="string">'postman-token'</span>: <span class="string">"1df3b2b6-b682-edf7-4804-572ac5a03420"</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">response = requests.request(<span class="string">"POST"</span>, url, data=payload, headers=headers, params=querystring)</div><div class="line"></div><div class="line">print(response.text)</div></pre></td></tr></table></figure>
<p>可以看到 加入了大量的 <code>^</code> 符号，这个在Python中是运算符<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">^	按位异或运算符：当两对应的二进位相异时，结果为<span class="number">1</span>	(a ^ b) 输出结果 <span class="number">49</span> ，二进制解释： <span class="number">0011</span> <span class="number">0001</span></div></pre></td></tr></table></figure></p>
<p>这也是 postman 的一个问题</p>
<h2 id="请求失误"><a href="#请求失误" class="headerlink" title="请求失误"></a>请求失误</h2><p>这个问题，我也不是很懂，有的请求 postman 返回错误，但是复制代码到 Python 环境中运行是可以获得数据的，所以最好是多次验证。</p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十八篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;简单模拟请求的工具 (๑• . •๑)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/7BTDQgq.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="https://zhangslob.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="postman" scheme="https://zhangslob.github.io/tags/postman/"/>
    
  </entry>
  
  <entry>
    <title>有了她，谁敢阻止我学习</title>
    <link href="https://zhangslob.github.io/2018/01/30/%E6%9C%89%E4%BA%86%E5%A5%B9%EF%BC%8C%E8%B0%81%E6%95%A2%E9%98%BB%E6%AD%A2%E6%88%91%E5%AD%A6%E4%B9%A0/"/>
    <id>https://zhangslob.github.io/2018/01/30/有了她，谁敢阻止我学习/</id>
    <published>2018-01-30T13:31:40.000Z</published>
    <updated>2018-01-30T13:44:02.289Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十七篇原创文章
</code></pre><p>我爱学习 (๑• . •๑)</p>
<p><img src="https://i.imgur.com/Y5AMeSm.jpg" alt=""></p>
<a id="more"></a>
<h1 id="公众号"><a href="#公众号" class="headerlink" title="公众号"></a>公众号</h1><p><strong>Python爱好者社区，微信号：python_shequ</strong></p>
<p>人生苦短，我用Python。分享Python相关的技术文章、工具资源、精选课程、视频教程、热点资讯、学习资料等。每天自动更新和推送。</p>
<p><strong>Python爬虫分享，微信号：python_shequ</strong></p>
<p>知乎专栏“爬虫从入门到放弃”作者。目前在知乎连载爬虫从入门到放弃到精通系列文章</p>
<p><strong>张俊红，微信号：zhangjunhong0428</strong></p>
<p>中国统计网专栏作者，数据分析路上的学习者与实践者，与你分享我的所见、所学、所想。</p>
<p><strong>数据小魔方，微信号：datamofang</strong></p>
<p>专注于数据可视化及商务智能的原创技能分享平台！</p>
<p><strong>Python爬虫与算法进阶，微信号：zhangslob</strong></p>
<p>分享Python知识，关注爬虫与算法,让我们一起从萌新变成大牛吧!</p>
<h1 id="在线书籍"><a href="#在线书籍" class="headerlink" title="在线书籍"></a>在线书籍</h1><ol>
<li><a href="https://github.com/taizilongxu/interview_python" target="_blank" rel="external">关于Python的面试题</a></li>
<li><a href="https://www.readwithu.com/" target="_blank" rel="external">草根学Python</a></li>
<li><a href="https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/" target="_blank" rel="external">python数据结构和算法</a></li>
<li><a href="http://pythonguidecn.readthedocs.io/zh/latest/" target="_blank" rel="external">Python最佳实践指南！</a></li>
<li><a href="http://python3-cookbook.readthedocs.io/zh_CN/latest/index.html" target="_blank" rel="external">Python Cookbook 3rd Edition Documentation</a></li>
<li><a href="https://taizilongxu.gitbooks.io/stackoverflow-about-python/content/" target="_blank" rel="external">Stackoverflow about Python</a></li>
<li><a href="https://eastlakeside.gitbooks.io/interpy-zh/content/" target="_blank" rel="external">Python进阶</a></li>
</ol>
<h1 id="免费视频"><a href="#免费视频" class="headerlink" title="免费视频"></a>免费视频</h1><ol>
<li><a href="http://study.163.com/course/courseMain.htm?courseId=378003" target="_blank" rel="external">零基础入门学习Python</a> 话说我当时就看的小甲鱼视频入门的</li>
<li><a href="https://www.icourse163.org/course/BIT-268001?tid=1002001005" target="_blank" rel="external">Python语言程序设计</a> 嵩天老师的一系列课程</li>
<li><a href="https://chinagdg.org/2016/03/machine-learning-recipes-for-new-developers/" target="_blank" rel="external">Google机器学习</a> 简单入门</li>
<li><a href="https://www.coursera.org/learn/suanfa-jichu/home/welcome" target="_blank" rel="external">算法基础</a> 北大博士的课程。。</li>
<li><a href="http://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="external">吴恩达机器学习</a> 这个就不评价了，都知道</li>
</ol>
<h1 id="爬虫教程"><a href="#爬虫教程" class="headerlink" title="爬虫教程"></a>爬虫教程</h1><ol>
<li><a href="https://piaosanlang.gitbooks.io/spiders/" target="_blank" rel="external">爬虫教程</a> 某位大佬所写，由浅入深</li>
<li><a href="http://lawtech0902.com/categories/Scrapy/" target="_blank" rel="external">Scrapy分类</a> 质量非常高</li>
<li><a href="https://germey.gitbooks.io/python3webspider/content/" target="_blank" rel="external">Python3网络爬虫实战</a> 崔大的书，马上会出版</li>
<li><a href="https://liangqingyu.gitbooks.io/bigdata-crawler/content/" target="_blank" rel="external">大数据之网络爬虫</a> 主要是垂直型网络爬虫</li>
<li><a href="https://www.biaodianfu.com/scrapy-redis.html" target="_blank" rel="external">使用Scrapy-redis实现分布式爬取</a> 分布式资料</li>
<li><a href="http://zhxfei.com/2017/06/03/scrapy_redis/" target="_blank" rel="external">使用Docker部署scrapy-redis分布式爬虫</a> 同样是分布式资料</li>
</ol>
<p>最重要的一点是：不要光收藏，不去看</p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十七篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我爱学习 (๑• . •๑)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Y5AMeSm.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Git" scheme="https://zhangslob.github.io/categories/Git/"/>
    
    
      <category term="Git" scheme="https://zhangslob.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>烦人的Git ( ´•︵•` )</title>
    <link href="https://zhangslob.github.io/2018/01/29/%E7%83%A6%E4%BA%BA%E7%9A%84Git%20%E7%BB%88%E4%BA%8E%E5%AD%A6%E4%BC%9A%E4%BA%86%20-%C2%B4%E2%80%A2%EF%B8%B5%E2%80%A2/"/>
    <id>https://zhangslob.github.io/2018/01/29/烦人的Git 终于学会了 -´•︵•/</id>
    <published>2018-01-29T13:30:41.000Z</published>
    <updated>2018-01-29T13:34:38.856Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十六篇原创文章
</code></pre><p>终于会用Git了 (๑• . •๑)</p>
<p><img src="https://i.imgur.com/WPD1iB7.jpg" alt=""></p>
<a id="more"></a>
<p>Git对于新手来说，真的很烦人哎，好在找到了好工具 ——　PyCharm</p>
<h1 id="使用PyCharm进行代码管理"><a href="#使用PyCharm进行代码管理" class="headerlink" title="使用PyCharm进行代码管理"></a>使用PyCharm进行代码管理</h1><p>在VCS里有Git，里面有常用的操作，clone、pull、push等等。</p>
<p><img src="https://i.imgur.com/DALuEKz.png" alt=""></p>
<p>更方便的是，在左下角，有Version Control，可以清晰的看到日志改变，图中另一个标记的位置可以直接进行commit，如下图所示</p>
<p><img src="https://i.imgur.com/32W836Q.png" alt=""></p>
<p><img src="https://i.imgur.com/7Zx6Nps.png" alt=""></p>
<p>更多参考官方文档 <a href="https://www.jetbrains.com/help/pycharm/enabling-version-control.html" target="_blank" rel="external">Enabling Version Control</a></p>
<p>我修改了主题，可以在这里改 <a href="https://plugins.jetbrains.com/plugin/8006-material-theme-ui" target="_blank" rel="external">Material Theme UI</a></p>
<p><img src="https://plugins.jetbrains.com/files/8006/screenshot_17528.png" alt=""></p>
<p>emmm，简单的方法就是这样，还是需要了解下Git的基本操作的</p>
<h1 id="创建新仓库"><a href="#创建新仓库" class="headerlink" title="创建新仓库"></a>创建新仓库</h1><p>创建新文件夹，打开，然后执行 </p>
<p><code>git init</code></p>
<p>以创建新的 git 仓库。</p>
<p>或者按照官方推荐的例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">git clone git@github.com:zhangslob/test_test.git</div><div class="line">echo <span class="string">"# test_test"</span> &gt;&gt; README.md</div><div class="line">git init</div><div class="line">git add README.md</div><div class="line">git commit -m <span class="string">"first commit"</span></div><div class="line">git remote add origin git@github.com:zhangslob/test_test.git</div><div class="line">git push -u origin master</div><div class="line"></div><div class="line"><span class="comment"># 其中的命令下面会讲到</span></div></pre></td></tr></table></figure>
<h1 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h1><p>执行如下命令以创建一个本地仓库的克隆版本：</p>
<p><code>git clone /path/to/repository</code></p>
<p>如果是远端服务器上的仓库，你的命令会是这个样子：</p>
<p><code>git clone username@host:/path/to/repository</code></p>
<h1 id="添加和提交"><a href="#添加和提交" class="headerlink" title="添加和提交"></a>添加和提交</h1><p>你可以提出更改（把它们添加到暂存区），使用如下命令：</p>
<p><code>git add .</code></p>
<p>这是 git 基本工作流程的第一步；使用如下命令以实际提交改动：</p>
<p><code>git commit -m &quot;代码提交信息&quot;</code></p>
<p>现在，你的改动已经提交到了 HEAD，但是还没到你的远端仓库。</p>
<h1 id="推送改动"><a href="#推送改动" class="headerlink" title="推送改动"></a>推送改动</h1><p>你的改动现在已经在本地仓库的 HEAD 中了。执行如下命令以将这些改动提交到远端仓库：</p>
<p><code>git push origin master</code></p>
<p>可以把 master 换成你想要推送的任何分支。 </p>
<h1 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h1><p>分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认的”分支。在其他分支上进行开发，完成后再将它们合并到主分支上。</p>
<p>创建一个叫做“develop”的分支，并切换过去：</p>
<p><code>git checkout -b develop</code></p>
<p>切换回主分支：</p>
<p><code>git checkout master</code></p>
<p>再把新建的分支删掉：</p>
<p><code>git branch -d develop</code></p>
<p>除非你将分支推送到远端仓库，不然该分支就是 不为他人所见的：</p>
<p><code>git push origin &lt;branch&gt;</code></p>
<h1 id="更新与合并"><a href="#更新与合并" class="headerlink" title="更新与合并"></a>更新与合并</h1><p>要更新你的本地仓库至最新改动，执行：</p>
<p><code>git pull</code></p>
<p>以在你的工作目录中 获取（fetch） 并 合并（merge） 远端的改动。</p>
<p>要合并其他分支到你的当前分支（例如 master），执行：</p>
<p><code>git merge &lt;branch&gt;</code></p>
<p>在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现冲突（conflicts）。 这时候就需要你修改这些文件来手动合并这些冲突（conflicts）。改完之后，你需要执行如下命令以将它们标记为合并成功：</p>
<p><code>git add &lt;filename&gt;</code></p>
<p>在合并改动之前，你可以使用如下命令预览差异：</p>
<p><code>git diff &lt;source_branch&gt; &lt;target_branch&gt;</code></p>
<h1 id="替换本地改动"><a href="#替换本地改动" class="headerlink" title="替换本地改动"></a>替换本地改动</h1><p>假如你操作失误，你可以使用如下命令替换掉本地改动：</p>
<p><code>git checkout -- &lt;filename&gt;</code></p>
<p>此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到暂存区的改动以及新文件都不会受到影响。</p>
<p>假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git fetch <span class="built_in">origin</span></div><div class="line">git <span class="built_in">reset</span> --hard <span class="built_in">origin</span>/master</div></pre></td></tr></table></figure></p>
<h1 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h1><ol>
<li>使用 <code>git status</code> 可以迅速查看当前状态，若添加文件，会显示绿色的文字</li>
<li>使用Github的客户端，可以事半功倍 <a href="https://desktop.github.com/" target="_blank" rel="external">https://desktop.github.com/</a></li>
</ol>
<p>更多可以参考</p>
<p><a href="https://git-scm.com/book/zh/v2" target="_blank" rel="external">Git 文档</a></p>
<h1 id="Git-Commit-Message-模版"><a href="#Git-Commit-Message-模版" class="headerlink" title="Git Commit Message 模版"></a>Git Commit Message 模版</h1><h2 id="模版命名约定"><a href="#模版命名约定" class="headerlink" title="模版命名约定"></a>模版命名约定</h2><p>模版名称由以下部分组成</p>
<p><code>gitmessage-${语言}[-full]</code></p>
<p>以 <code>-full</code> 结尾的模版包含了 message 组成区域的解释以及书写的指导，推荐新手选择该模版上手，在熟悉书写规范后换成简单版本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#    feat     (新功能)</div><div class="line">#    fix      (问题修复)</div><div class="line">#    refactor (代码重构)</div><div class="line">#    style    (代码风格改动、格式变化等，无实现改动)</div><div class="line">#    docs     (文档更新)</div><div class="line">#    test     (增加、重构测试，无实现改动)</div><div class="line">#    chore    (修改一些配置文件如 .gitignore 等，无实现改动)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十六篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;终于会用Git了 (๑• . •๑)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/WPD1iB7.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Git" scheme="https://zhangslob.github.io/categories/Git/"/>
    
    
      <category term="Git" scheme="https://zhangslob.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>学点算法之字符串的乱序检查</title>
    <link href="https://zhangslob.github.io/2018/01/27/%E5%AD%A6%E7%82%B9%E7%AE%97%E6%B3%95%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E4%B9%B1%E5%BA%8F%E6%A3%80%E6%9F%A5/"/>
    <id>https://zhangslob.github.io/2018/01/27/学点算法之字符串的乱序检查/</id>
    <published>2018-01-27T10:52:56.000Z</published>
    <updated>2018-01-27T10:59:40.517Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十五篇原创文章
</code></pre><p>老板，我要做算法工程师！</p>
<p><img src="https://i.imgur.com/z10Tn6k.jpg" alt=""><br><a id="more"></a></p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>字符串的乱序检查。</p>
<p>一个字符串是另一个字符串的乱序。如果第二个字符串只是第一个的重新排列，例如，’heart’ 和 ‘earth’ 就是乱序字符串。’python’ 和 ‘typhon’ 也是。为了简单起见，我们假设所讨论的两个字符串具有<strong>相等的长度</strong>，并且他们由 26 个小写字母集合组成。我们的目标是写一个布尔函数，它将两个字符串做参数并返回它们是不是回文。</p>
<h1 id="解法1-检查"><a href="#解法1-检查" class="headerlink" title="解法1:检查"></a>解法1:检查</h1><p>我们对乱序问题的第一个解法是检查第一个字符串是不是出现在第二个字符串中。如果可以检验到每一个字符，那两个字符串一定是回文。可以通过用 None 替换字符来完成检查。但是，由于 Python 字符串是不可变的，所以第一步是将第二个字符串转换为列表。第一个字符串中的每个字符可以通过检查在第二个列表中检查元素是否存在，如果存在，替换成 None。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">anagramSolution1</span><span class="params">(s1,s2)</span>:</span></div><div class="line">    alist = list(s2)</div><div class="line"></div><div class="line">    pos1 = <span class="number">0</span></div><div class="line">    stillOK = <span class="keyword">True</span></div><div class="line"></div><div class="line">    <span class="keyword">while</span> pos1 &lt; len(s1) <span class="keyword">and</span> stillOK:</div><div class="line">        pos2 = <span class="number">0</span></div><div class="line">        found = <span class="keyword">False</span></div><div class="line">        <span class="keyword">while</span> pos2 &lt; len(alist) <span class="keyword">and</span> <span class="keyword">not</span> found:</div><div class="line">            <span class="keyword">if</span> s1[pos1] == alist[pos2]:</div><div class="line">                found = <span class="keyword">True</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                pos2 = pos2 + <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> found:</div><div class="line">            alist[pos2] = <span class="keyword">None</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            stillOK = <span class="keyword">False</span></div><div class="line"></div><div class="line">        pos1 = pos1 + <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> stillOK</div><div class="line"></div><div class="line">print(anagramSolution1(<span class="string">'abcd'</span>,<span class="string">'dcba'</span>))</div></pre></td></tr></table></figure>
<p>s1 的每个字符都会在 s2 中进行最多 n 个字符的迭代</p>
<p>s2 列表中的 n 个位置将被访问一次来匹配来自 s1 的字符。访问次数可以写成 1 到 n 整数的和，可以写成</p>
<p><img src="https://xidianwlc.gitbooks.io/python-data-structrue-and-algrothms/content/2.%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/2.4.%E4%B8%80%E4%B8%AA%E4%B9%B1%E5%BA%8F%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A3%80%E6%9F%A5%E7%9A%84%E4%BE%8B%E5%AD%90/assets/2.4.1%20%E6%B1%82%E5%92%8C.png" alt=""></p>
<p>当 n 变大，n^2 这项占据主导，1/2 可以忽略。所以这个算法复杂度为 O(n^2 )。</p>
<p><img src="https://wx1.sinaimg.cn/orj360/b04b21b7ly1fnssg4x3mdj20dc0dcgno.jpg" alt=""></p>
<h1 id="解法2-排序和比较"><a href="#解法2-排序和比较" class="headerlink" title="解法2:排序和比较"></a>解法2:排序和比较</h1><p>另一个解决方案是利用这么一个事实，即使 s1,s2 不同，它们只有由完全相同的字符组成，它们才是回文。所以，如果我们按照字母顺序排列每个字符串，从 a 到 z，如果两个字符串相同，则这两个字符串为回文。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">anagramSolution2</span><span class="params">(s1,s2)</span>:</span></div><div class="line">    alist1 = list(s1)</div><div class="line">    alist2 = list(s2)</div><div class="line"></div><div class="line">    alist1.sort()</div><div class="line">    alist2.sort()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> alist1 == alist2:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line">print(anagramSolution2(<span class="string">'abcde'</span>,<span class="string">'edcba'</span>))</div></pre></td></tr></table></figure>
<p>这个算法比较简单，只用到了排序算法，那么排序算法的复杂度还是多少呢？</p>
<p>在<a href="https://github.com/qiwsir/algorithm/blob/master/python_sort.md" target="_blank" rel="external">这里</a>找到了答案</p>
<p>python中的sorted算法，网上有人撰文，说比较低级。其实不然，通过阅读官方文档，发现python中的sorted排序，真的是高大上，用的Timsort算法。什么是Timsort，请看 wiki的解释：<a href="http://en.wikipedia.org/wiki/Timsort" target="_blank" rel="external">http://en.wikipedia.org/wiki/Timsort</a></p>
<p>另外，国内有一个文档，适当翻译：<a href="http://blog.csdn.net/yangzhongblog/article/details/8184707" target="_blank" rel="external">http://blog.csdn.net/yangzhongblog/article/details/8184707</a>，这里截取一个不同排序算法比较的图示，就明白sorted的威力了。</p>
<p>从时间复杂度来看，Timsort是威武的。</p>
<p><img src="https://github.com/qiwsir/algorithm/raw/master/pics/timsort1.png" alt=""></p>
<p>从空间复杂度来讲，需要的开销在数量大的时候会增大。</p>
<p><img src="https://github.com/qiwsir/algorithm/raw/master/pics/timsort2.png" alt=""></p>
<p><img src="https://wx2.sinaimg.cn/orj360/b04b21b7ly1fnssg5nhwuj20dc0dcmz2.jpg" alt=""></p>
<h1 id="解法3-穷举法"><a href="#解法3-穷举法" class="headerlink" title="解法3: 穷举法"></a>解法3: 穷举法</h1><p>解决这类问题的强力方法是<strong>穷举所有可能性</strong>。</p>
<p>对于回文检测，我们可以生成 s1 的所有乱序字符串列表，然后查看是不是有 s2。这种方法有一点困难。当 s1 生成所有可能的字符串时，第一个位置有 n 种可能，第二个位置有 n-1 种，第三个位置有 n-3 种，等等。总数为 <code>n∗(n−1)∗(n−2)∗...∗3∗2∗1n∗(n−1)∗(n−2)∗...∗3∗2∗1</code>， 即 <code>n!</code> 。</p>
<p>虽然一些字符串可能是重复的，程序也不可能提前知道这样，所以他仍然会生成 <code>n!</code> 个字符串。<br>事实证明，n! 比 n^2 增长还快，事实上，如果 s1 有 20个字符长，则将有 <code>20! = 2,432,902,008,176,640,000</code> 个字符串产生。如果我们每秒处理一种可能字符串，那么需要 <code>77,146,816,596</code> 年才能过完整个列表。</p>
<p>所以当然不会采取这种方案了。</p>
<p><img src="https://wx2.sinaimg.cn/orj360/b04b21b7ly1fnssg6fej0j20dc0dc40h.jpg" alt=""></p>
<h1 id="解法4-计数和比较"><a href="#解法4-计数和比较" class="headerlink" title="解法4: 计数和比较"></a>解法4: 计数和比较</h1><p>我们最终解决回文的方法是利用两个乱序字符串具有相同的 a, b, c 等等的事实。</p>
<p>我们首先计算的是每个字母出现的次数。由于有 26 个可能的字符，我们就用 一个长度为 26 的列表，每个可能的字符占一个位置。每次看到一个特定的字符，就增加该位置的计数器。最后如果两个列表的计数器一样，则字符串为乱序字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">anagramSolution4</span><span class="params">(s1,s2)</span>:</span></div><div class="line">    c1 = [<span class="number">0</span>]*<span class="number">26</span></div><div class="line">    c2 = [<span class="number">0</span>]*<span class="number">26</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(s1)):</div><div class="line">        pos = ord(s1[i])-ord(<span class="string">'a'</span>)</div><div class="line">        c1[pos] = c1[pos] + <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(s2)):</div><div class="line">        pos = ord(s2[i])-ord(<span class="string">'a'</span>)</div><div class="line">        c2[pos] = c2[pos] + <span class="number">1</span></div><div class="line"></div><div class="line">    j = <span class="number">0</span></div><div class="line">    stillOK = <span class="keyword">True</span></div><div class="line">    <span class="keyword">while</span> j&lt;<span class="number">26</span> <span class="keyword">and</span> stillOK:</div><div class="line">        <span class="keyword">if</span> c1[j]==c2[j]:</div><div class="line">            j = j + <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            stillOK = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> stillOK</div><div class="line"></div><div class="line">print(anagramSolution4(<span class="string">'apple'</span>,<span class="string">'pleap'</span>))</div></pre></td></tr></table></figure>
<p>同样，这个方案有多个迭代，但是和第一个解法不一样，它不是嵌套的。两个迭代都是 n, 第三个迭代，比较两个计数列表，需要 26 步，因为有 26 个字母。一共 T(n)=2n+26T(n)=2n+26，即 O(n)，我们找到了一个线性量级的算法解决这个问题。</p>
<p>如果让我自己来选择，我可能会选第二种，第二种最简单，也最好理解。但是最后的结论表明 <strong>解法4</strong> 才是最优解法，排序固然简单，但是但数量很大的时候，可能远不止我们想的那么简单。</p>
<p>在结束这个例子之前，我们来讨论下空间花费，虽然最后一个方案在线性时间执行，但它需要额外的存储来保存两个字符计数列表。换句话说，该算法牺牲了空间以获得时间。</p>
<p>很多情况下，你需要在空间和时间之间做出权衡。这种情况下，额外空间不重要，但是如果有数百万个字符，就需要关注下。作为一个计算机科学家，当给定一个特定的算法，将由你决定如何使用计算资源。</p>
<blockquote>
<p>如有错误，请指出</p>
</blockquote>
<p><a href="https://m.weibo.cn/status/4200069676754911" target="_blank" rel="external">图片来源</a></p>
<p>各位下期见，不聊了，又该搬砖了。。。</p>
<p><img src="https://wx2.sinaimg.cn/orj360/b04b21b7ly1fnssg6v2b1j20dc0dcq4t.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十五篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;老板，我要做算法工程师！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/z10Tn6k.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="算法" scheme="https://zhangslob.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://zhangslob.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="https://zhangslob.github.io/tags/Python/"/>
    
      <category term="排序" scheme="https://zhangslob.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>使用logging管理爬虫</title>
    <link href="https://zhangslob.github.io/2018/01/15/%E4%BD%BF%E7%94%A8logging%E7%AE%A1%E7%90%86%E7%88%AC%E8%99%AB/"/>
    <id>https://zhangslob.github.io/2018/01/15/使用logging管理爬虫/</id>
    <published>2018-01-15T13:39:35.000Z</published>
    <updated>2018-01-15T13:43:53.655Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十四篇原创文章
</code></pre><p>使用日志记录程序运行状态</p>
<p><img src="https://farm5.staticflickr.com/4246/35254379756_c9fe23f843_k_d.jpg" alt=""></p>
<a id="more"></a>
<blockquote>
<p><a href="http://pythonguidecn.readthedocs.io/zh/latest/writing/logging.html" target="_blank" rel="external">文档</a></p>
</blockquote>
<p><a href="https://docs.python.org/2/library/logging.html#module-logging" target="_blank" rel="external">日志</a> 模块自2.3版本开始便是Python标准库的一部分。它被简洁的描述在 <a href="https://www.python.org/dev/peps/pep-0282/" target="_blank" rel="external">PEP 282</a>。 众所周知，除了 <a href="https://docs.python.org/3/howto/logging.html#logging-basic-tutorial" target="_blank" rel="external">基础日志指南</a> 部分，该文档并不容易阅读。</p>
<p>日志的两个目的：</p>
<ul>
<li><strong>诊断日志</strong> 记录与应用程序操作相关的日志。例如，用户遇到的报错信息， 可通过搜索诊断日志获得上下文信息。</li>
<li><strong>审计日志</strong> 为商业分析而记录的日志。从审计日志中，可提取用户的交易信息， 并结合其他用户资料构成用户报告或者用来优化商业目标。</li>
</ul>
<h1 id="…-或者打印"><a href="#…-或者打印" class="headerlink" title="… 或者打印?"></a>… 或者打印?</h1><p>当需要在命令行应用中显示帮助文档时， <code>打印</code> 是一个相对于日志更好的选择。 而在其他时候，日志总能优于 <code>打印</code> ，理由如下：</p>
<ul>
<li>日志事件产生的 <a href="https://docs.python.org/3/library/logging.html#logrecord-attributes" target="_blank" rel="external">日志记录</a> ，包含清晰可用的诊断信息，如文件名称、路径、函数名和行号等。</li>
<li>包含日志模块的应用，默认可通过根记录器对应用的日志流进行访问，除非您将日志过滤了。</li>
<li>可通过 <code>logging.Logger.setLevel()</code> 方法有选择地记录日志， 或可通过设置 <code>logging.Logger.disabled</code> 属性为 <code>True</code> 来禁用。</li>
</ul>
<h1 id="库中的日志"><a href="#库中的日志" class="headerlink" title="库中的日志"></a>库中的日志</h1><p><a href="https://docs.python.org/3/howto/logging.html" target="_blank" rel="external">日志指南</a> 中含 <a href="https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library" target="_blank" rel="external">库日志配置</a> 的说明。由于是 <em>用户</em> ，而非库来指明如何响应日志事件， 因此这里有一个值得反复说明的忠告：</p>
<blockquote>
<p>注解<br>强烈建议不要向您的库日志中加入除NullHandler外的其它处理程序。</p>
</blockquote>
<p>在库中，声明日志的最佳方式是通过 <code>__name__</code> 全局变量： logging 模块通过点(dot)运算符创建层级排列的日志，因此，用 <code>__name__</code> 可以避免名字冲突。</p>
<p>以下是一个来自 <a href="https://github.com/requests/requests" target="_blank" rel="external">requests 资源</a> 的最佳实践的例子 —— 把它放置在您的 <code>__init__.py</code> 文件中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line">logging.getLogger(__name__).addHandler(logging.NullHandler())</div></pre></td></tr></table></figure></p>
<h1 id="应用程序中的日志"><a href="#应用程序中的日志" class="headerlink" title="应用程序中的日志"></a>应用程序中的日志</h1><p>应用程序开发的权威指南， <a href="https://12factor.net/" target="_blank" rel="external">应用的12要素</a> ，也在其中一节描述了 <a href="https://12factor.net/logs" target="_blank" rel="external">日志的作用</a> 。它特别强调将日志视为事件流， 并将其发送至由应用环境所处理的标准输出中。</p>
<p>配置日志至少有以下三种方式：</p>
<ul>
<li>使用INI格式文件：<ul>
<li>优点: 使用 logging.config.listen() 函数监听socket，可在运行过程中更新配置</li>
<li>缺点: 通过源码控制日志配置较少（ 例如 子类化定制的过滤器或记录器）。</li>
</ul>
</li>
<li>使用字典或JSON格式文件：<ul>
<li>优点: 除了可在运行时动态更新，在Python 2.6之后，还可通过 json 模块从其它文件中导入配置。</li>
<li>缺点: 很难通过源码控制日志配置。</li>
</ul>
</li>
<li>使用源码：<ul>
<li>优点: 对配置绝对的控制。</li>
<li>缺点: 对配置的更改需要对源码进行修改。</li>
</ul>
</li>
</ul>
<h1 id="通过INI文件进行配置的例子"><a href="#通过INI文件进行配置的例子" class="headerlink" title="通过INI文件进行配置的例子"></a>通过INI文件进行配置的例子</h1><p>我们假设文件名为 <code>logging_config.ini</code> 。关于文件格式的更多细节，请参见 <a href="https://docs.python.org/3/howto/logging.html" target="_blank" rel="external">日志指南</a> 中的 <a href="https://docs.python.org/3/howto/logging.html#configuring-logging" target="_blank" rel="external">日志配置</a> 部分。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">[loggers]</div><div class="line">keys=root</div><div class="line"></div><div class="line">[handlers]</div><div class="line">keys=stream_handler</div><div class="line"></div><div class="line">[formatters]</div><div class="line">keys=formatter</div><div class="line"></div><div class="line">[logger_root]</div><div class="line">level=DEBUG</div><div class="line">handlers=stream_handler</div><div class="line"></div><div class="line">[handler_stream_handler]</div><div class="line">class=StreamHandler</div><div class="line">level=DEBUG</div><div class="line">formatter=formatter</div><div class="line">args=(sys.stderr,)</div><div class="line"></div><div class="line">[formatter_formatter]</div><div class="line">format=%(asctime)s %(name)-12s %(levelname)-8s %(message)s</div></pre></td></tr></table></figure>
<p>然后在源码中调用 <code>logging.config.fileConfig()</code> 方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line"><span class="keyword">from</span> logging.config <span class="keyword">import</span> fileConfig</div><div class="line"></div><div class="line">fileConfig(<span class="string">'logging_config.ini'</span>)</div><div class="line">logger = logging.getLogger()</div><div class="line">logger.debug(<span class="string">'often makes a very good meal of %s'</span>, <span class="string">'visiting tourists'</span>)</div></pre></td></tr></table></figure></p>
<h1 id="通过字典进行配置的例子"><a href="#通过字典进行配置的例子" class="headerlink" title="通过字典进行配置的例子"></a>通过字典进行配置的例子</h1><p>Python 2.7中，您可以使用字典实现详细配置。<a href="https://www.python.org/dev/peps/pep-0391/" target="_blank" rel="external">PEP 391</a> 包含了一系列字典配置的强制和 非强制的元素。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line"><span class="keyword">from</span> logging.config <span class="keyword">import</span> dictConfig</div><div class="line"></div><div class="line">logging_config = dict(</div><div class="line">    version = <span class="number">1</span>,</div><div class="line">    formatters = &#123;</div><div class="line">        <span class="string">'f'</span>: &#123;<span class="string">'format'</span>:</div><div class="line">              <span class="string">'%(asctime)s %(name)-12s %(levelname)-8s %(message)s'</span>&#125;</div><div class="line">        &#125;,</div><div class="line">    handlers = &#123;</div><div class="line">        <span class="string">'h'</span>: &#123;<span class="string">'class'</span>: <span class="string">'logging.StreamHandler'</span>,</div><div class="line">              <span class="string">'formatter'</span>: <span class="string">'f'</span>,</div><div class="line">              <span class="string">'level'</span>: logging.DEBUG&#125;</div><div class="line">        &#125;,</div><div class="line">    root = &#123;</div><div class="line">        <span class="string">'handlers'</span>: [<span class="string">'h'</span>],</div><div class="line">        <span class="string">'level'</span>: logging.DEBUG,</div><div class="line">        &#125;,</div><div class="line">)</div><div class="line"></div><div class="line">dictConfig(logging_config)</div><div class="line"></div><div class="line">logger = logging.getLogger()</div><div class="line">logger.debug(<span class="string">'often makes a very good meal of %s'</span>, <span class="string">'visiting tourists'</span>)</div></pre></td></tr></table></figure></p>
<p>通过源码直接配置的例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line"></div><div class="line">logger = logging.getLogger()</div><div class="line">handler = logging.StreamHandler()</div><div class="line">formatter = logging.Formatter(</div><div class="line">        <span class="string">'%(asctime)s %(name)-12s %(levelname)-8s %(message)s'</span>)</div><div class="line">handler.setFormatter(formatter)</div><div class="line">logger.addHandler(handler)</div><div class="line">logger.setLevel(logging.DEBUG)</div><div class="line"></div><div class="line">logger.debug(<span class="string">'often makes a very good meal of %s'</span>, <span class="string">'visiting tourists'</span>)</div></pre></td></tr></table></figure>
<p>官方文档说完了，来看看具体的应用。</p>
<p>默认的日志级别设置为 <code>WARNING</code> （日志级别等级 <code>CRITICAL</code> &gt; <code>ERROR</code> &gt; <code>WARNING</code> &gt; <code>INFO</code> &gt; <code>DEBUG</code> &gt; <code>NOTSET</code>）, 小于 <code>WARNING</code> 级别的日志都不输出, 大于等于 <code>WARNING</code> 级别的日志都会输出。</p>
<h1 id="简单的将日志打印到屏幕"><a href="#简单的将日志打印到屏幕" class="headerlink" title="简单的将日志打印到屏幕"></a>简单的将日志打印到屏幕</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> logging</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    logging.basicConfig(level=logging.DEBUG,</div><div class="line">    format=<span class="string">'%(asctime)s %(filename)s [line:%(lineno)d] %(levelname)s %(message)s'</span>,</div><div class="line">            datefmt=<span class="string">'%a, %d %b %Y %H:%M:%S'</span>,</div><div class="line">            filename=<span class="string">'test.log'</span>,</div><div class="line">            filemode=<span class="string">'w'</span>)</div><div class="line">    console = logging.StreamHandler()</div><div class="line">    console.setLevel(logging.WARNING)</div><div class="line">    formatter=logging.Formatter(<span class="string">'%(name)-12s: %(levelname)-8s %(message)s'</span>)</div><div class="line">    console.setFormatter(formatter)</div><div class="line">    logging.getLogger(<span class="string">''</span>).addHandler(console)</div><div class="line">    logging.debug(<span class="string">'This is DEBUG'</span>)</div><div class="line">    logging.info(<span class="string">'This is INFO'</span>)</div><div class="line">    logging.warning(<span class="string">'This is WARNING'</span>)</div></pre></td></tr></table></figure>
<p>输出:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">root        : WARNING  This is WARNING</div></pre></td></tr></table></figure></p>
<p><code>test.log</code>文件中包含：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Mon, 15 Jan 2018 20:19:45 sfda.py [line:90] DEBUG This is DEBUG</div><div class="line">Mon, 15 Jan 2018 20:19:45 sfda.py [line:91] INFO This is INFO</div><div class="line">Mon, 15 Jan 2018 20:19:45 sfda.py [line:92] WARNING This is WARNING</div></pre></td></tr></table></figure></p>
<p>注意：由于日志写入模式设置为 <code>w</code> ，因此重复运行时会将之前的日志清空。</p>
<p>logging.basicConfig 函数各参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">filename: 指定日志文件名</div><div class="line">filemode: 和file函数意义相同，指定日志文件的打开模式，’w’或’a’</div><div class="line">format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示:</div><div class="line">%(levelno)s: 打印日志级别的数值</div><div class="line">%(levelname)s: 打印日志级别名称</div><div class="line">%(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0]</div><div class="line">%(filename)s: 打印当前执行程序名</div><div class="line">%(funcName)s: 打印日志的当前函数</div><div class="line">%(lineno)d: 打印日志的当前行号</div><div class="line">%(asctime)s: 打印日志的时间</div><div class="line">%(thread)d: 打印线程ID</div><div class="line">%(threadName)s: 打印线程名称</div><div class="line">%(process)d: 打印进程ID</div><div class="line">%(message)s: 打印日志信息</div><div class="line">datefmt: 指定时间格式，同time.strftime()</div><div class="line">level: 设置日志级别，默认为logging.WARNING</div><div class="line">stream: 指定将日志的输出流，可以指定输出到sys.stderr,</div><div class="line">sys.stdout或者文件，默认输出到sys.stderr，当stream和filename同时指定时，stream被忽略</div></pre></td></tr></table></figure>
<h1 id="logging-三大模块：-Logger-Handler-，-Format"><a href="#logging-三大模块：-Logger-Handler-，-Format" class="headerlink" title="logging 三大模块： Logger ,  Handler ， Format"></a>logging 三大模块： <code>Logger</code> ,  <code>Handler</code> ， <code>Format</code></h1><h2 id="logger"><a href="#logger" class="headerlink" title="logger"></a>logger</h2><p> <code>logger</code> 通过 <code>getLogger</code> 函数得到， 可以在不同的模块中使用不同的 <code>logger</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line">logger = logging.getLogger(__name__)</div><div class="line">logger.debug(<span class="string">'some infomation'</span>)</div></pre></td></tr></table></figure></p>
<h2 id="Handler"><a href="#Handler" class="headerlink" title="Handler"></a>Handler</h2><p> <code>handler</code> 有多种， 可以记录到 <code>console</code>， 或者到文件， 文件也可以自动 <code>rotate</code>， 常用的几个 <code>handler</code></p>
<ul>
<li>StreamHandler 打印到终端</li>
<li>FileHandler 保存到文件</li>
<li>RotatingFileHandler 保存到文件， 达到一定大小之后备份文件。</li>
<li>TimedRotatingFileHandler 定时备份</li>
</ul>
<h2 id="Format"><a href="#Format" class="headerlink" title="Format"></a>Format</h2><p> <code>Formatter</code> 对象设置日志信息最后的规则、结构和内容，默认的时间格式为 <code>%Y-%m-%d %H:%M:%S</code></p>
<h1 id="Scrapy-与-Logging"><a href="#Scrapy-与-Logging" class="headerlink" title="Scrapy 与 Logging"></a>Scrapy 与 Logging</h1><blockquote>
<p><a href="https://doc.scrapy.org/en/latest/topics/logging.html" target="_blank" rel="external">文档</a></p>
</blockquote>
<p>Scrapy uses Python’s builtin logging system for event logging. We’ll provide some simple examples to get you started, but for more advanced use-cases it’s strongly suggested to read thoroughly its documentation.</p>
<blockquote>
<p>Scrapy使用Python的内置日志记录系统进行事件日志记录。 我们将提供一些简单的示例来帮助您开始，但对于更高级的用例，强烈建议您仔细阅读其文档。</p>
</blockquote>
<h2 id="Log-levels"><a href="#Log-levels" class="headerlink" title="Log levels"></a>Log levels</h2><ol>
<li>logging.CRITICAL - for critical errors (highest severity)</li>
<li>logging.ERROR - for regular errors</li>
<li>logging.WARNING - for warning messages</li>
<li>logging.INFO - for informational messages</li>
<li>logging.DEBUG - for debugging messages (lowest severity)</li>
</ol>
<h2 id="How-to-log-messages"><a href="#How-to-log-messages" class="headerlink" title="How to log messages"></a>How to log messages</h2><p> quick example</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line">logger = logging.getLogger()</div><div class="line">logger.warning(<span class="string">"This is a warning"</span>)</div></pre></td></tr></table></figure>
<h2 id="Logging-from-Spiders"><a href="#Logging-from-Spiders" class="headerlink" title="Logging from Spiders"></a>Logging from Spiders</h2><p>该记录器是使用Spider的名称创建的，但是您可以使用任何您想要的自定义Python记录器。 例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line">logger = logging.getLogger(<span class="string">'mycustomlogger'</span>)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line"></div><div class="line">    name = <span class="string">'myspider'</span></div><div class="line">    start_urls = [<span class="string">'https://scrapinghub.com'</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        logger.info(<span class="string">'Parse function called on %s'</span>, response.url)</div></pre></td></tr></table></figure>
<h2 id="在middlewares中应用"><a href="#在middlewares中应用" class="headerlink" title="在middlewares中应用"></a>在middlewares中应用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> logging</div><div class="line"></div><div class="line">logger = logging.getLogger(__name__)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></div><div class="line">        request.meta[<span class="string">'proxy'</span>] = random.choice(proxy_list)</div><div class="line">        spider.logger.info(<span class="string">'get ip: &#123;&#125;'</span>.format(request.meta[<span class="string">'proxy'</span>]))</div></pre></td></tr></table></figure>
<h2 id="使用Scrapy记录爬取日志"><a href="#使用Scrapy记录爬取日志" class="headerlink" title="使用Scrapy记录爬取日志"></a>使用Scrapy记录爬取日志</h2><p>在 <code>settings.py</code> 中修改:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">LOG_STDOUT = <span class="keyword">True</span></div><div class="line">LOG_FILE = <span class="string">'scrapy_log.txt'</span></div></pre></td></tr></table></figure></p>
<p>然后开始运行爬虫，日志不会打印，会保存到 <code>scrapy_log.txt</code> 文件中。</p>
<h2 id="使用errbacks在请求处理中捕获异常"><a href="#使用errbacks在请求处理中捕获异常" class="headerlink" title="使用errbacks在请求处理中捕获异常"></a>使用errbacks在请求处理中捕获异常</h2><p>请求的errback是在处理异常时被调用的函数。</p>
<p>它接收Twisted Failure实例作为第一个参数，可用于跟踪连接建立超时，DNS错误等。</p>
<p>这里有一个爬虫日志记录所有的错误和捕捉一些特定的错误，例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.spidermiddlewares.httperror <span class="keyword">import</span> HttpError</div><div class="line"><span class="keyword">from</span> twisted.internet.error <span class="keyword">import</span> DNSLookupError</div><div class="line"><span class="keyword">from</span> twisted.internet.error <span class="keyword">import</span> TimeoutError, TCPTimedOutError</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ErrbackSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">"errback_example"</span></div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://www.httpbin.org/"</span>,              <span class="comment"># HTTP 200 expected</span></div><div class="line">        <span class="string">"http://www.httpbin.org/status/404"</span>,    <span class="comment"># Not found error</span></div><div class="line">        <span class="string">"http://www.httpbin.org/status/500"</span>,    <span class="comment"># server issue</span></div><div class="line">        <span class="string">"http://www.httpbin.org:12345/"</span>,        <span class="comment"># non-responding host, timeout expected</span></div><div class="line">        <span class="string">"http://www.httphttpbinbin.org/"</span>,       <span class="comment"># DNS error expected</span></div><div class="line">    ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> self.start_urls:</div><div class="line">            <span class="keyword">yield</span> scrapy.Request(u, callback=self.parse_httpbin,</div><div class="line">                                    errback=self.errback_httpbin,</div><div class="line">                                    dont_filter=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_httpbin</span><span class="params">(self, response)</span>:</span></div><div class="line">        self.logger.info(<span class="string">'Got successful response from &#123;&#125;'</span>.format(response.url))</div><div class="line">        <span class="comment"># do something useful here...</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">errback_httpbin</span><span class="params">(self, failure)</span>:</span></div><div class="line">        <span class="comment"># log all failures</span></div><div class="line">        self.logger.error(repr(failure))</div><div class="line"></div><div class="line">        <span class="comment"># in case you want to do something special for some errors,</span></div><div class="line">        <span class="comment"># you may need the failure's type:</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> failure.check(HttpError):</div><div class="line">            <span class="comment"># these exceptions come from HttpError spider middleware</span></div><div class="line">            <span class="comment"># you can get the non-200 response</span></div><div class="line">            response = failure.value.response</div><div class="line">            self.logger.error(<span class="string">'HttpError on %s'</span>, response.url)</div><div class="line"></div><div class="line">        <span class="keyword">elif</span> failure.check(DNSLookupError):</div><div class="line">            <span class="comment"># this is the original request</span></div><div class="line">            request = failure.request</div><div class="line">            self.logger.error(<span class="string">'DNSLookupError on %s'</span>, request.url)</div><div class="line"></div><div class="line">        <span class="keyword">elif</span> failure.check(TimeoutError, TCPTimedOutError):</div><div class="line">            request = failure.request</div><div class="line">            self.logger.error(<span class="string">'TimeoutError on %s'</span>, request.url)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十四篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用日志记录程序运行状态&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://farm5.staticflickr.com/4246/35254379756_c9fe23f843_k_d.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://zhangslob.github.io/categories/Python/"/>
    
    
      <category term="2017" scheme="https://zhangslob.github.io/tags/2017/"/>
    
      <category term="logging" scheme="https://zhangslob.github.io/tags/logging/"/>
    
  </entry>
  
  <entry>
    <title>2017 梦醒时分</title>
    <link href="https://zhangslob.github.io/2017/12/31/2017-%E6%A2%A6%E9%86%92%E6%97%B6%E5%88%86/"/>
    <id>https://zhangslob.github.io/2017/12/31/2017-梦醒时分/</id>
    <published>2017-12-31T08:09:41.000Z</published>
    <updated>2017-12-31T08:58:12.240Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十三篇原创文章
</code></pre><p><img src="https://i.imgur.com/hDfodeg.jpg" alt=""></p>
<a id="more"></a>
<p>首先说说自己吧，小歪、来自湖北、在武汉中南民族大学本科社会学专业，目前在北漂，职位是<strong>客户支持&amp;爬虫工程师</strong>。</p>
<p>先说说自己的大学吧，大学时基本是不知道自己想要什么，也没有思考过自己以后去做什么这个问题，可以说是玩了三年，到了大四，面临着毕业找工作的压力，出去找实习，可是自己啥也不会啊。就是在这个情况下，我接触到了<strong>Python</strong>。</p>
<p>翻看自己一年前写的博客 <a href="https://zhuanlan.zhihu.com/p/25147821" target="_blank" rel="external">Python练习第一题，在图片上加入数字</a>，我在里面曾经说过这样一句话：</p>
<blockquote>
<p>最后，我想给自己定个目标，2017年利用Python找到一份工作，养活自己。</p>
</blockquote>
<p><img src="https://i.imgur.com/RVU7Vrt.png" alt=""></p>
<p>在今天，也就是2017年12月31日，我可以自豪地说，<strong>我完成了我的目标</strong>。</p>
<p>这其中发生了很多有趣的事，我现在都说一说吧。</p>
<h1 id="第一个offer"><a href="#第一个offer" class="headerlink" title="第一个offer"></a>第一个offer</h1><p>临近毕业的时候当然也有跑宣讲会什么的，各种机缘巧合下，拿到了第一个offer，是食品行业的，好像是什么质检员，我感觉自己还是想去互联网行业的，所以拒绝了。</p>
<h1 id="第二个offer"><a href="#第二个offer" class="headerlink" title="第二个offer"></a>第二个offer</h1><p>第二个是某派新闻，号称党媒旗下的媒体。刚去的时候，做的是审核类的工作，最后才发现里面的各种坑。好险自己发现的早，时间已经来到17年的3月份。</p>
<h1 id="第三个offer"><a href="#第三个offer" class="headerlink" title="第三个offer"></a>第三个offer</h1><p><img src="https://www.hellobi.com/images/logo/logo.png" alt=""></p>
<p>到3月份，我还没找到工作，但是自己心灰意冷，很害怕刚毕业就失业，但是在武汉根本很难找到合适的工作。恰好在这个时候，认识了天善的勇哥，大家可以去看看天善的课程，都是一线大牛人物，<a href="https://www.hellobi.com/" target="_blank" rel="external">天善智能</a>。然后我就去上海了，去上海的一个月时间，我自己改变是非常大的。认识了很多大佬，见识了很多新的想法。</p>
<p>在天善做的是新媒体这一块，但是就有写这样一篇文章，叫做<a href="https://zhuanlan.zhihu.com/p/26409764" target="_blank" rel="external">为了找一份Python实习，我用爬虫收集数据</a>，很巧的事情是，现在我的BOSS真好看到了这篇文章，然后就联系我了，然后我就来了。</p>
<h1 id="第四个offer"><a href="#第四个offer" class="headerlink" title="第四个offer"></a>第四个offer</h1><p><img src="http://zaoshu.io/asserts/logo.png" alt=""></p>
<p>我是6月份来到北京的，到现在已经有半年了，一直呆在<a href="http://zaoshu.io/" target="_blank" rel="external">造数</a>。</p>
<p>做过运营、做过客户支持，最近开始接触项目，可以使用<strong>Python</strong>开发项目，虽然有很多做得不好的地方，但是对我来说这是一个开始。</p>
<p>我每天会花很多时间去学习（周末除外，因为我要吃鸡，haha），也会经常写博客，记录自己的学习经历，接触项目之后才发现自己以前的各种毛病，这些都不是问题，坚持学习就好。</p>
<p>这期间，我还配合造数的首席爬虫工程师<strong>小X</strong>制作了一期<a href="https://www.dcxueyuan.com/#/classDetail/classIntroduce/17" target="_blank" rel="external">Python爬虫</a>课程，课程还是挺精彩的，我学习进阶部分后，感觉对Scrapy框架入门了。</p>
<p>有兴趣的可以看看，但是我并不推荐，因为学Python或者爬虫最好的还是去看文档，文档是最好的选择。这种培训课程只有一个好处，就是及时的帮你解答疑惑。</p>
<p>好了，不扯远了，继续回到今天的主题。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>2017年对我来说就是“<strong>梦醒时分</strong>”，总算是在这一年完成了自己的梦想，接下来就需要自己更加努力，去弥补自己所缺乏的。</p>
<p>对于我这个文科生来说，确实是挺不容易的，但是就像歌词里的：</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27901832&auto=0&height=66"></iframe>

<blockquote>
<p>新的风暴已经出现<br>怎么能够停滞不前<br>穿越时空竭尽全力<br>我会来到你身边<br>微笑面对危险<br>梦想成真不会遥远<br>鼓起勇气坚定向前<br>奇迹一定会出现</p>
</blockquote>
<p>非常欢迎大家与我交流，有什么疑问可以直接在评论中说出来，我定会知无不言。</p>
<p>也可以加我微信交流： zhang7350</p>
<p>最后，如果你也想要学Python找工作，你应该先问自己一个问题： 我喜欢Python吗？</p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十三篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/hDfodeg.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="总结" scheme="https://zhangslob.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="年度总结" scheme="https://zhangslob.github.io/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
      <category term="2017" scheme="https://zhangslob.github.io/tags/2017/"/>
    
  </entry>
  
  <entry>
    <title>采集方案策略之App抓包</title>
    <link href="https://zhangslob.github.io/2017/12/23/%E9%87%87%E9%9B%86%E6%96%B9%E6%A1%88%E7%AD%96%E7%95%A5%E4%B9%8BApp%E6%8A%93%E5%8C%85/"/>
    <id>https://zhangslob.github.io/2017/12/23/采集方案策略之App抓包/</id>
    <published>2017-12-23T13:40:26.000Z</published>
    <updated>2017-12-23T13:48:09.728Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十二篇原创文章
</code></pre><p><img src="https://i.imgur.com/NRBPj4t.jpg" alt=""></p>
<a id="more"></a>
<h1 id="采集方案策略设计"><a href="#采集方案策略设计" class="headerlink" title="采集方案策略设计"></a>采集方案策略设计</h1><p>在群里看到有人询问饿了么的参数，正好感兴趣，就来时间一番。</p>
<p>这里引用下大佬的一段话：</p>
<p>首先大的地方，我们想抓取某个数据源，我们要知道大概有哪些路径可以获取到数据源，基本上无外乎三种：</p>
<ul>
<li>PC端网站</li>
<li>针对移动设备响应式设计的网站（也就是很多人说的H5, 虽然不一定是H5）；</li>
<li>移动App</li>
</ul>
<p>原则是能抓移动App的，最好抓移动App，如果有针对移动设备优化的网站，就抓针对移动设备优化的网站，最后考虑PC网站。因为移动App基本都是API很简单，而移动设备访问优化的网站一般来讲都是结构简单清晰的HTML，而PC网站自然是最复杂的了；<br>针对PC端网站和移动网站的做法一样，分析思路可以一起讲，移动App单独分析。</p>
<p>其实很多网页都有移动端，像微博，我知道这三个：</p>
<ol>
<li>weibo.com</li>
<li>weibo.cn</li>
<li>m.weibo.cn</li>
</ol>
<p>最简单的当然是第二种了，对于今天的受害者——饿了么来说，当然，首选也是移动端。</p>
<h1 id="饿了么抓包分析"><a href="#饿了么抓包分析" class="headerlink" title="饿了么抓包分析"></a>饿了么抓包分析</h1><p>这里抓包工具选择<strong>Fiddler</strong>，这里不讲如何配置，具体参考 <a href="https://www.cnblogs.com/abao0/p/7008995.html" target="_blank" rel="external">用Fiddler对Android应用进行抓包</a></p>
<p>下面打开手机的饿了么，原本以为会有数据，结果，竟然是这样</p>
<p><img src="https://i.imgur.com/KmL5L9d.png" alt=""></p>
<p>去询问了专业人士，了解了有些应用不允许用户抓包，会有相应的限制。好吧，这就能难倒我了吗？？</p>
<p>当我切换到<strong>发现</strong>类目下，发现有奇怪的提示</p>
<p><img src="https://i.imgur.com/zF1e1iV.png" alt=""></p>
<p>在疯狂点击<strong>继续访问</strong>后，我终于可以正常访问了。</p>
<p><img src="https://i.imgur.com/DwDXNM8.jpg" alt=""></p>
<p>那么就可以在Fiddler中查看对应的数据了。这里直接把接口展示出来：<a href="https://restapi.ele.me/shopping/v1/find/recommendation?offset=20&amp;limit=40&amp;latitude=39.93245&amp;longitude=116.50097" target="_blank" rel="external">饿了么接口</a></p>
<p>浏览器直接打开，貌似没有啥验证</p>
<p><img src="https://i.imgur.com/S2IcQvP.png" alt=""></p>
<p>具体分析里面的参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">offset:20</div><div class="line">limit:40</div><div class="line">latitude:39.93245</div><div class="line">longitude:116.50097</div></pre></td></tr></table></figure>
<p>有4个参数， <code>offset</code> 和 <code>limit</code> 就很常见了，翻页和每页的数据，至于 <code>latitude</code> 和 <code>longitude</code> 仔细观察就知道，经纬度嘛，把它该修改为你想采集的位置的经纬度就好。</p>
<p>抓包分析之后，接下来采集数据就很简单了，数据字段标识：</p>
<p><code>food_id</code> 是商品ID，接口是：<code>https://www.ele.me/restapi/shopping/v1/foods?food_ids%5B%5D=712859937</code></p>
<p>打开此链接就是商品详情内容</p>
<p><img src="https://i.imgur.com/n5zMOEl.png" alt=""></p>
<p><img src="https://i.imgur.com/YzUyohN.jpg" alt=""></p>
<blockquote>
<p>可爱的小笼包</p>
</blockquote>
<p><code>restaurant_id</code> 是店铺ID，接口 <code>https://www.ele.me/shop/157458556</code></p>
<p>打开就是店铺详情页，当然，也有移动端：<code>https://h5.ele.me/shop/#id=157458556</code></p>
<p>这样进行商品采集就比较轻松了。</p>
<p>好饿，容我先点个外卖。</p>
<hr>
<h1 id="微信公众号抓包分析"><a href="#微信公众号抓包分析" class="headerlink" title="微信公众号抓包分析"></a>微信公众号抓包分析</h1><p>既然都看了饿了么，那也来看看微信吧。</p>
<p>使用Fiddler抓出来的curl命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -k -i --raw -o 0.dat &quot;https://mp.weixin.qq.com/mp/profile_ext?action=getmsg&amp;__biz=MzA4NzA1OTc5Nw==&amp;f=json&amp;offset=20&amp;count=10&amp;appmsg_token=936_iWFH%%252F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~&quot; -H &quot;User-Agent: Mozilla/5.0 (Linux; Android 7.1.1; MI 6 Build/NMF26X; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.49 Mobile MQQBrowser/6.2 TBS/043632 Safari/537.36 MicroMessenger/6.5.23.1180 NetType/WIFI Language/zh_CN&quot; -H &quot;Accept-Encoding: gzip, deflate&quot; -H &quot;Accept: */*&quot; -H &quot;Connection: keep-alive&quot; -H &quot;Host: mp.weixin.qq.com&quot; -H &quot;X-Requested-With: XMLHttpRequest&quot; -H &quot;Referer: https://mp.weixin.qq.com/mp/profile_ext?action=home&amp;__biz=MzA4NzA1OTc5Nw==&amp;scene=124&amp;devicetype=android-25&amp;version=26051732&amp;lang=zh_CN&amp;nettype=WIFI&amp;a8scene=3&amp;pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8%%2BLUbHoWGGBEp&amp;wx_header=1&quot; -H &quot;Accept-Language: zh-CN,en-US;q=0.8&quot; -H &quot;Cookie: pgv_pvi=4831552512; pgv_si=s989715456; sd_userid=18991505459750403; sd_cookie_crttime=1505459750403; tvfe_boss_uuid=a8e4e4f1ab6cd93d; pgv_info=ssid=s8735681072; pgv_pvid=4201362299; rewardsn=8d8b49dfb1811092eefe; wxtokenkey=19643e9f2ee569a10857d365bba88556d220fd33c1a0666b5d028a72b5bcd901; wxuin=838107840; devicetype=android-25; version=26051732; lang=zh_CN; pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8+LUbHoWGGBEp; wap_sid2=CMCF0o8DElxUVDVJR3o1ZldpbDlHWWdjQ0xMU3lxM3BWTUozTFFuZFhrUEJaanhoSmZ1aEVncnU0VzFIaWR3QkVVVXFuTUlMTlkxNFZjTnRCMEt1VHJjV3UzQVNOYWdEQUFBfjD6rvjRBTgMQJRO&quot; -H &quot;Q-UA2: QV=3&amp;PL=ADR&amp;PR=WX&amp;PP=com.tencent.mm&amp;PPVN=6.5.23&amp;TBSVC=43602&amp;CO=BK&amp;COVC=043632&amp;PB=GE&amp;VE=GA&amp;DE=PHONE&amp;CHID=0&amp;LCID=9422&amp;MO= MI6 &amp;RL=1080*1920&amp;OS=7.1.1&amp;API=25&quot; -H &quot;Q-GUID: 569ade09b5931656e4f49098113e88cb&quot; -H &quot;Q-Auth: 31045b957cf33acf31e40be2f3e71c5217597676a9729f1b&quot; -H &quot;Content-Type: application/json; charset=UTF-8&quot; -H &quot;Cache-Control: no-cache, must-revalidate&quot; -H &quot;RetKey: 14&quot; -H &quot;LogicRet: 0&quot;</div></pre></td></tr></table></figure>
<p>直接在浏览器中打开，会提示错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">ret: -3,</div><div class="line">errmsg: &quot;no session&quot;,</div><div class="line">cookie_count: 0</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>使用postman分析，最后Python的代码是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">url = <span class="string">"https://mp.weixin.qq.com/mp/profile_ext"</span></div><div class="line"></div><div class="line">querystring = &#123;<span class="string">"action"</span>:<span class="string">"getmsg"</span>,<span class="string">"__biz"</span>:<span class="string">"MzA4NzA1OTc5Nw=="</span>,<span class="string">"f"</span>:<span class="string">"json"</span>,<span class="string">"offset"</span>:<span class="string">"20"</span>,<span class="string">"count"</span>:<span class="string">"10"</span>,<span class="string">"appmsg_token"</span>:<span class="string">"936_iWFH%2F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~"</span>&#125;</div><div class="line"></div><div class="line">headers = &#123;</div><div class="line">    <span class="string">'user-agent'</span>: <span class="string">"Mozilla/5.0 (Linux; Android 7.1.1; MI 6 Build/NMF26X; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.49 Mobile MQQBrowser/6.2 TBS/043632 Safari/537.36 MicroMessenger/6.5.23.1180 NetType/WIFI Language/zh_CN"</span>,</div><div class="line">    <span class="string">'accept-encoding'</span>: <span class="string">"gzip, deflate"</span>,</div><div class="line">    <span class="string">'accept'</span>: <span class="string">"*/*"</span>,</div><div class="line">    <span class="string">'connection'</span>: <span class="string">"keep-alive"</span>,</div><div class="line">    <span class="string">'host'</span>: <span class="string">"mp.weixin.qq.com"</span>,</div><div class="line">    <span class="string">'x-requested-with'</span>: <span class="string">"XMLHttpRequest"</span>,</div><div class="line">    <span class="string">'referer'</span>: <span class="string">"https://mp.weixin.qq.com/mp/profile_ext?action=home&amp;__biz=MzA4NzA1OTc5Nw==&amp;scene=124&amp;devicetype=android-25&amp;version=26051732&amp;lang=zh_CN&amp;nettype=WIFI&amp;a8scene=3&amp;pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8%%2BLUbHoWGGBEp&amp;wx_header=1"</span>,</div><div class="line">    <span class="string">'accept-language'</span>: <span class="string">"zh-CN,en-US;q=0.8"</span>,</div><div class="line">    <span class="string">'cookie'</span>: <span class="string">"pgv_pvi=4831552512; pgv_si=s989715456; sd_userid=18991505459750403; sd_cookie_crttime=1505459750403; tvfe_boss_uuid=a8e4e4f1ab6cd93d; pgv_info=ssid=s8735681072; pgv_pvid=4201362299; rewardsn=8d8b49dfb1811092eefe; wxtokenkey=19643e9f2ee569a10857d365bba88556d220fd33c1a0666b5d028a72b5bcd901; wxuin=838107840; devicetype=android-25; version=26051732; lang=zh_CN; pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8+LUbHoWGGBEp; wap_sid2=CMCF0o8DElxUVDVJR3o1ZldpbDlHWWdjQ0xMU3lxM3BWTUozTFFuZFhrUEJaanhoSmZ1aEVncnU0VzFIaWR3QkVVVXFuTUlMTlkxNFZjTnRCMEt1VHJjV3UzQVNOYWdEQUFBfjD6rvjRBTgMQJRO"</span>,</div><div class="line">    <span class="string">'q-ua2'</span>: <span class="string">"QV=3&amp;PL=ADR&amp;PR=WX&amp;PP=com.tencent.mm&amp;PPVN=6.5.23&amp;TBSVC=43602&amp;CO=BK&amp;COVC=043632&amp;PB=GE&amp;VE=GA&amp;DE=PHONE&amp;CHID=0&amp;LCID=9422&amp;MO= MI6 &amp;RL=1080*1920&amp;OS=7.1.1&amp;API=25"</span>,</div><div class="line">    <span class="string">'q-guid'</span>: <span class="string">"569ade09b5931656e4f49098113e88cb"</span>,</div><div class="line">    <span class="string">'q-auth'</span>: <span class="string">"31045b957cf33acf31e40be2f3e71c5217597676a9729f1b"</span>,</div><div class="line">    <span class="string">'content-type'</span>: <span class="string">"application/json; charset=UTF-8"</span>,</div><div class="line">    <span class="string">'cache-control'</span>: <span class="string">"no-cache"</span>,</div><div class="line">    <span class="string">'retkey'</span>: <span class="string">"14"</span>,</div><div class="line">    <span class="string">'logicret'</span>: <span class="string">"0"</span>,</div><div class="line">    &#125;</div><div class="line"></div><div class="line">response = requests.request(<span class="string">"GET"</span>, url, headers=headers, params=querystring, verify=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">print(response.json())</div></pre></td></tr></table></figure>
<p>这个时候的参数有</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">action:getmsg</div><div class="line">__biz:MzA4NzA1OTc5Nw==</div><div class="line">f:json</div><div class="line">offset:20</div><div class="line">count:10</div><div class="line">appmsg_token:936_iWFH%%252F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~</div></pre></td></tr></table></figure>
<p>目前还不清楚这些参数的作用，再抓一个试试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">action:getmsg</div><div class="line">__biz:MjM5NzI3NDg4MA==</div><div class="line">f:json</div><div class="line">offset:10</div><div class="line">count:10</div><div class="line">appmsg_token:936_kFNdYU3DJ%%252B%%252BVfHfEGImXqB5DMbIeqtSR75ZFZQ~~</div></pre></td></tr></table></figure>
<p>估计就是 <code>__biz</code> 和 <code>appmsg_token</code> 这两个参数对应不同的公众号</p>
<p>对了，上面的代码会出现一个问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">InsecureRequestWarning: Unverified HTTPS request <span class="keyword">is</span> being made. Adding certificate verification <span class="keyword">is</span> strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html<span class="comment">#ssl-warnings</span></div><div class="line">  InsecureRequestWarning)</div></pre></td></tr></table></figure>
<p>解决方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</div><div class="line">urllib3.disable_warnings()</div></pre></td></tr></table></figure>
<h1 id="App的分析"><a href="#App的分析" class="headerlink" title="App的分析"></a>App的分析</h1><p>App类使用的工具是Fidder，手机和电脑在一个局域网内，先用Fidder配置好端口，然后手机设置代理，ip为电脑的ip，端口为设置的端口，然后如果手机上请求网络内容时，Fidder会显示相应地请求，那么就ok了，分析的大体逻辑基本一致，限制会相对少很多，但是也有几种情况需要注意：</p>
<ul>
<li>加密，App有时候也有一些加密的字段，这个时候,一般来讲都会进行反编译进行分析，找到对应的代码片段，逆推出加密方法；</li>
<li>gzip压缩或者base64编码，base64编码的辨别度较高，有时候数据被gzip压缩了，不过Charles都是有自动解密的；</li>
<li>https证书，有的https请求会验证证书, Fidder提供了证书，可以在官网找到，手机访问，然后信任添加就可以。</li>
</ul>
<p>最后，<strong>祝大家圣诞节快乐</strong></p>
<p><img src="https://i.imgur.com/zQTLLCy.gif" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十二篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/NRBPj4t.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="https://zhangslob.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="抓包" scheme="https://zhangslob.github.io/tags/%E6%8A%93%E5%8C%85/"/>
    
  </entry>
  
  <entry>
    <title>【RNG vs SKT】弹幕的自然语言的初步分析</title>
    <link href="https://zhangslob.github.io/2017/12/20/%E3%80%90RNG-vs-SKT%E3%80%91%E5%BC%B9%E5%B9%95%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%9A%84%E5%88%9D%E6%AD%A5%E5%88%86%E6%9E%90/"/>
    <id>https://zhangslob.github.io/2017/12/20/【RNG-vs-SKT】弹幕的自然语言的初步分析/</id>
    <published>2017-12-20T14:17:35.000Z</published>
    <updated>2017-12-20T14:40:46.728Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十一篇原创文章
</code></pre><p><img src="https://i.imgur.com/xf7kEI8.jpg" alt=""></p>
<a id="more"></a>
<p>前排 @皇族电子竞技俱乐部</p>
<p>==================================</p>
<p>S7中RNG对阵SKT，想必是全世界LOL玩家关注的重点。在比赛开始前，使用小葫芦把斗鱼S7直播间的弹幕都抓下来，想着做一小点分析，看看会得出什么结论。</p>
<p>因为数据量和分析深度等原因，以下内容仅供娱乐观赏</p>
<h1 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h1><p>使用小葫芦采集2万多条弹幕数据，RNG对阵SKT斗鱼直播间的弹幕，最后得到约4万可用数据</p>
<h1 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h1><p>使用jieba分词，算法如下</p>
<ul>
<li>基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)</li>
<li>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合</li>
<li>对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法</li>
</ul>
<p>简单处理之后，看起来是这样</p>
<p><img src="https://i.imgur.com/Zm6mMti.jpg" alt=""></p>
<p>顺便做个统计，看看都在说什么。不加停用词是这样的，全是 “<strong>666</strong>”</p>
<p><img src="https://i.imgur.com/df59MQw.jpg" alt=""></p>
<p>RNG和牛逼是出现最多的词语，同时也发现“<strong>卢本伟牛逼</strong>”</p>
<p><img src="https://i.imgur.com/6PcshNl.jpg" alt=""></p>
<blockquote>
<p>弱弱问一句“唐梦琼”是谁</p>
</blockquote>
<p>下面是词云，Python的词云做不来不好看，所以我使用的工具 <a href="https://timdream.org/wordcloud/#" target="_blank" rel="external">HTML5 Word Cloud</a></p>
<p><img src="https://i.imgur.com/R6ltujq.jpg" alt=""></p>
<blockquote>
<p>弹幕内容词云</p>
</blockquote>
<p><img src="https://i.imgur.com/PMISApU.jpg" alt=""></p>
<blockquote>
<p>用户昵称词云</p>
</blockquote>
<h1 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h1><p>这里使用的是 <a href="https://github.com/isnowfy/snownlp" target="_blank" rel="external">isnowfy/snownlp</a>。SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了<a href="https://github.com/sloria/TextBlob" target="_blank" rel="external">TextBlob</a>的启发而写的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> snownlp <span class="keyword">import</span> SnowNLP</div><div class="line"></div><div class="line">s = SnowNLP(<span class="string">u'这个东西真心很赞'</span>)</div><div class="line">s.sentiments    <span class="comment"># 0.9769663402895832 positive的概率</span></div></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/iEuDbFT.jpg" alt=""></p>
<p>有点难看，直接看数据吧，得到的结果是在 [0，1] 之间的<strong>positive的概率</strong></p>
<p>51659中有44705个大于0.5，占比86.54%，有 6954条弹幕低于0.5，占比13.46%。</p>
<p>弹幕中积极的概率还是相当高的，说明观众还是比较赞赏比赛的。</p>
<p><img src="https://i.imgur.com/FAOkrny.jpg" alt=""></p>
<h1 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h1><p>TF-IDF是信息检索领域非常重要的搜索词重要性度量；tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。</p>
<p>词频TF(Term Frequency)</p>
<p>词w在文档d中出现次数count(w, d)和文档d中总词数size(d)的比值：</p>
<p><img src="https://i.imgur.com/OttArJD.jpg" alt=""></p>
<p>基于 TF-IDF 算法的关键词抽取：</p>
<ul>
<li>RNG</li>
<li>rng</li>
<li>贺电</li>
<li>加油</li>
<li>发来</li>
<li>666</li>
<li>6666</li>
<li>66666</li>
<li>恭喜</li>
<li>66666666666666</li>
<li>666666</li>
<li>66666666666</li>
<li>6666666</li>
<li>skt</li>
<li>李哥</li>
<li>66666666666666666666</li>
<li>SKT</li>
<li>66666666</li>
<li>666666666</li>
<li>马甲</li>
</ul>
<p>在没有加上停用词的前提下，可以看到效果并不理想</p>
<p>TextRank是在Google的PageRank算法启发下，针对文本里的句子设计的权重算法，目标是自动摘要。它利用投票的原理，让每一个单词给它的邻居（术语称窗口）投赞成票，票的权重取决于自己的票数。</p>
<p><img src="https://i.imgur.com/AI6dJ5P.jpg" alt=""></p>
<p>基于 TextRank 算法的关键词抽取：</p>
<ul>
<li>发来</li>
<li>贺电</li>
<li>加油</li>
<li>学院</li>
<li>机器人</li>
<li>大学</li>
<li>职业</li>
<li>船长</li>
<li>技术</li>
<li>小炮</li>
<li>没有</li>
<li>解说</li>
<li>中国</li>
<li>经济</li>
<li>开始</li>
<li>无敌</li>
<li>香锅</li>
<li>垃圾</li>
<li>老鼠</li>
<li>科技</li>
</ul>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>Word2vec是一个将单词表征成向量的形式，它可以把文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度。</p>
<h2 id="计算相似度："><a href="#计算相似度：" class="headerlink" title="计算相似度："></a>计算相似度：</h2><p>首先是RNG：</p>
<ul>
<li>rng 0.9893965721130371</li>
<li>加油 0.9829007983207703</li>
<li>必胜 0.9790929555892944</li>
<li>Rng 0.9743078947067261</li>
<li>恭喜 0.9733642339706421</li>
<li>中国队 0.9397183656692505</li>
<li>关 0.9283092021942139</li>
<li>&lt; 0.9278833866119385</li>
<li>燕尾港 0.9222617149353027</li>
<li>B 0.9143030643463135</li>
</ul>
<p><strong>RNG加油</strong></p>
<p>看看SKT：</p>
<ul>
<li>skt 0.9911665320396423</li>
<li>皇族 0.972029983997345</li>
<li>＾ 0.970970094203949</li>
<li>稳住 0.9653508067131042</li>
<li>干死 0.9643667340278625</li>
<li>牛比 0.9629441499710083</li>
<li>一起 0.9625348448753357</li>
<li>@ 0.9619969725608826</li>
<li>鸟巢 0.9608791470527649</li>
<li>冠军 0.9608250856399536</li>
</ul>
<p><strong>稳住，干死SKT？</strong></p>
<p>贺电：</p>
<ul>
<li>学院 0.9811943769454956</li>
<li>大学 0.980491042137146</li>
<li>技术 0.9766334295272827</li>
<li>职业 0.9691535234451294</li>
<li>电子科技 0.9668481349945068</li>
<li>发来 0.9619022607803345</li>
<li>科技 0.9594647884368896</li>
<li>山东 0.9568137526512146</li>
<li>重庆 0.9440888166427612</li>
<li>哈尔滨 0.939836859703064</li>
</ul>
<p><strong>山东XX学院发来贺电</strong></p>
<p>加油：</p>
<ul>
<li>必胜 0.9876022934913635</li>
<li>RNG 0.9829007983207703</li>
<li>rng 0.960281252861023</li>
<li>Rng 0.9591789841651917</li>
<li>恭喜 0.9551880359649658</li>
<li>中国队 0.9436988830566406</li>
<li>关 0.94183349609375</li>
<li>！ 0.921385645866394</li>
<li>~ 0.9148629903793335</li>
<li>@ 0.9062787294387817</li>
</ul>
<p><strong>RNG加油，RNG必胜</strong></p>
<p>小狗：</p>
<ul>
<li>吹 0.9970275163650513</li>
<li>无敌 0.996719241142273</li>
<li>神超 0.996111273765564</li>
<li>后期 0.9959050416946411</li>
<li>厉害 0.9957337975502014</li>
<li>凶 0.9957261681556702</li>
<li>强 0.9955072402954102</li>
<li>一个 0.9954395890235901</li>
<li>干 0.99541175365448</li>
<li>起来 0.9952359199523926</li>
</ul>
<p><strong>狗吹？</strong></p>
<p>李哥：</p>
<ul>
<li>还是 0.9825356602668762</li>
<li>电话 0.9700809717178345</li>
<li>承 0.9697628617286682</li>
<li>心脏 0.9686012864112854</li>
<li>陈文泽在 0.9681863188743591</li>
<li>麻痹 0.9680625200271606</li>
<li>响 0.9674116373062134</li>
<li>以为 0.9664229154586792</li>
<li>狗哥 0.9592204689979553</li>
<li>不 0.9589840769767761</li>
</ul>
<p><strong>你李哥还是你李哥</strong></p>
<p>MLXG：</p>
<ul>
<li>宣告 0.9958090782165527</li>
<li>mlxg 0.9953181147575378</li>
<li>死亡 0.995277464389801</li>
<li>b 0.9949076771736145</li>
<li>6666 0.9947425723075867</li>
<li>丑 0.9943945407867432</li>
<li>10 0.9943088293075562</li>
<li>辣鸡 0.9940722584724426</li>
<li>干死 0.9940391778945923</li>
<li>锤 0.9939616918563843</li>
</ul>
<p><strong>香锅和死亡宣告有啥关系</strong></p>
<p>小虎：</p>
<ul>
<li>笑笑 0.9971799850463867</li>
<li>看到 0.9967395663261414</li>
<li>解说 0.9961692690849304</li>
<li>不是 0.9959656000137329</li>
<li>中单 0.9951503872871399</li>
<li>假 0.9950063824653625</li>
<li>为什么 0.9944812655448914</li>
<li>又 0.9942663908004761</li>
<li>么 0.9938984513282776</li>
<li>里奥 0.9937981367111206</li>
</ul>
<p><strong>小虎与加里奥（：</strong></p>
<p>letme：</p>
<ul>
<li>难受 0.9964221715927124</li>
<li>笑话 0.9959778785705566</li>
<li>哦 0.9958946108818054</li>
<li>世界 0.9958213567733765</li>
<li>毒奶 0.9957934021949768</li>
<li>KPL 0.9957884550094604</li>
<li>上单 0.9956253170967102</li>
<li>瓜皮 0.9955945014953613</li>
<li>快 0.9953423738479614</li>
<li>打团 0.9953156113624573</li>
</ul>
<p><strong>真难受啊</strong></p>
<h1 id="To-Do"><a href="#To-Do" class="headerlink" title="To Do"></a>To Do</h1><ol>
<li>可以使用朴素贝叶斯做分类模型</li>
<li>使用机器学习性能评估指标预测精确率和准确率</li>
<li>欢迎补充</li>
</ol>
<h1 id="可参考资料"><a href="#可参考资料" class="headerlink" title="可参考资料"></a>可参考资料</h1><ol>
<li><a href="http://dsqiu.iteye.com/blog/1704960" target="_blank" rel="external">中文分词基本算法介绍</a></li>
<li><a href="https://my.oschina.net/letiantian/blog/352693" target="_blank" rel="external">ICTCLAS 汉语词性标注集</a></li>
<li><a href="http://www.blogjava.net/zhenandaci/category/31868.html" target="_blank" rel="external">文本分类技术</a></li>
<li><a href="http://blog.csdn.net/zhzhl202/article/details/8197109" target="_blank" rel="external">文本分类与SVM</a></li>
<li><a href="http://blog.csdn.net/tbkken/article/details/8062358" target="_blank" rel="external">基于贝叶斯算法的文本分类算法</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA3MDg0MjgxNQ==&amp;mid=2652389734&amp;idx=1&amp;sn=585d22c0b2aea755a072f5dfefca868b&amp;scene=23&amp;srcid=0530UUkS6jjRAsVoT2icemoY#rd" target="_blank" rel="external">基于libsvm的中文文本分类原型</a></li>
<li><a href="https://cosx.org/2013/03/lda-math-text-modeling#comments" target="_blank" rel="external">LDA-math-文本建模</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_8af1069601019flb.html" target="_blank" rel="external">情感分析资源</a></li>
<li><a href="https://wenku.baidu.com/view/37e374355727a5e9856a61bc.html" target="_blank" rel="external">面向情感分析的特征抽取技术研究</a></li>
<li><a href="http://52opencourse.com/235/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%AC%E4%B8%83%E8%AF%BE-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%EF%BC%88sentiment-analysis%EF%BC%89" target="_blank" rel="external">斯坦福大学自然语言处理第七课-情感分析</a></li>
<li><a href="http://blog.jobbole.com/77709/0" target="_blank" rel="external">深度学习、自然语言处理和表征方法</a></li>
<li><a href="http://licstar.net/archives/328" target="_blank" rel="external">Deep Learning in NLP （一）词向量和语言模型</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十一篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/xf7kEI8.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://zhangslob.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://zhangslob.github.io/tags/NLP/"/>
    
      <category term="自然语言处理" scheme="https://zhangslob.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>scrapy学习实例（四）采集淘宝数据并展示</title>
    <link href="https://zhangslob.github.io/2017/12/19/scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E5%9B%9B%EF%BC%89%E9%87%87%E9%9B%86%E6%B7%98%E5%AE%9D%E6%95%B0%E6%8D%AE%E5%B9%B6%E5%B1%95%E7%A4%BA/"/>
    <id>https://zhangslob.github.io/2017/12/19/scrapy学习实例（四）采集淘宝数据并展示/</id>
    <published>2017-12-19T13:09:07.000Z</published>
    <updated>2017-12-19T14:36:19.258Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第二十篇原创文章
</code></pre><p><img src="https://i.imgur.com/IzOzuZz.png" alt=""></p>
<a id="more"></a>
<p>本节代码 ： <a href="https://github.com/zhangslob/Taobao_duoshou" target="_blank" rel="external">zhangslob/Taobao_duoshou</a> <strong>万水千山总是情，给个star行不行</strong></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=28830411&auto=1&height=66"></iframe>

<p>铛铛铛，懒惰了一段时间，咱接着学Scrapy。这一期玩点新花样，使用<a href="http://flask.pocoo.org/" target="_blank" rel="external">Flask</a>展示我们的数据。效果如下图：</p>
<p><img src="https://i.imgur.com/IzOzuZz.png" alt=""></p>
<p><img src="https://i.imgur.com/aDqbnMU.png" alt=""></p>
<blockquote>
<p>有些重复的 ╮(╯Д╰)╭</p>
</blockquote>
<p>过程简单来说有两步：</p>
<ol>
<li>使用Scrapy采集淘宝数据</li>
<li>使用Flask展示数据</li>
</ol>
<h1 id="采集数据"><a href="#采集数据" class="headerlink" title="采集数据"></a>采集数据</h1><h2 id="抓包分析"><a href="#抓包分析" class="headerlink" title="抓包分析"></a>抓包分析</h2><p>其实采集淘宝数据的方法真的很多很多，这里不讨论Selenium，只说如何抓包分析，先提供几个接口，供观众老爷观赏：</p>
<ol>
<li><a href="https://s.taobao.com/list?seller_type=taobao&amp;json=on" target="_blank" rel="external">https://s.taobao.com/list?seller_type=taobao&amp;json=on</a></li>
<li><a href="https://s.m.taobao.com/search?event_submit_do_new_search_auction=1&amp;_input_charset=utf-8&amp;topSearch=1&amp;atype=b&amp;searchfrom=1&amp;action=home%3Aredirect_app_action&amp;from=1&amp;q=%E6%B0%B4%E6%9E%9C&amp;sst=1&amp;n=44&amp;buying=buyitnow&amp;m=api4h5&amp;abtest=25&amp;wlsort=25&amp;page=1" target="_blank" rel="external">https://s.m.taobao.com/search?event_submit_do_new_search_auction=1&amp;_input_charset=utf-8&amp;topSearch=1&amp;atype=b&amp;searchfrom=1&amp;action=home%3Aredirect_app_action&amp;from=1&amp;q=%E6%B0%B4%E6%9E%9C&amp;sst=1&amp;n=44&amp;buying=buyitnow&amp;m=api4h5&amp;abtest=25&amp;wlsort=25&amp;page=1</a></li>
<li><a href="http://h5.m.taobao.com/app/selectassistant/www/choiceness/index.html?m=select&amp;vm=nw&amp;ttid=null&amp;utd_id=null&amp;page=2&amp;n=44&amp;q=%E6%B0%B4%E6%9E%9C" target="_blank" rel="external">http://h5.m.taobao.com/app/selectassistant/www/choiceness/index.html?m=select&amp;vm=nw&amp;ttid=null&amp;utd_id=null&amp;page=2&amp;n=44&amp;q=%E6%B0%B4%E6%9E%9C</a></li>
</ol>
<p>这三种接口都可以采集数据，别问我怎么知道的，经历过千百次失败。<br>这里选择的是第二种，大家可以试试这几种的区别。</p>
<p>使用第二种去采集数据时，返回的是json数据，数据量已经很多。</p>
<p><img src="https://i.imgur.com/snWOQbo.png" alt=""></p>
<p>其中有<strong>几点坑</strong>，分享下。</p>
<ol>
<li>URL不同。淘宝和天猫的链接是不同的，移动端和网页端是不同的。</li>
<li>这里显示的 <code>commentCount</code> （评论数量）并不是真实的，你可以打开详情页对比</li>
<li>评论数量的接口。淘宝和天猫的都可以使用 <code>https://rate.taobao.com/detailCount.do?itemId=商品ID</code>，每件商品都有自己的唯一ID</li>
</ol>
<h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>这里直接把主代码贴出来，使用 <code>Mongo</code> 保存数据。 </p>
<p><code>taobao.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> copy</div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> TaobaoInfoItem</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">from</span> traceback <span class="keyword">import</span> format_exc</div><div class="line"></div><div class="line">kw = [<span class="string">'网络服务'</span>, <span class="string">'装潢'</span>, <span class="string">'护理'</span>, <span class="string">'速食'</span>, <span class="string">'运动鞋'</span>, <span class="string">'运动服'</span>, <span class="string">'男装'</span>, <span class="string">'配件'</span>, <span class="string">'蔬果'</span>, <span class="string">'干货'</span>, <span class="string">'攻略'</span>, <span class="string">'地毯'</span>, <span class="string">'文具'</span>, <span class="string">'书籍'</span>, <span class="string">'人偶'</span>, <span class="string">'饰品'</span>, <span class="string">'报纸'</span>, <span class="string">'时尚饰品'</span>, <span class="string">'美发'</span>, <span class="string">'运动包'</span>, <span class="string">'粮油'</span>, <span class="string">'吃喝玩乐折扣券'</span>, <span class="string">'工具'</span>, <span class="string">'彩妆'</span>, <span class="string">'演出'</span>, <span class="string">'童装'</span>, <span class="string">'个性定制'</span>, <span class="string">'数码相机'</span>, <span class="string">'日化'</span>, <span class="string">'游戏'</span>, <span class="string">'尿片'</span>, <span class="string">'安防'</span>, <span class="string">'摄像机'</span>, <span class="string">'厨房电器'</span>, <span class="string">'办公设备'</span>, <span class="string">'网店'</span>, <span class="string">'ZIPPO'</span>, <span class="string">'杂志'</span>, <span class="string">'礼品'</span>, <span class="string">'摄影器材'</span>, <span class="string">'喂哺等用品'</span>, <span class="string">'软件'</span>, <span class="string">'笔记本电脑'</span>, <span class="string">'明星'</span>, <span class="string">'登山'</span>, <span class="string">'居家日用'</span>, <span class="string">'户外'</span>, <span class="string">'电脑硬件'</span>, <span class="string">'流行首饰'</span>, <span class="string">'娃娃'</span>, <span class="string">'收纳'</span>, <span class="string">'影视'</span>, <span class="string">'音乐'</span>, <span class="string">'电玩'</span>, <span class="string">'音像'</span>, <span class="string">'香水'</span>, <span class="string">'水产'</span>, <span class="string">'热销女包'</span>, <span class="string">'大家电'</span>, <span class="string">'其他保健营养品'</span>, <span class="string">'箱包皮具'</span>, <span class="string">'瑞士军刀'</span>, <span class="string">'3C数码配件市场'</span>, <span class="string">'电脑周边'</span>, <span class="string">'男包'</span>, <span class="string">'玩具'</span>, <span class="string">'U盘'</span>, <span class="string">'模型'</span>, <span class="string">'孕妇装'</span>, <span class="string">'窗帘'</span>, <span class="string">'眼镜'</span>, <span class="string">'促销店铺'</span>, <span class="string">'五金'</span>, <span class="string">'旅行'</span>, <span class="string">'洗护'</span>, <span class="string">'清洁'</span>, <span class="string">'移动存储'</span>, <span class="string">'卫浴'</span>, <span class="string">'野营'</span>, <span class="string">'颈环配件'</span>, <span class="string">'童鞋'</span>, <span class="string">'家装饰品'</span>, <span class="string">'显示器'</span>, <span class="string">'闪存卡'</span>, <span class="string">'传统滋补品'</span>, <span class="string">'耗材'</span>, <span class="string">'灯具'</span>]</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TaobaoSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'taobao'</span></div><div class="line">    allowed_domains = [<span class="string">'taobao.com'</span>]</div><div class="line">    start_urls = [<span class="string">'https://s.m.taobao.com/search?event_submit_do_new_search_auction=1&amp;_input_charset=utf-8&amp;topSearch=1&amp;atype=b&amp;searchfrom=1&amp;action=home%3Aredirect_app_action&amp;from=1&amp;q=&#123;&#125;&amp;sst=1&amp;n=44&amp;buying=buyitnow&amp;m=api4h5&amp;abtest=25&amp;wlsort=25&amp;page=&#123;&#125;'</span>.format(i, y) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">101</span>) <span class="keyword">for</span> y <span class="keyword">in</span> kw]</div><div class="line"></div><div class="line">    <span class="comment"># 其他接口 http://h5.m.taobao.com/app/selectassistant/www/choiceness/index.html?m=select&amp;vm=nw&amp;ttid=null&amp;utd_id=null&amp;page=2&amp;n=44&amp;q=%E6%B0%B4%E6%9E%9C</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = TaobaoInfoItem()</div><div class="line">        ListItem = json.loads(response.text)[<span class="string">'listItem'</span>]</div><div class="line"></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> ListItem:</div><div class="line">            item[<span class="string">'name'</span>] = i[<span class="string">'name'</span>]</div><div class="line">            item[<span class="string">'title'</span>] = i[<span class="string">'title'</span>]</div><div class="line">            item[<span class="string">'area'</span>] = i[<span class="string">'area'</span>]</div><div class="line"></div><div class="line">            <span class="comment"># 处理不同的URL</span></div><div class="line">            url = []</div><div class="line">            <span class="keyword">if</span> <span class="string">'https:'</span> <span class="keyword">not</span> <span class="keyword">in</span> i[<span class="string">'url'</span>]:</div><div class="line">                <span class="keyword">if</span> <span class="string">'detail.m.tmall.com'</span> <span class="keyword">in</span> i[<span class="string">'url'</span>]:</div><div class="line">                    url.append(<span class="string">'https:'</span> + i[<span class="string">'url'</span>].replace(<span class="string">'.m'</span>,<span class="string">''</span>))</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    url.append(<span class="string">'https:'</span> + i[<span class="string">'url'</span>])</div><div class="line">            <span class="keyword">if</span> <span class="string">'https:'</span>  <span class="keyword">in</span> i[<span class="string">'url'</span>]:</div><div class="line">                url.append(i[<span class="string">'url'</span>])</div><div class="line">            item[<span class="string">'url'</span>] = url</div><div class="line">            print(item[<span class="string">'url'</span>])</div><div class="line"></div><div class="line">            <span class="comment"># 评论网址</span></div><div class="line">            comment_url = []</div><div class="line">            ur = item[<span class="string">'url'</span>]</div><div class="line">            comment_url.append(ur[<span class="number">0</span>] + <span class="string">'#J_Reviews'</span>)</div><div class="line">            item[<span class="string">'comment_url'</span>] = comment_url</div><div class="line"></div><div class="line">            item[<span class="string">'fastPostFee'</span>] = i[<span class="string">'fastPostFee'</span>]</div><div class="line">            item[<span class="string">'sales'</span>] = i[<span class="string">'act'</span>]</div><div class="line">            item[<span class="string">'price'</span>] = i[<span class="string">'price'</span>]</div><div class="line">            item[<span class="string">'originalPrice'</span>] = i[<span class="string">'originalPrice'</span>]</div><div class="line">            item[<span class="string">'nick'</span>] = i[<span class="string">'nick'</span>]</div><div class="line">            item[<span class="string">'id'</span>] = i[<span class="string">'item_id'</span>]</div><div class="line">            item[<span class="string">'loc'</span>] = i[<span class="string">'sellerLoc'</span>]</div><div class="line"></div><div class="line">            <span class="comment"># 图片链接</span></div><div class="line">            img_url = []</div><div class="line">            img_url.append(<span class="string">'http:'</span> + i[<span class="string">'img2'</span>])</div><div class="line">            item[<span class="string">'img_url'</span>] = img_url</div><div class="line"></div><div class="line">            count_url = []</div><div class="line">            count_url.append(<span class="string">'https://rate.taobao.com/detailCount.do?itemId='</span> + i[<span class="string">'item_id'</span>])</div><div class="line"></div><div class="line">            <span class="keyword">for</span> url <span class="keyword">in</span> count_url:</div><div class="line">                <span class="keyword">yield</span> scrapy.Request(</div><div class="line">                    url,</div><div class="line">                    callback=self.detail_parse,</div><div class="line">                    meta=&#123;<span class="string">'item'</span>: copy.deepcopy(item)&#125;, <span class="comment"># 使用copy.deepcopy深复制，否则数据不对啊</span></div><div class="line">                    dont_filter=<span class="keyword">True</span>,</div><div class="line">                    errback=self.error_back</div><div class="line">                )</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = response.meta[<span class="string">'item'</span>]</div><div class="line">        pat_count = <span class="string">'&#123;"count":(.*?)&#125;'</span></div><div class="line">        item[<span class="string">'count'</span>] = re.findall(pat_count, str(response.body))</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">error_back</span><span class="params">(self, e)</span>:</span></div><div class="line">        _ = e</div><div class="line">        self.logger.error(format_exc())</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">'''</span></div><div class="line">['羽绒服', '毛呢大衣', '毛衣', '冬季外套', '新品', '裤子', '连衣裙', '腔调', '秋冬新品', '淘特莱斯', '淘先生', '拾货', '秋冬外套', '时尚套装', '潮牌', '爸爸装', '春新品', '性感诱惑', '甜美清新', '简约优雅', '奢华高贵', '运动风', '塑身', '基础内衣', '轻薄款', '长款', '短款', '毛领', '加厚', '被子', '鹅绒', '新品', '秋款', '夹克', '卫衣', '西装', '风衣', '皮衣', '毛呢外套', '薄羽绒', '无钢圈', '无痕文胸', '蕾丝内衣', '运动文胸', '聚拢文胸', '大码文胸', '抹胸式', '隐形', '廓形', '双面呢', '羊绒', '中长款', '短款', '毛领', '设计师款', '系带', 'T恤', '长袖T', '打底衫', '纯色', '衬衫', '长袖款', '商务款', '时尚款', '睡衣套装', '睡裙', '睡袍浴袍', '外穿家居', '女士睡衣', '男士睡衣', '情侣睡衣', '亲子睡衣', '马海毛', '貂绒', '羊绒', '羊毛', '开衫', '中长款', '短款', '卡通', '休闲裤', '工装裤', '运动裤', '长裤', '牛仔裤', '小脚裤', '哈伦裤', '直筒裤', '女士内裤', '男士内裤', '三角裤', '平角裤', '丁字裤', '阿罗裤', '星期裤', '低腰', '外套', '套装', '风衣', '卫衣', '真皮皮衣', '马甲', '小西装', '唐装', '中老年', '薄毛衣', '针织开衫', '圆领毛衣', 'V领毛衣', '纯色毛衣', '民族风', '羊毛衫', '羊绒衫', '船袜', '男人袜', '连裤袜', '隐形袜', '收腹裤', '塑身衣', '美体裤', '收腹带', '帆布鞋', '高帮', '低帮', '内增高', '懒人鞋', '厚底', '韩版', '系带', '情侣款', '运动风鞋', '厚底', '内增高', '星星鞋', '系带', '上新', '人气款', '单肩包', '斜挎包', '手提包', '迷你包', '手拿包', '小方包', '棒球帽', '鸭舌帽', '遮阳帽', '渔夫帽', '草帽', '平顶帽', '嘻哈帽', '贝雷帽', '牛仔帽', '爵士帽', '高跟', '平底', '厚底', '中跟', '粗跟', '坡跟', '浅口', '尖头', '圆头', '运动款', '头层牛皮', '内增高', '松糕鞋', '豆豆鞋', '商务', '休闲', '潮范', '胸包', '腰包', '单肩', '斜跨', '手提', '手拿', '帆布', '牛皮', '女士腰带', '男士皮带', '帆布腰带', '腰封', '腰链', '针扣头', '平滑扣', '自动扣', '真皮', '正品', '厚底', '内增高', '星星鞋', '系带', '一脚蹬', '魔术贴', '气垫', '网状', '印花', '铆钉', '水洗皮', '卡通', '原宿', '糖果色', '商务', '运动', '帆布', '牛皮', '女士围巾', '男士围巾', '披肩', '丝巾', '假领', '小方巾', '三角巾', '大方巾', '真丝', '雪纺', '棉质', '亚麻', '蕾丝', '青春潮流', '商务皮鞋', '休闲皮鞋', '正装皮鞋', '商务休闲', '布洛克', '内增高', '反绒皮', '真皮', '潮流低帮', '韩版', '英伦', '复古', '铆钉', '编织', '豹纹', '大头', '拉杆箱', '密码箱', '学生箱', '子母箱', '拉杆包', '万向轮', '飞机轮', '航空箱', '铝框', '女士手套', '男士手套', '真皮手套', '蕾丝手套', '防晒手套', '半指手套', '分指手套', '连指手套', '短款手套', '长款手套', '皮鞋', '低帮', '反绒皮', '大头鞋', '豆豆鞋', '帆船鞋', '懒人鞋', '帆布/板鞋', '高帮', '凉鞋/拖鞋', '沙滩鞋', '人字拖', '皮凉鞋', '洞洞鞋', '钱包', '潮包馆', '真皮包', '手机包', '大牌', 'coach', 'MK', 'MCM', '毛线', '鞋垫', '鞋带', '领带', '领结', '袖扣', '手帕', '布面料', '耳套', '领带夹', '婚纱配件', '皮带扣', '英国牛栏', '英国爱他美', '美赞臣', '雅培', '澳洲爱他美', '可瑞康', '惠氏', '贝因美', '推车', '驱蚊器', '婴儿床', '理发器', '奶瓶', '餐椅', '背带腰凳', '安全座椅', '内衣', '内裤', '喂奶枕', '收腹带', '妈咪包', '待产包', '防辐射服', '储奶袋', '米粉', '肉松', '磨牙棒', '果泥', '益生菌', '清火开胃', '钙铁锌', '维生素', '花王', 'moony', '大王', '帮宝适', '雀氏', '好奇', '妈咪宝贝', '安儿乐', '海淘奶粉', '海淘辅食', '海淘营养品', '直邮花王', '海淘洗护', '海淘奶瓶', '海淘餐具', '海淘孕产', 'T恤', '连衣裙', '泳装', '套装', '衬衫', '防晒服', '半身裙', '短裤', '凉鞋', '沙滩鞋', '洞洞鞋', '网鞋', '学步鞋', '拖鞋', '帆布鞋', '宝宝鞋', '母女裙', '父子装', '亲子T恤', '亲子衬衫', '亲子套装', '母女鞋', '父子鞋', '家庭鞋', '沙滩戏水', '早教启蒙', '拼插益智', '遥控模型', '运动户外', '学习爱好', '卡通公仔', '亲子互动', '电动车', '自行车', '学步车', '手推车', '三轮车', '滑板车', '扭扭车', '儿童轮滑', '早教机', '点读机', '健身架', '布书', '串/绕珠', '床/摇铃', '爬行垫', '木质拼图', '卸妆', '面膜', '洁面', '防晒', '面霜', '爽肤水', '眼霜', '乳液', '补水', '美白', '收缩毛孔', '控油', '祛痘', '祛斑', '去黑眼圈', '去黑头', 'BB霜', '粉底液', '唇膏', '隔离', '遮瑕', '指甲油', '粉饼', '彩妆套装', '女士香水', '男士香水', '中性香水', '淡香水', '古龙水', '香精', '复方精油', '香体乳', '洗发水', '护发素', '染发', '烫发', '造型', '假发', '洗护套装', '假发配件', '美胸', '纤体', '胸部护理', '身体护理', '塑身', '脱毛', '手部保养', '足部护理', '眼线', '睫毛膏', '眼影', '眉笔', '假睫毛', '眼霜', '双眼皮贴', '眼部护理', '劲能醒肤', '清洁面膜', '男性主义', '剃须膏', '男士套装', '男士防晒', '火山岩', '爽身走珠', '抗皱', '抗敏感', '保湿', '去眼袋', '滋润', '抗氧化', '深层清洁', '雅诗兰黛', '兰蔻', '资生堂', '自然乐园', 'SK-II', '悦诗风吟', '水宝宝', '契尔氏', '芦荟胶', '彩妆盘', '腮红', '香氛', '高光棒', '修容', 'V脸', '去角质', '洁面', '爽肤水', '精华', '乳液', '鼻贴', '马油', '牛肉干', '鲜花饼', '红枣', '糖果', '巧克力', '山核桃', '松子', '卤味', '饼干', '话梅', '蔓越莓', '薯片', '奇异果', '芒果', '樱桃', '橙子', '秋葵', '苹果', '番茄', '柠檬', '椰子', '榴莲', '大米', '橄榄油', '小米', '黄豆', '赤豆', '火腿', '香肠', '木耳', '香菇', '豆瓣酱', '海参', '龙虾', '瑶柱', '土鸡', '牛排', '三文鱼', '咸鸭蛋', '皮蛋', '五花肉', '北极贝', '鸡尾酒', '红酒', '啤酒', '白酒', '梅酒', '洋酒', '清酒', '滋补酒', '茅台', '五粮液', '麦片', '咖啡', '牛奶', '柚子茶', '酸梅汤', '矿泉水', '酵素', '藕粉', '姜茶', '酸奶粉', '铁观音', '红茶', '花草茶', '龙井', '普洱', '黑茶', '碧螺春', '毛峰', '袋泡茶', '白茶', '枸杞', '人参', '石斛', '燕窝', '雪蛤', '蜂蜜', '天麻', '花粉', '党参', '红花', '芒果干', '鱼子酱', '咖啡', '橄榄油', '薯片', '巧克力', '咖喱', '方便面', '红酒', '麦片', '项链', '手链', '戒指', '发饰', '银饰', '水晶', '耳饰', '手镯', '翡翠', '彩宝', '蜜蜡', '裸钻', '珍珠', '黄金', '钻石', '金条', '和田玉', '翡翠', '水晶/佛珠', '黄金', '手表', '眼镜', '瑞士表', '机械表', '时装表', '儿童表', '电子表', '情侣表', '石英表', '手表配件', '太阳镜', '偏光镜', '近视镜', '司机镜', '护目镜', '眼镜配件', '运动镜', '老花镜', 'zippo', '电子烟', '烟斗', '瑞士军刀', '绝美酒具', '风格男表', '佛珠', '水晶', '碧玺', '925银', '施华洛', '翡翠', '珍珠', '黄金', '银项链', '流行风格', '天然水晶', '锆石水晶', '佛珠项链', '人造水晶', '925银', '翡翠', '和田玉', '复古泰银', '粉晶手镯', '黄金手镯', '日韩', '甜美', '复古/宫廷', '欧美', '瑞丽', '波西米亚', '民族风', '发饰', '项链', '套装', '耳饰', '韩式', '头饰', '三件套', '合金配件', '银饰', '水晶配珠', '琉璃', '珍珠母贝', '有机玻璃', '人造水晶', '设计师', '半包装修', '全包装修', '全案装修', '装修监理', '清包施工', '局部装修', '验房量房', '装修空气质量检测', '装修污染治理', '整体橱柜', '定制衣柜', '定制吊顶', '定制淋浴房', '门', '窗', '定制柜', '楼梯', '榻榻米定制', '地暖', '吸顶灯', '吊灯', '吸吊两用灯', '筒灯', '射灯', '台灯', '落地灯', '室外灯', '壁灯', '小夜灯', '浴室柜', '普通马桶', '花洒套装', '一体智能马桶', '智能马桶盖板', '淋浴房', '面盆龙头', '地漏', '五金挂件', '浴霸', 'PVC墙纸', '无纺布墙纸', '纯纸墙纸', '墙布', '沙粒墙纸', '绒面墙纸', '定制壁画', '3D墙纸', '实木地板', '实木复合地板', '强化复合地板', '竹地板', '户外地板', 'PVC地板', '防静电地板', '防潮膜', '踢脚线', '地板龙骨', '仿古砖', '釉面砖', '玻化砖', '微晶石', '马赛克', '抛晶砖', '通体砖', '花片', '腰线', '瓷砖背景墙', '插座', '开关', '电线', '监控器材', '智能家居', '防盗报警器材', '消防报警设备', '接线板插头', '布线箱', '断路器', '涂料乳胶漆', '油漆', '水管', '板材', '木方', '阳光房', '线条', '天然大理石', '人造大理石', '防水涂料', '实木床', '布艺床', '皮艺床', '床垫', '衣柜', '斗柜', '梳妆台', '子母床', '床头柜', '儿童床', '皮艺沙发', '布艺沙发', '沙发床', '实木沙发', '懒人沙发', '电视柜', '茶几', '鞋柜', '玄关厅', '衣帽架', '餐桌', '折叠餐桌', '欧式餐桌', '实木餐桌', '大理石餐桌', '餐椅', '餐边柜', '换鞋凳', '角柜', '屏风', '餐桌', '折叠餐桌', '欧式餐桌', '实木餐桌', '大理石餐桌', '餐椅', '餐边柜', '换鞋凳', '角柜', '屏风', '蚊帐', '三开蚊帐', '凉席', '凉席套件', '冰丝席', '藤席', '牛皮席', '夏凉被', '空调被', '天丝套件', '床单', '床笠', '四件套', '全棉套件', '被套', '蚕丝被', '羽绒被', '枕头', '乳胶枕', '记忆枕', '床褥', '毛毯', '定制窗帘', '地毯', '沙发垫', '靠垫', '桌布桌旗', '飘窗垫', '地垫', '餐垫', '防尘罩', '椅垫', '成品窗帘', '沙发罩', '摆件', '花瓶', '仿真花', '台钟闹钟', '香薰炉', '储物罐', '装饰碗盘', '木雕', '烟灰缸', '纸巾盒', '蜡烛烛台', '仿真饰品', '现代装饰画', '无框画', '后现代画', '油画', '挂钟', '照片墙', '新中式', '北欧家饰', '美式乡村', '挂钩搁板', '装饰挂钩', '壁饰', '扇子', '毛巾', '浴巾', '口罩', '隔音耳塞', '竹炭包', '眼罩', '夏季凉拖', '居家鞋', '夏季清凉', '湿巾', '晴雨伞', '驱蚊灯', '驱蚊液', '冰格', '保鲜产品', '密封罐', '防潮制品', '电扇/冰垫', '5元小物', '被子防尘袋', '收纳盒', '收纳袋', '大衣/西服罩', '护洗袋', '收纳凳', '鞋柜', '置物架', '桌用收纳', '内衣收纳', '洗发护发', '沐浴露', '漱口水', '卫生巾', '洗手液', '牙膏', '纸巾', '香皂', '沐浴球/浴擦/浴刷', '指甲刀', '剃须刮毛刀', '沐浴球', '浴室角架', '浴帘杆', '拖把', '垃圾桶', '梳子镜子', '围裙', '百洁布', '海绵擦', '餐具', '锅具', '刀具', '炖锅', '蒸锅', '汤锅', '煎锅', '压力锅', '炒锅', '菜板砧板', '一次性餐桌用品', '酒杯酒具', '咖啡器具', '碗盘碟', '刀叉勺', '餐具瓷器套装', '餐桌小物', '饭盒', '厨房储物', '一次性餐桌用品', '茶具', '茶壶', '飘逸杯', '功夫茶杯', '玻璃杯', '杯垫', '保温杯', '马克杯', '保温壶', '情侣杯', '晒衣篮', '晾衣杆', '脏衣篮', '衣架', '家庭清洁剂', '蓝泡泡', '管道疏通器', '塑胶手套', '医药箱', '垃圾袋', '汽车首页', '新车先购', '车海淘', '二手车', '爱车估价', 'suv', '别克', '大众', '宝马', '座垫', '座套', '脚垫', '香水', '旅行床', '遮阳挡', '挂件摆件', '安全座椅', '专车专用座垫', '脚垫', '安全座椅', '香水', '钥匙包', '挂件', '座套', '后备箱垫', '置物箱', '智能车机', '后视镜', '安卓导航', '便携GPS', 'DVD导航', '电子狗', '流动测速', '导航软件', '记录仪', '预警仪', 'GPS', '车机', '倒车雷达', '智能后视镜', '蓝牙', '防盗器', 'MP3', '4S保养', '电瓶安装', '配件安装', '隔热膜', '洗车卡', '镀晶镀膜', '连锁保养', '上门服务', '行车记录仪', '逆变器', '跟踪器', '充电器', '充气泵', '胎压监测', '车载冰箱', '空气净化', '车衣', 'SUV踏板', '晴雨挡', '改色膜', '汽车车标', '车牌架', '轮胎', '雨刮器', '机油滤芯', '空气滤芯', '空调滤芯', '减震', '刹车片', '火花塞', '轮胎', '雨刮', '机油', '高亮大灯', '挡泥板', '保险杠', '车顶架', '轮眉', '轮毂', '排气', '保险杠', '汽车包围', '氙气灯', '车顶架', '脚踏板', '大灯总成', '尾翼', '轮毂', '汽车装饰灯', '排气筒', '尾喉', '车身饰条', '添加剂', '防冻液', '玻璃水', '车蜡', '补漆笔', '洗车机', '洗车水枪', '车掸蜡拖', '车蜡', '洗车机', '补漆笔', '抛光机', '打蜡海绵', '车用水桶', '擦车巾', '车刷', '装饰条', '车贴', '尾喉', '改色膜', '防爆膜', '晴雨挡', '日行灯', '车衣', '夏季座垫', '遮阳挡', '防眩蓝镜', '防晒手套', 'iPhone', '小米', '华为', '三星', '魅族', '纽扣', '酷派', 'VIVO', 'iPad', '小米', '三星', '10寸', '台电', 'win8', '蓝魔', '华为', 'DIY电脑', '一体机', '路由器', '显示器', '学生', 'CPU', '移动硬盘', '无线鼠标', '苹果', '联想', 'Thinkpad', '戴尔', '华硕', 'Acer', '神州', '三星', '单反', '自拍神器', '拍立得', '佳能', '微单', '镜头', '卡西欧', '尼康', '充电宝', '智能穿戴', '蓝牙耳机', 'iPhone6壳', '电脑包', '手机贴膜', '手机壳套', '三脚架', '保护壳套', '炫彩贴膜', '移动电源', '相机配件', '手机零件', '自拍神器', '移动POS支付', '电池', '儿童手表', 'Apple Watch', '智能手表', '智能手环', '智能配饰', '智能健康', '智能排插', '智能眼镜', '游戏掌机', '家用游戏机', '游戏手柄', 'PS主机', 'XBOX', '任天堂配件', 'PS主机配件', 'XBOX配件', '路由器', '网关', '交换机', '光纤设备', '网络存储设备', '无线上网卡', 'TP-LINK', '小米路由器', 'MP3', 'MP4', '录音笔', '索尼', '飞利浦', 'ipod', '爱国者', '耳机', 'U盘', '闪存卡', '记忆棒', '移动硬盘', '希捷', '三星', 'Sandisk', '金士顿', '电磁炉', '电水壶', '料理机', '电饭煲', '榨汁机', '净水器', '豆浆机', '烤箱', '电风扇', '空调扇', '挂烫机', '扫地机', '吸尘器', '加湿器', '除湿机', '对讲机', '空气净化', '理发器', '电子称', '美容仪', '按摩椅', '按摩披肩', '血压计', '足浴器', '电动牙刷', '剃须刀', '耳机', '音响', '网络机顶盒', '麦克风', '扩音器', 'HiFi套装', '蓝光DVD', '低音炮', '打印机', '投影仪', '硒鼓墨盒', 'A4纸', '一体机', '学生文具', '保险柜', '电纸书', '学习机', '冰箱', '空调', '平板电视', '油烟机', '燃气灶', '消毒柜', '厨电套装', '热水器', '洗衣机', '包装设备', '包装纸箱', '塑料袋', '包装胶带', '铭牌', '快递袋', '气泡膜', '真空机', '笔记本', '文件袋', '钢笔', '胶粘用品', '铅笔', '计算器', '白板', '台历', '设计定制', '企业用品定制', 'T恤印制', '杯子定制', 'ppt模板', '班服定制', '洗照片', '人偶定制', '电子电工', '气动元件', '水泵', '阀门', '电钻', '焊接设备', '万用表', '雕刻机', '办公家具', '商业设施', '办公桌', '陈列柜', '货架', '广告牌', '文件柜', '沙发', '网络设备', '电子元器件', '路由器', '交换机', '光纤设备', '视频会议', '无线安全保密', '机柜', '餐饮美食', '冰淇淋', '火锅', '购物卡券', '体检配镜', '美容美甲', '保险理财', '婚纱摄影', '旅行团购', '住在帝都', '住在魔都', '住在杭州', '住在南京', '住在广州', '住在青岛', '住在宁波', '住在成都', '少儿英语', '小学教育', '潜能开发', '家长训练', '孕产育儿', '少儿绘画', '婴幼早教', '音乐', 'Q币充值', '点卡充值', '充游戏币', '游戏代练', '超值账号', '手游充值', '电竞比赛', '游戏帮派', '潇洒一室', '靠谱二室', '舒适三房', '大四室', '私藏别墅', '景观居所', '轨道沿线', '学区房', '实用英语', '网站制作', 'IT技能', '会计职称', '一对一', '办公软件', '日语', '编程', '英雄联盟', '剑侠情缘3', '征途2', '魔域', '我叫MT', '刀塔传奇', 'DOTA2', 'DNF', '魔兽世界', '自助餐', '个性写真', '儿童写真', '电影票团购', '上门服务', '周边旅游', '境外旅游', '基金理财', '魅力健身', '时尚美妆', '手工DIY', '舞蹈', '减肥瑜伽', '个人形象', '美剧英语', '摄影', '美女陪练', '轻松甩肉', '基金理财', '淘宝美工', '办公技能', '婚纱摄影', '婚礼策划', '三亚婚拍', '厦门婚拍', '青岛婚拍', '北京婚拍', '杭州婚拍', '上海婚拍', '新娘跟妆', '婚礼跟拍', '婚礼司仪', '婚车租赁', '任意洗', '洗外套', '洗西装', '洗鞋', '洗四件套', '洗烫衬衫', '皮包护理', '洗窗帘', '洗地毯', '在线洗衣', '洗礼服', '洗玩具', '开荒保洁', '厨房保洁', '公司保洁', '家电清洗', '空调清洗', '洗油烟机', '冰箱清洗', '擦玻璃', '家政服务', '家庭保洁', '保洁服务', '钟点工', '洗衣机清洗', '卫生间保洁', '上门养车', '洗车', '封釉镀膜', '内饰清洗', '空调清洗', '汽车维修', '充加油卡', '年检代办', '玻璃贴膜', '汽车装饰', '底盘装甲', '四轮定位', '汽车改装', '违章代办', '汽车隔音', '上门按摩', '常规体检', '入职体检', '老人体检', '四维彩超', '孕前检查', '体检报告', '专业洗牙', '烤瓷牙', '胃部检测', '月嫂', '催乳师', '育儿嫂', '营养师', '普通保姆', '涉外保姆', '产后陪护', '临时看护', '管家', '烧饭阿姨', '宠物寄养', '宠物美容', '宠物配种', '宠物洗澡', '宠物摄影', '宠物托运', '宠物训练', '宠物医疗', '水族服务', '宠物绝育', '宠物洗牙', '宠物造型', '宠物体检', '居家搬家', '公司搬运', '空调拆装', '家电搬运', '家具搬运', '打孔', '电路维修', '甲醛测试', '开锁换锁', '杀虫消毒', '高空清洁', '除尘除螨', '网上办事', '代缴费', '代排队', '交罚单', '叫醒服务', '宝宝起名', '学车报名', '代邮代取', '话费充值', '代送鲜花', '水电煤缴费', '同城速递', '代办档案', '宽带费', '机场停车', '专利申请', '法律咨询', '专业翻译', '开发建站', '图片处理', '视频制作', '名片制作', '商标转让', '打印', '复印', '商标注册', '私人律师', '合同文书', '出国翻译', '手机维修', 'pad维修', '修台式机', '相机维修', '修笔记本', '修复印机', '修游戏机', '修导航仪', '软件服务', '延保服务', '硬件维修', '苹果维修', '小米维修', '三星维修', '安卓刷机', '数据恢复', '电脑维修', 'ipad维修', '华为维修', '重装系统', '家电维修', '相机维修', '硬盘维修', '苹果换屏', '换主板', '名企招聘', '高薪岗位', '文案编辑', '网店推广', '开发技术', '活动策划', '美工设计', '金牌客服', '大促客服', '网页设计', '人才认证', '图片设计', '摄影师', '店长', '运营主管', '客服主管', '美工主管', '跑步鞋', '篮球鞋', '休闲鞋', '足球鞋', '帆布鞋', '训练鞋', '徒步鞋', '登山鞋', '限量版', '板鞋', 'Rosherun', '运动套装', '运动卫衣', '长裤', '皮肤风衣', '健身服', '球服', '耐克', '阿迪达斯', '三叶草', '美津浓', '彪马', '狼爪', '山地车', '公路车', '骑行服', '头盔', '装备', '零件', '工具', '护具', '折叠车', '死飞', '水壶架', '行李架', '羽毛球拍', '羽毛球服', '羽毛球', '网球拍', '篮球', '篮球服', '足球', '足球服', '乒乓球拍', '橄榄球', '台球', '高尔夫', '吊床', '头灯', '遮阳棚', '望远镜', '照明', '野营帐篷', '野外照明', '烧烤炉', '望远镜', '潜水镜', '防潮垫', '皮划艇', '皮肤衣', '防晒衣', '冲锋衣', '探路者', '速干裤', '迷彩服', '战术靴', '登山鞋', 'crocs', '溯溪鞋', '户外鞋', '麻将机', '轮滑', '麻将', '象棋', '雀友', '飞镖', '桌上足球', '风筝', '陀螺', '空竹', '沙袋', '太极服', '甩脂机', '轮滑装备', '跑步机', '舞蹈', '瑜伽', '哑铃', '仰卧板', '踏步机', '划船机', '卧推器', '健身车', '呼啦圈', '舞蹈', '瑜伽', '广场舞', '舞蹈鞋', '拉丁鞋', '广场舞套装', '肚皮舞服装', '瑜伽垫', '瑜伽球', '瑜伽服', '鱼饵', '套装', '路亚', '附件', '鱼钩', '钓鱼工具', '船/艇', '台钓竿', '海钓竿', '溪流竿', '路亚竿', '矶钓杆', '单肩背包', '旅行包', '双肩背包', '挎包', '户外摄影包', '头巾', '运动水壶', '防水包', '电池', '电自行车', '平衡车', '滑板车', '头盔', '摩托车', '老年代步', '独轮车', '遮阳伞', '扭扭车', '折叠车', '仿真植物', '干花', 'DIY花', '手捧花', '鲜果蓝', '仿真蔬果', '开业花篮', '花瓶', '绿植同城', '园艺方案', '多肉植物', '桌面盆栽', '蔬菜种子', '水培花卉', '苔藓景观', '空气凤梨', '肥料', '花盆花器', '花卉药剂', '营养土', '园艺工具', '洒水壶', '花架', '铺面石', '热带鱼', '孔雀鱼', '底栖鱼', '虾螺', '龙鱼', '罗汉鱼', '锦鲤', '金鱼', '水母', '灯科鱼', '乌龟', '水草', '底砂', '水草泥', '沉木', '仿真水草', '假山', '氧气泵', '过滤器', '水草灯', '加热棒', '鱼粮', '水质维护', '硝化细菌', '除藻剂', '龟粮', '兔兔', '仓鼠', '龙猫', '雪貂', '粮食零食', '医疗保健', '笼子', '鹦鹉', '鸟笼', '观赏鸟', '蚂蚁工坊', '蜘蛛', '蚕', '大牌狗粮', '宠物服饰', '狗厕所', '宠物窝', '航空箱', '海藻粉', '羊奶粉', '宠物笼', '储粮桶', '剃毛器', '营养膏', '上门服务', '全新钢琴', '智能钢琴', '中古钢琴', '尤克里里', '民谣吉他', '萨克斯风', '口琴', '小提琴', '高达', '手办', '盒蛋', '兵人', '变形金刚', '圣衣神话', '钢铁侠', 'BJD', '拼装', '人偶', '猫砂', '猫粮', '猫爬架', '猫窝', '猫砂盆', '化毛膏', '猫罐头', '喂食器', '折耳猫', '猫抓板', '猫玩具', '猫笼', '古筝', '二胡', '葫芦丝', '战鼓', '古琴', '陶笛', '琵琶', '笛子', '动漫T恤', '动漫抱枕', 'COS', '背包', '项链', '颜文字', '哆啦A梦', '大白', '手表', '盗墓笔记', '海贼', '火影', 'LOL', '杀菌剂', '杀虫剂', '除草剂', '调节剂', '杀螨剂', '杀鼠剂', '敌敌畏', '草甘膦', '园林种苗', '动物种苗', '蔬菜种苗', '水果种苗', '粮油种子', '药材种苗', '食用菌种', '辣木籽', '氮肥', '磷肥', '钾肥', '叶面肥', '新型肥料', '复合肥', '生物肥料', '有机肥', '耕种机械', '收割机械', '农机配件', '植保机械', '拖拉机', '施肥机械', '粮油设备', '微耕机', '塑料薄膜', '大棚膜', '防渗膜', '鱼塘专用', '薄膜', '遮阳网', '篷布', '防虫网', '镰刀', '锹', '高压水枪', '锨', '镐', '耙子', '锄头', '叉', '猪饲料', '羊饲料', '牛饲料', '预混料', '饲料原料', '全价料', '饲料添加剂', '浓缩料', '加工设备', '养殖器械', '渔业用具', '养殖服务', '配种服务', '养鸡设备', '挤奶机', '母猪产床', '化学药', '中兽药', '抗生素', '驱虫', '消毒剂', '疫苗', '阿莫西林', '氟苯尼考']</div><div class="line"></div><div class="line">'''</div></pre></td></tr></table></figure>
<p>说明几点：</p>
<ol>
<li>在使用 <code>meta</code> 传递数据的时候，要使用 <code>copy.deepcopy</code> 深复制，详情查阅 <a href="http://blog.csdn.net/bestbzw/article/details/52894883" target="_blank" rel="external">【scrapy】item传递出错</a></li>
<li>关于搜索词。因为淘宝对搜索结果只会返回100页，所以我们这里增加索引词，来获得更多数据。可以在代码末尾发现更多关键词，有1000多个，这个list是从这里获得 <a href="https://www.taobao.com/markets/tbhome/market-list" target="_blank" rel="external">淘宝首页行业市场</a></li>
</ol>
<h2 id="反爬处理"><a href="#反爬处理" class="headerlink" title="反爬处理"></a>反爬处理</h2><p>呃呃，有意思的是，自己跑了一会获得了几万条数据，并没有任何的异常，没有在 <code>middlewares.py</code> 加代理，连 <code>UA</code> 都是固定的，好奇怪。</p>
<p><strong>可能长得帅的人品都比较好吧。</strong></p>
<h1 id="展示数据"><a href="#展示数据" class="headerlink" title="展示数据"></a>展示数据</h1><p>mongo 中的数据</p>
<p><img src="https://i.imgur.com/UBmhKoG.png" alt=""> </p>
<p>数据并不直观，所以我们选择展示出来，做一个小小的聚合搜索</p>
<p>同样看看主文件 <code>server.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></div><div class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, session, g, redirect, url_for, \</div><div class="line">    abort, render_template, flash</div><div class="line"><span class="keyword">from</span> bson <span class="keyword">import</span> json_util</div><div class="line"><span class="keyword">from</span> bson.objectid <span class="keyword">import</span> ObjectId</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line">conn = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</div><div class="line">db = conn[<span class="string">'taobao'</span>]</div><div class="line">goods_coll = db[<span class="string">'search'</span>]</div><div class="line">cate_coll = db[<span class="string">'categories'</span>]</div><div class="line">app = Flask(__name__)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">toJson</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="keyword">return</span> json.dumps(</div><div class="line">               data,</div><div class="line">               default=json_util.default,</div><div class="line">               ensure_ascii=<span class="keyword">False</span></div><div class="line">           )</div><div class="line"></div><div class="line"><span class="meta">@app.errorhandler(404)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">page_not_found</span><span class="params">(e)</span>:</span></div><div class="line">    <span class="keyword">return</span> render_template(<span class="string">'404.html'</span>), <span class="number">404</span></div><div class="line"></div><div class="line"><span class="meta">@app.route('/', methods=['GET'])</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">if</span> request.method == <span class="string">'GET'</span>:</div><div class="line">        total = goods_coll.count()</div><div class="line">        <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>, total=total)</div><div class="line">    <span class="comment">#if request.form['key']:</span></div><div class="line">    <span class="comment">#    key = request.form['key']</span></div><div class="line">    <span class="comment">#    return redirect(url_for('get_goods', key=key, page=1))</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="meta">@app.route('/search', methods=['GET'])</span></div><div class="line"><span class="meta">@app.route('/search/&lt;item&gt;', methods=['GET'])</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_goods</span><span class="params">(item=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> request.method == <span class="string">'GET'</span>:</div><div class="line">        page = request.args.get(<span class="string">'page'</span>, <span class="number">1</span>, type=int)</div><div class="line">        limit = request.args.get(<span class="string">'limit'</span>, <span class="number">30</span>, type=int)</div><div class="line">        p = (page - <span class="number">1</span>) * limit</div><div class="line">        offset = request.args.get(<span class="string">'offset'</span>, p, type=int)</div><div class="line">        catid = request.args.get(<span class="string">'catid'</span>, <span class="keyword">None</span>, type=str)</div><div class="line">        jsons = request.args.get(<span class="string">'json'</span>, <span class="string">'off'</span>)</div><div class="line">        keyword = request.args.get(<span class="string">'key'</span>, <span class="string">''</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> keyword:</div><div class="line">            keyword = item</div><div class="line"></div><div class="line">        <span class="keyword">if</span> catid:</div><div class="line">            cursor = goods_coll.find(&#123;<span class="string">'categories.catid'</span>: catid&#125;)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            cursor = goods_coll.find(&#123;<span class="string">'title'</span>: &#123;<span class="string">'$regex'</span>: keyword&#125; &#125;)</div><div class="line">        <span class="comment">#total = cursor.count()</span></div><div class="line">        <span class="comment">#flash('已查询到 %d 个结果.'%total)</span></div><div class="line">        results = cursor.skip(offset).limit(limit)</div><div class="line">        resultList = []</div><div class="line">        <span class="keyword">for</span> result <span class="keyword">in</span> results:</div><div class="line">            resultList.append(result)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> jsons == <span class="string">'off'</span>:</div><div class="line">            <span class="keyword">return</span> render_template(<span class="string">'search.html'</span>, entries=resultList)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> toJson(resultList)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="comment">#app.run(host='0.0.0.0')</span></div><div class="line">    app.run(debug=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>这里是借鉴了别人的项目，地址在这 <a href="https://github.com/1dot75cm/taobaobao" target="_blank" rel="external">1dot75cm/taobaobao</a></p>
<p>您可以自定义在 <code>\templates</code>目录下修改对应的html文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">λ ls</div><div class="line">404.html  categories.html  index.html  layout.html  search.html</div></pre></td></tr></table></figure>
<p>直接运行  <code>server.py</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">λ piython server.py</div><div class="line"> * Restarting with <span class="built_in">stat</span></div><div class="line"> * Debugger is active!</div><div class="line"> * Debugger PIN: 503-651-032</div><div class="line"> * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)</div></pre></td></tr></table></figure>
<p>打开 <a href="http://127.0.0.1:5000/" target="_blank" rel="external">http://127.0.0.1:5000/</a> 即可查看结果</p>
<h1 id="To-Do"><a href="#To-Do" class="headerlink" title="To Do"></a>To Do</h1><ul>
<li>数据去重</li>
<li>价格及销量可视化展示</li>
<li>速度太慢，应使用分布式</li>
<li>评论采集</li>
<li>加强爬虫的反爬措施</li>
</ul>
<p>Github ： <a href="https://github.com/zhangslob/Taobao_duoshou" target="_blank" rel="external">zhangslob/Taobao_duoshou</a> <strong>万水千山总是情，给个star行不行</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://github.com/1dot75cm/taobaobao" target="_blank" rel="external">1dot75cm/taobaobao</a></li>
<li><a href="http://blog.csdn.net/bestbzw/article/details/52894883" target="_blank" rel="external">【scrapy】item传递出错</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第二十篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/IzOzuZz.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
      <category term="Flask" scheme="https://zhangslob.github.io/tags/Flask/"/>
    
      <category term="淘宝" scheme="https://zhangslob.github.io/tags/%E6%B7%98%E5%AE%9D/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy学习实例（三）采集批量网页</title>
    <link href="https://zhangslob.github.io/2017/12/12/Scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E4%B8%89%EF%BC%89%E9%87%87%E9%9B%86%E6%89%B9%E9%87%8F%E7%BD%91%E9%A1%B5/"/>
    <id>https://zhangslob.github.io/2017/12/12/Scrapy学习实例（三）采集批量网页/</id>
    <published>2017-12-12T13:11:17.000Z</published>
    <updated>2017-12-19T14:39:05.546Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十九篇原创文章
</code></pre><p><img src="https://i.imgur.com/ALWvu6F.png" alt=""></p>
<a id="more"></a>
<p>先来首火影压压惊  (｡・`ω´･)</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=784594&auto=1&height=66"></iframe>

<p>最开始接触 <code>Rules</code>是在Scrapy的文档上看到的，但是并看读懂这是什么意思。接下来看别人的案例，有使用到Rules，便花了很多时间去了解。</p>
<blockquote>
<p>解释：<br>Rule是在定义抽取链接的规则，上面的两条规则分别对应列表页的各个分页页面和详情页，关键点在于通过<code>restrict_xpath</code>来限定只从页面特定的部分来抽取接下来将要爬取的链接。 </p>
</blockquote>
<p>其实用我的话来说就是，一个是可以便捷的进行翻页操作，二是可以采集二级页面，相当于打开获得详情页内容。所以若使用了 <code>Rules</code>，可以便捷的帮助我们采集批量网页。</p>
<h1 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h1><p><a href="http://python.usyiyi.cn/documents/scrapy_12/topics/spiders.html#crawlspider-example" target="_blank" rel="external">CrawlSpider示例</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">'example.com'</span></div><div class="line">    allowed_domains = [<span class="string">'example.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://www.example.com'</span>]</div><div class="line"></div><div class="line">    rules = (</div><div class="line">        <span class="comment"># Extract links matching 'category.php' (but not matching 'subsection.php')</span></div><div class="line">        <span class="comment"># and follow links from them (since no callback means follow=True by default).</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'category\.php'</span>, ), deny=(<span class="string">'subsection\.php'</span>, ))),</div><div class="line"></div><div class="line">        <span class="comment"># Extract links matching 'item.php' and parse them with the spider's method parse_item</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">'item\.php'</span>, )), callback=<span class="string">'parse_item'</span>),</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></div><div class="line">        self.logger.info(<span class="string">'Hi, this is an item page! %s'</span>, response.url)</div><div class="line">        item = scrapy.Item()</div><div class="line">        item[<span class="string">'id'</span>] = response.xpath(<span class="string">'//td[@id="item_id"]/text()'</span>).re(<span class="string">r'ID: (\d+)'</span>)</div><div class="line">        item[<span class="string">'name'</span>] = response.xpath(<span class="string">'//td[@id="item_name"]/text()'</span>).extract()</div><div class="line">        item[<span class="string">'description'</span>] = response.xpath(<span class="string">'//td[@id="item_description"]/text()'</span>).extract()</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure>
<blockquote>
<p>该spider将从example.com的首页开始爬取，获取category以及item的链接并对后者使用 parse_item 方法。 对于每个item response，将使用XPath从HTML中提取一些数据，并使用它填充Item。</p>
</blockquote>
<h1 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h1><p>为了更好的理解，我们来看看实际案例中<code>Rules</code>如何使用</p>
<h2 id="豆瓣应用"><a href="#豆瓣应用" class="headerlink" title="豆瓣应用"></a>豆瓣应用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">rules = [Rule(LinkExtractor(allow=(<span class="string">r'https://movie.douban.com/top250\?start=\d+.*'</span>))),</div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">r'https://movie.douban.com/subject/\d+'</span>)),</div><div class="line">            callback=<span class="string">'parse_item'</span>, follow=<span class="keyword">False</span>)</div><div class="line">]</div></pre></td></tr></table></figure>
<p>如果接触过<code>django</code>，那么可以发现这个规则与<code>django</code>的路由系统十分相似（django都已经忘完了 -_-！），其实这里使用的正则匹配。</p>
<p>使用 <code>r&#39;https://movie.douban.com/top250\?start=\d+.*&#39;</code>来匹配翻页链接，如：</p>
<ul>
<li><a href="https://movie.douban.com/top250?start=25&amp;filter=" target="_blank" rel="external">https://movie.douban.com/top250?start=25&amp;filter=</a></li>
<li><a href="https://movie.douban.com/top250?start=50&amp;filter=" target="_blank" rel="external">https://movie.douban.com/top250?start=50&amp;filter=</a></li>
</ul>
<p>使用<code>https://movie.douban.com/subject/\d+</code>来匹配具体电影的链接，如：</p>
<ul>
<li><a href="https://movie.douban.com/subject/1292052/" target="_blank" rel="external">https://movie.douban.com/subject/1292052/</a></li>
<li><a href="https://movie.douban.com/subject/1291546/" target="_blank" rel="external">https://movie.douban.com/subject/1291546/</a></li>
</ul>
<h2 id="链家应用"><a href="#链家应用" class="headerlink" title="链家应用"></a>链家应用</h2><p>爬虫的通常需要在一个网页里面爬去其他的链接，然后一层一层往下爬，scrapy提供了LinkExtractor类用于对网页链接的提取，使用LinkExtractor需要使用<code>CrawlSpider</code>爬虫类中，<code>CrawlSpider</code>与<code>Spider</code>相比主要是多了<code>rules</code>，可以添加一些规则，先看下面这个例子，爬取链家网的链接</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LianjiaSpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">"lianjia"</span></div><div class="line"></div><div class="line">    allowed_domains = [<span class="string">"lianjia.com"</span>]</div><div class="line"></div><div class="line">    start_urls = [</div><div class="line">        <span class="string">"http://bj.lianjia.com/ershoufang/"</span></div><div class="line">    ]</div><div class="line"></div><div class="line">    rules = [</div><div class="line">        <span class="comment"># 匹配正则表达式,处理下一页</span></div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">r'http://bj.lianjia.com/ershoufang/pg\s+$'</span>,)), callback=<span class="string">'parse_item'</span>),</div><div class="line"></div><div class="line">        <span class="comment"># 匹配正则表达式,结果加到url列表中,设置请求预处理函数</span></div><div class="line">        <span class="comment"># Rule(FangLinkExtractor(allow=('http://www.lianjia.com/client/', )), follow=True, process_request='add_cookie')</span></div><div class="line">    ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="comment"># 这里与之前的parse方法一样，处理</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>同样的，使用<code>r&#39;http://bj.lianjia.com/ershoufang/pg\s+$&#39;</code>来匹配下一页链接，如：</p>
<ul>
<li><a href="https://bj.lianjia.com/ershoufang/pg2/" target="_blank" rel="external">https://bj.lianjia.com/ershoufang/pg2/</a></li>
<li><a href="https://bj.lianjia.com/ershoufang/pg3/" target="_blank" rel="external">https://bj.lianjia.com/ershoufang/pg3/</a></li>
</ul>
<p>还可以使用 <code>r&#39;https://bj.lianjia.com/ershoufang/\d+.html&#39;</code>来匹配详情页链接，如：</p>
<ul>
<li><a href="https://bj.lianjia.com/ershoufang/101102126888.html" target="_blank" rel="external">https://bj.lianjia.com/ershoufang/101102126888.html</a></li>
<li><a href="https://bj.lianjia.com/ershoufang/101100845676.html" target="_blank" rel="external">https://bj.lianjia.com/ershoufang/101100845676.html</a></li>
</ul>
<h1 id="学习参数"><a href="#学习参数" class="headerlink" title="学习参数"></a>学习参数</h1><h2 id="Rule对象"><a href="#Rule对象" class="headerlink" title="Rule对象"></a>Rule对象</h2><p>Role对象有下面参数</p>
<ul>
<li><code>link_extractor</code>：链接提取规则</li>
<li><code>callback</code>：link_extractor提取的链接的请求结果的回调</li>
<li><code>cb_kwargs</code>：附加参数，可以在回调函数中获取到</li>
<li><code>follow</code>：表示提取的链接请求完成后是否还要应用当前规则（boolean），如果为False则不会对提取出来的网页进行进一步提取，默认为False</li>
<li><code>process_links</code>：处理所有的链接的回调，用于处理从response提取的links，通常用于过滤（参数为link列表）</li>
<li><code>process_request</code>：链接请求预处理（添加header或cookie等）</li>
</ul>
<h2 id="LinkExtractor"><a href="#LinkExtractor" class="headerlink" title="LinkExtractor"></a>LinkExtractor</h2><p>LinkExtractor常用的参数有：</p>
<ul>
<li><code>allow</code>：提取满足正则表达式的链接</li>
<li><code>deny</code>：排除正则表达式匹配的链接（优先级高于allow）</li>
<li><code>allow_domains</code>：允许的域名（可以是str或list）</li>
<li><code>deny_domains</code>：排除的域名（可以是str或list）</li>
<li><code>restrict_xpaths</code>：提取满足XPath选择条件的链接（可以是str或list）</li>
<li><code>restrict_css</code>：提取满足css选择条件的链接（可以是str或list）</li>
<li><code>tags</code>：提取指定标签下的链接，默认从a和area中提取（可以是str或list）</li>
<li><code>attrs</code>：提取满足拥有属性的链接，默认为href（类型为list）</li>
<li><code>unique</code>：链接是否去重（类型为boolean）</li>
<li><code>process_value</code>：值处理函数（优先级大于allow）</li>
</ul>
<p>关于LinkExtractor的详细参数介绍见<a href="https://doc.scrapy.org/en/latest/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml" target="_blank" rel="external">官网</a></p>
<blockquote>
<p>注意：在编写抓取Spider规则时，避免使用<code>parse</code>作为回调，因为<code>CrawlSpider</code>使用<code>parse</code>方法自己实现其逻辑。因此，如果你覆盖<code>parse</code>方法，爬行<code>Spider</code>将不再工作。</p>
</blockquote>
<p>最后说一个自己犯过的低级错误，我用Scrapy有个习惯，创建一个项目之后，直接<code>cd</code>目录，然后使用<code>genspider</code>命令，然后。。</p>
<figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">D:<span class="symbol">\B</span>ackup<span class="symbol">\桌</span>面</div><div class="line">λ scrapy startproject example</div><div class="line">New Scrapy project 'example', using template directory 'c:<span class="symbol">\\</span>users<span class="symbol">\\</span>administrator<span class="symbol">\\</span>appdata<span class="symbol">\\</span>local<span class="symbol">\\</span>programs<span class="symbol">\\</span>python<span class="symbol">\\</span>python36<span class="symbol">\\</span>lib<span class="symbol">\\</span>site-packages<span class="symbol">\\</span>scrapy<span class="symbol">\\</span>templates<span class="symbol">\\</span>project', created in:</div><div class="line">    D:<span class="symbol">\B</span>ackup<span class="symbol">\桌</span>面<span class="symbol">\e</span>xample</div><div class="line"></div><div class="line">You can start your first spider with:</div><div class="line">    cd example</div><div class="line">    scrapy genspider example example.com</div><div class="line"></div><div class="line">D:<span class="symbol">\B</span>ackup<span class="symbol">\桌</span>面</div><div class="line">λ cd example</div><div class="line"></div><div class="line">D:<span class="symbol">\B</span>ackup<span class="symbol">\桌</span>面<span class="symbol">\e</span>xample</div><div class="line">λ scrapy genspider em example.com</div><div class="line">Created spider 'em' using template 'basic' in module:</div><div class="line">  example.spiders.em</div></pre></td></tr></table></figure>
<p>然后我的<code>em.py</code>就变成了这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'em'</span></div><div class="line">    allowed_domains = [<span class="string">'example.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://example.com/'</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>注意，这个时候是不能使用<code>Rules</code>方法的，因为object不对，应该是</p>
<p><code>class EmSpider(CrawlSpider)</code></p>
<p>而不是<code>class EmSpider(scrapy.Spider):</code></p>
<p>共勉！！！</p>
<p>下一节应该会讲到Scrapy中各个组件的作用，以及这张神图</p>
<p><img src="https://i.imgur.com/PhD0NhL.png" alt=""></p>
<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><ul>
<li><a href="http://python.usyiyi.cn/documents/scrapy_12/topics/spiders.html#crawlspider-example" target="_blank" rel="external">CrawlSpider示例</a></li>
<li><a href="https://segmentfault.com/a/1190000007073049#articleHeader14" target="_blank" rel="external">scrapy学习笔记</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十九篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ALWvu6F.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
      <category term="Rules" scheme="https://zhangslob.github.io/tags/Rules/"/>
    
  </entry>
  
  <entry>
    <title>统计学学习笔记（二）数据整理与展示</title>
    <link href="https://zhangslob.github.io/2017/12/06/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E6%95%B0%E6%8D%AE%E6%95%B4%E7%90%86%E4%B8%8E%E5%B1%95%E7%A4%BA/"/>
    <id>https://zhangslob.github.io/2017/12/06/统计学学习笔记（二）数据整理与展示/</id>
    <published>2017-12-06T14:19:18.000Z</published>
    <updated>2017-12-06T15:00:29.414Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十八篇原创文章
</code></pre><p><img src="https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=3270851487,1770960951&amp;fm=27&amp;gp=0.jpg" alt=""></p>
<a id="more"></a>
<p>这是统计学的第二篇笔记，主要记录了如何整理数据与展示数据，书本上是这样说，但是我觉得现在人们会更多的叫做数据清洗与数据可视化。命名无所谓，掌握方法就好。</p>
<p>下面是正文。</p>
<blockquote>
<p>可以接着这里看哦 <a href="https://zhangslob.github.io/2017/11/30/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/">统计学学习笔记（一）</a></p>
</blockquote>
<hr>
<h1 id="数据整理与展示"><a href="#数据整理与展示" class="headerlink" title="数据整理与展示"></a>数据整理与展示</h1><h3 id="3-1-数据的预处理"><a href="#3-1-数据的预处理" class="headerlink" title="3.1 数据的预处理"></a>3.1 数据的预处理</h3><h4 id="3-1-1-数据审核"><a href="#3-1-1-数据审核" class="headerlink" title="3.1.1 数据审核"></a>3.1.1 数据审核</h4><ul>
<li>概念：检查数据是否有错误</li>
<li>对于通过调查取得的原始数据，主要从完整性和准确性两个方面去审核</li>
<li>对于通过其他渠道获得的二手数据，主要审核数据的适用性和时效性</li>
</ul>
<h4 id="3-1-2-数据筛选"><a href="#3-1-2-数据筛选" class="headerlink" title="3.1.2 数据筛选"></a>3.1.2 数据筛选</h4><ul>
<li>删除某些不符合要求的数据和有明显错误的数据</li>
<li>将符合某种特定条件的数据筛选出来，而将不符合特定条件的数据予以剔除</li>
<li>Excel举例</li>
</ul>
<h4 id="3-1-3-数据排序"><a href="#3-1-3-数据排序" class="headerlink" title="3.1.3 数据排序"></a>3.1.3 数据排序</h4><ul>
<li>概念：按一定顺序将数据排列，以便于研究者通过数据发现一些明显的特征或趋势，找到解决问题的线索</li>
<li>通过数据类型选择排序方式：字母型数据、汉字型数据、数值型数据</li>
</ul>
<blockquote>
<p>这个好像也叫做数据清洗</p>
</blockquote>
<h3 id="3-2-品质数据的整理与展示"><a href="#3-2-品质数据的整理与展示" class="headerlink" title="3.2 品质数据的整理与展示"></a>3.2 品质数据的整理与展示</h3><h4 id="3-2-1-频数与频数分布"><a href="#3-2-1-频数与频数分布" class="headerlink" title="3.2.1 频数与频数分布"></a>3.2.1 频数与频数分布</h4><ul>
<li>落在某一特定类别中的数据个数，成为频数</li>
<li>把各个类别及落在其中的相应频数全部列出，并用表格形式表现出来，成为频数分布</li>
</ul>
<blockquote>
<p>好像不止表格吧</p>
</blockquote>
<ul>
<li>一个总体（或样本）中各个部分的数据与全部数据之比，成为比例</li>
<li>将比例乘以100得到的数值，成为百分比或百分数用%表示</li>
<li>总体（或样本）中各不同类别数值之间的比值，成为比率</li>
</ul>
<h4 id="3-2-2-品质数据的展示"><a href="#3-2-2-品质数据的展示" class="headerlink" title="3.2.2 品质数据的展示"></a>3.2.2 品质数据的展示</h4><ol>
<li>条形图</li>
<li>饼图</li>
<li>环形图</li>
</ol>
<h3 id="3-3-数值型数据的整理与展示"><a href="#3-3-数值型数据的整理与展示" class="headerlink" title="3.3 数值型数据的整理与展示"></a>3.3 数值型数据的整理与展示</h3><h4 id="3-3-1-数据分组"><a href="#3-3-1-数据分组" class="headerlink" title="3.3.1 数据分组"></a>3.3.1 数据分组</h4><ul>
<li>根据统计分析的需要，将原始数据按照某种标准划分成不同的组别，成为数据分组</li>
<li>在组距分组中，一个组的最小值称为下限，一个组的最大值称为上限</li>
<li>每一组的上限和下限之间的中间值称为组中值</li>
</ul>
<h4 id="3-3-2-数值型数据的图示"><a href="#3-3-2-数值型数据的图示" class="headerlink" title="3.3.2 数值型数据的图示"></a>3.3.2 数值型数据的图示</h4><ul>
<li>分组数据看分布：直方图（histogram）</li>
<li>未分组数据看分布：茎叶图和箱型图</li>
<li>多变量数据的展示：雷达图</li>
</ul>
<h3 id="3-4-使用图表的注意事项"><a href="#3-4-使用图表的注意事项" class="headerlink" title="3.4 使用图表的注意事项"></a>3.4 使用图表的注意事项</h3><h4 id="优秀图表特征："><a href="#优秀图表特征：" class="headerlink" title="优秀图表特征："></a>优秀图表特征：</h4><ol>
<li>显示数据</li>
<li>让读者把注意力放在图表内容上</li>
<li>避免歪曲</li>
<li>强调数据之间的比较</li>
<li>服务于一个明确的目的</li>
<li>有对图形的描述统计和文字说明</li>
</ol>
<h4 id="优秀图形应当："><a href="#优秀图形应当：" class="headerlink" title="优秀图形应当："></a>优秀图形应当：</h4><ol>
<li>精心设计，有助于洞察问题的实质</li>
<li>使复杂的观点得到简明、确切、高效的阐述</li>
<li>能在最短的时间内、以最少的笔墨给读者提供大量的信息</li>
<li>是多维的</li>
<li>表述数据的真实情况</li>
</ol>
<h1 id="品质数据"><a href="#品质数据" class="headerlink" title="品质数据"></a>品质数据</h1><p>您应该会和我一样提问，什么是品质数据？</p>
<p>品质数据:对产品或商品进行各种化学、物理、力学等试验后所得出的数据。</p>
<ol>
<li>品质数据：对产品或商品进行各种化学、物理、力学等试验后所得出的数据。</li>
<li>品质型数据：按品质标志分组所得到数据，包括分类数据和顺序数据，他们在整理和图形展示上的方法大体相同。</li>
</ol>
<blockquote>
<p>本文中提到的品质数据应该是后者</p>
</blockquote>
<h1 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h1><p>这里重点说下数据图表的选择。</p>
<p>就我自己的工作中，比较常用的就是直方图、折线图与饼图，词云图（如果算的话）。教材中也说了很多没用过的图，这个东西还是要根据自己的具体业务来操作。</p>
<p>这里推荐一个很好的网站，<a href="https://antv.alipay.com/zh-cn/vis/chart/tag-compare.html" target="_blank" rel="external">图表使用</a></p>
<p><img src="https://i.imgur.com/gxGglUB.png" alt=""></p>
<p>这个网站有多好，你一看便知，不多解释。</p>
<h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><p>有人说：一个分析项目基本八成时间在洗数据。那么什么是清洗数据。</p>
<blockquote>
<p>数据清洗是指发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性，处理无效值和缺失值等。与问卷审核不同，录入后的数据清理一般是由计算机而不是人工完成。</p>
</blockquote>
<p><strong>数据清洗的方法 </strong></p>
<ol>
<li>解决不完整数据（ 即值缺失）的方法</li>
<li>错误值的检测及解决方法</li>
<li>重复记录的检测及消除方法</li>
<li>不一致性（ 数据源内部及数据源之间）的检测及解决方法</li>
<li>转换构造</li>
<li>数据压缩</li>
</ol>
<p>老规矩，还是放一点代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</div><div class="line"><span class="keyword">import</span> matplotlib.path <span class="keyword">as</span> path</div><div class="line"></div><div class="line">fig, ax = plt.subplots()</div><div class="line"></div><div class="line"><span class="comment"># Fixing random state for reproducibility</span></div><div class="line">np.random.seed(<span class="number">19680801</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># histogram our data with numpy</span></div><div class="line"></div><div class="line">data = np.random.randn(<span class="number">1000</span>)</div><div class="line">n, bins = np.histogram(data, <span class="number">50</span>)</div><div class="line"></div><div class="line"><span class="comment"># get the corners of the rectangles for the histogram</span></div><div class="line">left = np.array(bins[:<span class="number">-1</span>])</div><div class="line">right = np.array(bins[<span class="number">1</span>:])</div><div class="line">bottom = np.zeros(len(left))</div><div class="line">top = bottom + n</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># we need a (numrects x numsides x 2) numpy array for the path helper</span></div><div class="line"><span class="comment"># function to build a compound path</span></div><div class="line">XY = np.array([[left, left, right, right], [bottom, top, top, bottom]]).T</div><div class="line"></div><div class="line"><span class="comment"># get the Path object</span></div><div class="line">barpath = path.Path.make_compound_path_from_polys(XY)</div><div class="line"></div><div class="line"><span class="comment"># make a patch out of it</span></div><div class="line">patch = patches.PathPatch(barpath)</div><div class="line">ax.add_patch(patch)</div><div class="line"></div><div class="line"><span class="comment"># update the view limits</span></div><div class="line">ax.set_xlim(left[<span class="number">0</span>], right[<span class="number">-1</span>])</div><div class="line">ax.set_ylim(bottom.min(), top.max())</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://matplotlib.org/_images/sphx_glr_histogram_path_001.png" alt="直方图绘制"></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://wiki.mbalib.com/wiki/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97" target="_blank" rel="external">数据清洗</a></li>
<li><a href="https://www.zhihu.com/question/22077960" target="_blank" rel="external">数据挖掘中常用的数据清洗方法有哪些？</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十八篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=3270851487,1770960951&amp;amp;fm=27&amp;amp;gp=0.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="统计学" scheme="https://zhangslob.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
    
      <category term="统计学" scheme="https://zhangslob.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
      <category term="数据" scheme="https://zhangslob.github.io/tags/%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy学习实例（二）采集无限滚动页面</title>
    <link href="https://zhangslob.github.io/2017/12/03/Scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E4%BA%8C%EF%BC%89%E9%87%87%E9%9B%86%E6%97%A0%E9%99%90%E6%BB%9A%E5%8A%A8%E9%A1%B5%E9%9D%A2/"/>
    <id>https://zhangslob.github.io/2017/12/03/Scrapy学习实例（二）采集无限滚动页面/</id>
    <published>2017-12-03T11:48:53.000Z</published>
    <updated>2017-12-06T14:34:14.165Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十七篇原创文章
</code></pre><p><img src="https://i.imgur.com/ON4MMPT.jpg" alt=""><br><a id="more"></a></p>
<p>上一篇写的是<a href="https://zhangslob.github.io/2017/11/29/Scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E4%B8%80%EF%BC%89/">采集虎嗅网首页的新闻数据</a>，有朋友对我说，采集多页试试看。后来研究下，虎嗅网首页是POST加载，<code>Form Data</code>中携带参数，所以只需要带上一个循环就好了。这是我最初的想法，先让我们看看Scrapy中<br>如何采集无限滚动页面？</p>
<p>先举个栗子，采集网站是<a href="http://spidyquotes.herokuapp.com/scroll" target="_blank" rel="external">quotes</a></p>
<p><img src="https://i.imgur.com/jztJDKa.png" alt=""></p>
<h1 id="分析网页"><a href="#分析网页" class="headerlink" title="分析网页"></a>分析网页</h1><p><img src="https://i.imgur.com/aaLuLEx.png" alt=""></p>
<p>下拉时，会发现更多新的请求，观察这些请求，返回的都是json数据，也就是我们所需的，再看看他们的不同，也就是参数的改变，完整链接是：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">http:<span class="regexp">//</span>spidyquotes.herokuapp.com<span class="regexp">/api/</span>quotes?page=<span class="number">2</span></div><div class="line">http:<span class="regexp">//</span>spidyquotes.herokuapp.com<span class="regexp">/api/</span>quotes?page=<span class="number">3</span></div><div class="line">http:<span class="regexp">//</span>spidyquotes.herokuapp.com<span class="regexp">/api/</span>quotes?page=<span class="number">4</span></div></pre></td></tr></table></figure>
<p>这就很清晰了。</p>
<p><img src="https://i.imgur.com/PT2kb8b.png" alt=""></p>
<p>返回的是json，我们需要解析，然后提取数据，那我们如何知道最多有多少条json呢，文件已经告诉我们了：</p>
<p><code>has_next:true</code></p>
<h1 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h1><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scrapy startproject <span class="selector-tag">quote</span></div><div class="line"></div><div class="line">cd <span class="selector-tag">quote</span></div><div class="line"></div><div class="line">scrapy genspider spiderquote http:<span class="comment">//spidyquotes.herokuapp.com/scroll</span></div></pre></td></tr></table></figure>
<h1 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h1><p>查看网站，采集text、author和tags这三个</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuoteItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    text = scrapy.Field()</div><div class="line">    author = scrapy.Field()</div><div class="line">    tag = scrapy.Field()</div></pre></td></tr></table></figure>
<h1 id="编写spider"><a href="#编写spider" class="headerlink" title="编写spider"></a>编写spider</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> json</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpiderquoteSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'spiderquote'</span></div><div class="line">    quotes_base_url = <span class="string">'http://spidyquotes.herokuapp.com/api/quotes?page=%s'</span></div><div class="line">    start_urls = [quotes_base_url % <span class="number">1</span>]</div><div class="line">    download_delay = <span class="number">1.5</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        data = json.loads(response.body)</div><div class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data.get(<span class="string">'quotes'</span>, []):</div><div class="line">            <span class="keyword">yield</span> &#123;</div><div class="line">                <span class="string">'text'</span>: item.get(<span class="string">'text'</span>),</div><div class="line">                <span class="string">'author'</span>: item.get(<span class="string">'author'</span>, &#123;&#125;).get(<span class="string">'name'</span>),</div><div class="line">                <span class="string">'tags'</span>: item.get(<span class="string">'tags'</span>),</div><div class="line">            &#125;</div><div class="line">        <span class="keyword">if</span> data[<span class="string">'has_next'</span>]:</div><div class="line">            next_page = data[<span class="string">'page'</span>] + <span class="number">1</span></div><div class="line">            <span class="keyword">yield</span> scrapy.Request(self.quotes_base_url % next_page)</div></pre></td></tr></table></figure>
<p>运行爬虫，然后就可以看到结果了。</p>
<h1 id="应用到虎嗅网"><a href="#应用到虎嗅网" class="headerlink" title="应用到虎嗅网"></a>应用到虎嗅网</h1><p>那么如何应用到虎嗅网呢？首先还是要去分析网页。</p>
<p><img src="https://i.imgur.com/x7YoUYW.png" alt=""></p>
<p>虎嗅网的参数有3个：</p>
<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">huxiu_hash_code:</span><span class="number">13</span>a3a353c52d424e1e263dda4d594e59</div><div class="line"><span class="symbol">page:</span><span class="number">3</span></div><div class="line"><span class="symbol">last_dateline:</span><span class="number">1512026700</span></div></pre></td></tr></table></figure>
<p>我们知道<code>page</code>就是翻页页码，<code>huxiu_hash_code</code>是一个不变的字符，<code>last_dateline</code>看起来像unix时间戳，验证确实如此。这个时间戳有必要带上吗，我想验证试试看。</p>
<p><img src="https://i.imgur.com/fF0ivj5.png" alt=""></p>
<p>在postman中测试，不带上<code>last_dateline</code>也是可以返回数据，并且这个json中已经告诉我们一共有多少页：</p>
<p><code>&quot;total_page&quot;: 1654</code></p>
<p>在主函数中我们可以依葫芦画瓢</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> huxiu.items <span class="keyword">import</span> HuxiuItem</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'HuXiu'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        url = <span class="string">'https://www.huxiu.com/v2_action/article_list'</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</div><div class="line">        <span class="comment"># FormRequest 是Scrapy发送POST请求的方法</span></div><div class="line">            <span class="keyword">yield</span> scrapy.FormRequest(</div><div class="line">                url = url,</div><div class="line">                formdata = &#123;<span class="string">"huxiu_hash_code"</span> : <span class="string">"13a3a353c52d424e1e263dda4d594e59"</span>, <span class="string">"page"</span> : str(i)&#125;,</div><div class="line">                callback = self.parse</div><div class="line">            )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = HuxiuItem()</div><div class="line">        data = json.loads(response.text)</div><div class="line">        s = etree.HTML(data[<span class="string">'data'</span>])</div><div class="line"></div><div class="line">        item[<span class="string">'title'</span>] = s.xpath(<span class="string">'//a[@class="transition msubstr-row2"]/text()'</span>)</div><div class="line">        item[<span class="string">'link'</span>] = s.xpath(<span class="string">'//a[@class="transition msubstr-row2"]/@href'</span>)</div><div class="line">        item[<span class="string">'author'</span>] = s.xpath(<span class="string">'//span[@class="author-name"]/text()'</span>)</div><div class="line">        item[<span class="string">'introduction'</span>] = s.xpath(<span class="string">'//div[@class="mob-sub"]/text()'</span>)</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> item</div></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/cntFATT.png" alt=""></p>
<p>输出的数据有点难看，是一段一段的。。</p>
<p>因为<code>data[&#39;data&#39;]</code>是一段html文件，所以这里选择的是xpath，不清楚这里是否直接使用Scrapy的xpath解析工具，如果可以，欢迎在评论中告诉我。</p>
<h1 id="本篇收获"><a href="#本篇收获" class="headerlink" title="本篇收获"></a>本篇收获</h1><ol>
<li>Scrapy采集动态网站：分析网页</li>
<li>使用Scrapy模拟post请求方法，<a href="https://doc.scrapy.org/en/latest/topics/request-response.html" target="_blank" rel="external">文档在这</a></li>
<li>刘亦菲好漂亮</li>
</ol>
<h1 id="待做事宜"><a href="#待做事宜" class="headerlink" title="待做事宜"></a>待做事宜</h1><ol>
<li>完善文件保存与解析</li>
<li>全站抓取大概用了3分钟，速度有点慢</li>
</ol>
<blockquote>
<p>若想评论，先翻长城</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十七篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ON4MMPT.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
      <category term="虎嗅" scheme="https://zhangslob.github.io/tags/%E8%99%8E%E5%97%85/"/>
    
  </entry>
  
  <entry>
    <title>统计学学习笔记（一）</title>
    <link href="https://zhangslob.github.io/2017/11/30/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://zhangslob.github.io/2017/11/30/统计学学习笔记（一）/</id>
    <published>2017-11-30T15:45:14.000Z</published>
    <updated>2017-12-01T14:48:31.526Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十六篇原创文章
</code></pre><p><img src="https://i.imgur.com/gmL4oSK.jpg" alt=""></p>
<a id="more"></a>
<p>这是学习统计学的第一篇笔记，以后尽量都放在这里吧。</p>
<p>发现使用hexo发文章的快捷键：</p>
<p><code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></p>
<p>下面是正文：</p>
<hr>
<h1 id="1、统计中的基本概念"><a href="#1、统计中的基本概念" class="headerlink" title="1、统计中的基本概念"></a>1、统计中的基本概念</h1><ol>
<li><strong>总体和样本</strong>。总体：所研究的全部个体；样本：总体中的一部分</li>
<li><strong>参数和统计量</strong>。参数：用来描述<strong>总体</strong>特征的概括性数字度量；统计量：用来描述<strong>样本</strong>特征的概括性数字度量。</li>
<li><strong>变量</strong>。变量、分类变量（事物类别的一个名称）、顺序变量（事物有序类别的一个名称）、数值型变量、离散型变量（只能取可数值的变量）、连续性变量。</li>
</ol>
<h1 id="2、数据的收集"><a href="#2、数据的收集" class="headerlink" title="2、数据的收集"></a>2、数据的收集</h1><h3 id="1、数据的间接来源"><a href="#1、数据的间接来源" class="headerlink" title="1、数据的间接来源"></a>1、数据的间接来源</h3><p><strong>二手数据</strong>：公开出版的或公开报道的数据。</p>
<h3 id="2、数据的直接来源"><a href="#2、数据的直接来源" class="headerlink" title="2、数据的直接来源"></a>2、数据的直接来源</h3><h4 id="（1）统计调查方式"><a href="#（1）统计调查方式" class="headerlink" title="（1）统计调查方式"></a>（1）统计调查方式</h4><ol>
<li>抽样调查：经济性、时效性强、适应面广、准确性高。</li>
<li>普查：一次性或周期性、规定调查时间、数据比较准确、范围比较狭窄。</li>
</ol>
<h4 id="（2）数据的收集方法"><a href="#（2）数据的收集方法" class="headerlink" title="（2）数据的收集方法"></a>（2）数据的收集方法</h4><ol>
<li>询问调查：访问调查、邮寄调查、电话调查、计算机辅助调查、座谈会、个别深度访问。</li>
<li>观察与实验：观察法、实验法。</li>
</ol>
<blockquote>
<p>竟然没网络爬虫，嘤嘤嘤</p>
</blockquote>
<h3 id="3、调查设计"><a href="#3、调查设计" class="headerlink" title="3、调查设计"></a>3、调查设计</h3><h4 id="（1）调查方案设计"><a href="#（1）调查方案设计" class="headerlink" title="（1）调查方案设计"></a>（1）调查方案设计</h4><ol>
<li>调查目的</li>
<li>调查对象和调查单位</li>
<li>调查项目和调查表</li>
</ol>
<h4 id="（2）调查问卷设计"><a href="#（2）调查问卷设计" class="headerlink" title="（2）调查问卷设计"></a>（2）调查问卷设计</h4><h5 id="a-调查问卷的基本结构："><a href="#a-调查问卷的基本结构：" class="headerlink" title="a.调查问卷的基本结构："></a>a.调查问卷的基本结构：</h5><ul>
<li>开头部分（问候语、填表说明、问卷编号 ）</li>
<li>甄别部分：过滤——筛选掉不需要的部分——针对特定人群</li>
<li>主体部分：调查的全部问题</li>
<li>背景部分：被调查者的背景资料</li>
</ul>
<h5 id="b-提问项目的设计："><a href="#b-提问项目的设计：" class="headerlink" title="b.提问项目的设计："></a>b.提问项目的设计：</h5><ul>
<li>提问的内容尽可能短</li>
<li>用词要确切、通俗</li>
<li>一个项目只包含一项内容</li>
<li>避免诱导性提问</li>
<li>避免否定式提问</li>
<li>避免敏感性问题</li>
</ul>
<h5 id="c-回答项目的设计"><a href="#c-回答项目的设计" class="headerlink" title="c.回答项目的设计"></a>c.回答项目的设计</h5><ul>
<li>开放性问题：灵活；整理资料困难</li>
<li>封闭性问题：两项选择法、多项选择法（单项选择型、多项选择型、限制选择型）</li>
<li>顺序选择法：按顺序排列</li>
<li>评定尺度法：和NPS有点像哦</li>
<li>双向列联法：表格表现</li>
</ul>
<h5 id="d-问题顺序的设计"><a href="#d-问题顺序的设计" class="headerlink" title="d.问题顺序的设计"></a>d.问题顺序的设计</h5><ul>
<li>问题的安排硬具有逻辑性</li>
<li>问题的顺序应先难后易</li>
<li>能引起被调查者兴趣的问题放在最前面</li>
<li>开放性问题放在后面</li>
</ul>
<h4 id="（3）统计数据的质量"><a href="#（3）统计数据的质量" class="headerlink" title="（3）统计数据的质量"></a>（3）统计数据的质量</h4><h5 id="a-统计数据的误差"><a href="#a-统计数据的误差" class="headerlink" title="a.统计数据的误差"></a>a.统计数据的误差</h5><h5 id="b-统计数据的误差"><a href="#b-统计数据的误差" class="headerlink" title="b.统计数据的误差"></a>b.统计数据的误差</h5><ul>
<li>精度</li>
<li>准确性</li>
<li>关联性</li>
<li>及时性</li>
<li>一致性</li>
<li><p>最低成本</p>
<hr>
</li>
</ul>
<p>这是统计学基础 第三版 (贾俊平)的记录，看了前两章，感觉受益匪浅，尤其是问卷的设计，比较系统、完整，可以应用在以后的工作中。</p>
<blockquote>
<p>书名：统计学基础 第三版</p>
<p>作者：贾俊平</p>
<p>出版社：中国人民大学出版社</p>
</blockquote>
<h1 id="最后说一说为什么要学统计学"><a href="#最后说一说为什么要学统计学" class="headerlink" title="最后说一说为什么要学统计学"></a>最后说一说为什么要学统计学</h1><p>最直接原因是<strong>工资高</strong>。可以去拉勾上看看“数据分析”、“数据挖掘”、“数据科学家”等职位，他们对学历的要求基本上都会有“统计学”。</p>
<p>对于我这种文科生来说，学习统计学是必经之路。<strong>敲门砖啊！</strong></p>
<p>最后记录下最近学习的数据科学的流程：</p>
<ol>
<li>业务理解</li>
<li>分析方法</li>
<li>数据要求</li>
<li>收集数据</li>
<li>数据理解</li>
<li>数据准备</li>
<li>建模（use and test）</li>
<li>模型评估</li>
<li>部署与反馈</li>
</ol>
<p>自己判断，缺少的是业务理解，对相关的业务知识了解太少；分析方法知道的太少了，接下来会着重学习一些常见的算法；数据准备也是一个大坑，不过好在自己有一些Python基础；建模才是最难的，慢慢来吧。</p>
<p><strong>最近几天需要个自己定一个学习任务，内容主要包括：统计学基础、常见算法、pandas处理数据及可视化、业务理解、Scrapy框架学习、前端（没错，学点前端很有必要）</strong></p>
<p>欢迎加我微信，一起来学习，嘤嘤嘤</p>
<blockquote>
<p>下面是常见的分析方法</p>
</blockquote>
<p><img src="https://i.imgur.com/poRqKVo.png" alt=""></p>
<p><img src="https://i.imgur.com/FLfwzDU.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十六篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/gmL4oSK.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="统计学" scheme="https://zhangslob.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
    
      <category term="统计学" scheme="https://zhangslob.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
      <category term="数据" scheme="https://zhangslob.github.io/tags/%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy学习实例（一）</title>
    <link href="https://zhangslob.github.io/2017/11/29/Scrapy%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%BE%8B%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://zhangslob.github.io/2017/11/29/Scrapy学习实例（一）/</id>
    <published>2017-11-29T13:52:32.000Z</published>
    <updated>2017-12-03T13:36:18.610Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十五篇原创文章
</code></pre><p><img src="https://i.imgur.com/n1TnhMv.png" alt=""><br><a id="more"></a></p>
<p>Hello，我又回来啦。以后就在这发文章吧，记录自己的学习历程。</p>
<blockquote>
<p>举头卖竹鼠，低头嘤嘤嘤。</p>
</blockquote>
<p>我会记录自己对Scrapy的学历经历，更重要的是理解。下面就开始吧，首先当然是创建一个项目啦！</p>
<p>我选择爬取<a href="https://www.huxiu.com/" target="_blank" rel="external">虎嗅网</a>首页的新闻列表。</p>
<h1 id="1、创建项目"><a href="#1、创建项目" class="headerlink" title="1、创建项目"></a>1、创建项目</h1><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">F:\Python\huxiu&gt;scrapy startproject huxiu</div><div class="line">New Scrapy project 'huxiu', using template directory 'c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scrapy\\templates\\pro</div><div class="line">ject', created in:</div><div class="line">    F:\Python\huxiu\huxiu</div><div class="line"></div><div class="line">You can start your first spider with:</div><div class="line">    cd huxiu</div><div class="line">    scrapy genspider example example.com</div><div class="line"></div><div class="line">F:\Python\huxiu&gt;cd huxiu</div><div class="line"></div><div class="line">F:\Python\huxiu\huxiu&gt;scrapy genspider huxiu huxiu.com</div><div class="line">Cannot create a spider with the same name as your project</div><div class="line"></div><div class="line">F:\Python\huxiu\huxiu&gt;scrapy genspider HuXiu huxiu.com</div><div class="line">Created spider 'HuXiu' using template 'basic' in module:</div><div class="line">  huxiu.spiders.HuXiu</div></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/MrrcuLg.png" alt=""></p>
<blockquote>
<p>记住爬虫和项目命名不一样</p>
</blockquote>
<h1 id="2、定义Item"><a href="#2、定义Item" class="headerlink" title="2、定义Item"></a>2、定义Item</h1><p>在<code>item.py</code>中创建<code>scrapy.Item</code>类，并定义它的类型为<code>scrapy.Field</code>的属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    title = scrapy.Field() <span class="comment">#标题</span></div><div class="line">    link = scrapy.Field() <span class="comment">#链接</span></div><div class="line">    author = scrapy.Field() <span class="comment">#作者</span></div><div class="line">    introduction = scrapy.Field() <span class="comment">#简介</span></div><div class="line">    time = scrapy.Field() <span class="comment">#时间</span></div></pre></td></tr></table></figure>
<h1 id="3、编写Spider"><a href="#3、编写Spider" class="headerlink" title="3、编写Spider"></a>3、编写Spider</h1><p><img src="https://i.imgur.com/uK5EU0x.png" alt=""></p>
<blockquote>
<p>一目了然</p>
</blockquote>
<p>在<code>huxiu/spider/HuXiu.py</code>中编写代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> huxiu.items <span class="keyword">import</span> HuxiuItem</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'HuXiu'</span></div><div class="line">    allowed_domains = [<span class="string">'huxiu.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://huxiu.com/'</span>]</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</div><div class="line">            item = HuxiuItem()</div><div class="line">            item[<span class="string">'title'</span>] = s.xpath(<span class="string">'h2/a/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'link'</span>] = s.xpath(<span class="string">'h2/a/@href'</span>)[<span class="number">0</span>].extract()</div><div class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</div><div class="line">            item[<span class="string">'author'</span>] = s.xpath(<span class="string">'div/a/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'introduction'</span>] = s.xpath(<span class="string">'div[2]/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'time'</span>] = s.xpath(<span class="string">'div/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            print(item)</div></pre></td></tr></table></figure>
<p>在终端输入命令</p>
<pre><code>scrapy crawl HuXiu
</code></pre><p>部分输出</p>
<p><img src="https://i.imgur.com/MqWJ2vP.png" alt=""></p>
<h1 id="4、深度爬取"><a href="#4、深度爬取" class="headerlink" title="4、深度爬取"></a>4、深度爬取</h1><p>哈哈，这里借用造数的命名了。其实就是爬取新闻详情页。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">from</span> huxiu.items <span class="keyword">import</span> HuxiuItem</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></div><div class="line">    name = <span class="string">'HuXiu'</span></div><div class="line">    allowed_domains = [<span class="string">'huxiu.com'</span>]</div><div class="line">    start_urls = [<span class="string">'http://huxiu.com/'</span>]</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</div><div class="line">            item = HuxiuItem()</div><div class="line">            item[<span class="string">'title'</span>] = s.xpath(<span class="string">'h2/a/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'link'</span>] = s.xpath(<span class="string">'h2/a/@href'</span>)[<span class="number">0</span>].extract()</div><div class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</div><div class="line">            item[<span class="string">'author'</span>] = s.xpath(<span class="string">'div/a/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'introduction'</span>] = s.xpath(<span class="string">'div[2]/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            item[<span class="string">'time'</span>] = s.xpath(<span class="string">'div/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">            <span class="comment">#print(item)</span></div><div class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_article)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_article</span><span class="params">(self, response)</span>:</span></div><div class="line">        item = HuxiuItem()</div><div class="line">        detail = response.xpath(<span class="string">'//div[@class="article-wrap"]'</span>)</div><div class="line">        item[<span class="string">'title'</span>] = detail.xpath(<span class="string">'h1/text()'</span>)[<span class="number">0</span>].extract().strip()</div><div class="line">        item[<span class="string">'link'</span>] = response.url</div><div class="line">        item[<span class="string">'author'</span>] = detail.xpath(<span class="string">'div[@class="article-author"]/span/a/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">        item[<span class="string">'time'</span>] = detail.xpath(<span class="string">'div[@class="article-author"]/div[@class="column-link-box"]/span/text()'</span>)[<span class="number">0</span>].extract()</div><div class="line">        print(item)</div><div class="line">        word = detail.xpath(<span class="string">'div[5]'</span>)</div><div class="line">        print(word[<span class="number">0</span>].xpath(<span class="string">'string(.)'</span>).extract()[<span class="number">0</span>])</div><div class="line">        <span class="keyword">yield</span> item</div></pre></td></tr></table></figure></p>
<p>输出结果</p>
<p><img src="https://i.imgur.com/JJoTvtU.png" alt=""></p>
<p>说明一点，<code>如何使用xpath获得多个标签下的文本</code>，这里参考了<a href="http://blog.csdn.net/MrLevo520/article/details/53158050" target="_blank" rel="external">解决：xpath取出指定多标签内所有文字text</a>，把文章详细内容打印出来，但是会遇到一些错误，可以使用<code>goose</code>来试试看。</p>
<h1 id="Python-Goose-Article-Extractor"><a href="#Python-Goose-Article-Extractor" class="headerlink" title="Python-Goose - Article Extractor"></a><a href="https://github.com/grangier/python-goose" target="_blank" rel="external">Python-Goose - Article Extractor</a></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> goose <span class="keyword">import</span> Goose</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> goose.text <span class="keyword">import</span> StopWordsChinese</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>url  = <span class="string">'http://www.bbc.co.uk/zhongwen/simp/chinese_news/2012/12/121210_hongkong_politics.shtml'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>g = Goose(&#123;<span class="string">'stopwords_class'</span>: StopWordsChinese&#125;)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>article = g.extract(url=url)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> article.cleaned_text[:<span class="number">150</span>]</div><div class="line">香港行政长官梁振英在各方压力下就其大宅的违章建筑（僭建）问题到立法会接受质询，并向香港民众道歉。</div><div class="line"></div><div class="line">梁振英在星期二（<span class="number">12</span>月<span class="number">10</span>日）的答问大会开始之际在其演说中道歉，但强调他在违章建筑问题上没有隐瞒的意图和动机。</div><div class="line"></div><div class="line">一些亲北京阵营议员欢迎梁振英道歉，且认为应能获得香港民众接受，但这些议员也质问梁振英有</div></pre></td></tr></table></figure>
<h1 id="参考文章："><a href="#参考文章：" class="headerlink" title="参考文章："></a>参考文章：</h1><ul>
<li><a href="https://www.xncoding.com/2016/03/10/scrapy-02.html" target="_blank" rel="external">Scrapy笔记02- 完整示例</a></li>
<li><a href="http://blog.csdn.net/MrLevo520/article/details/53158050" target="_blank" rel="external">解决：xpath取出指定多标签内所有文字text</a></li>
</ul>
<blockquote>
<p>若想评论，先翻长城</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十五篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/n1TnhMv.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/categories/Scrapy/"/>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scrapy" scheme="https://zhangslob.github.io/tags/Scrapy/"/>
    
      <category term="虎嗅" scheme="https://zhangslob.github.io/tags/%E8%99%8E%E5%97%85/"/>
    
  </entry>
  
  <entry>
    <title>爬虫三步走（二）解析源码</title>
    <link href="https://zhangslob.github.io/2017/05/26/%E7%88%AC%E8%99%AB%E4%B8%89%E6%AD%A5%E8%B5%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E8%A7%A3%E6%9E%90%E6%BA%90%E7%A0%81/"/>
    <id>https://zhangslob.github.io/2017/05/26/爬虫三步走（二）解析源码/</id>
    <published>2017-05-26T12:15:20.000Z</published>
    <updated>2017-05-26T12:16:58.084Z</updated>
    
    <content type="html"><![CDATA[<pre><code>这是崔斯特的第十四篇原创文章
</code></pre><p><img src="http://i.imgur.com/Dw2WeLb.png" alt=""></p>
<p>爬虫三步走：获取源码、解析源码、数据储存</p>
<a id="more"></a>
<p>上一期讲了如何获取网页源码的方法，这一期说一说怎么从其中获得我们需要的和数据。</p>
<p>解析网页的方法很多，最常见的就是BeautifulSoup和正则了，其他的像xpath、PyQuery等等，其中我觉得最好用的就是xpath了，xpath真的超级简单好用，学了之后再也不想取用<code>美丽汤</code>了。下面介绍xpath的使用方法。<br><img src="http://i.imgur.com/tVgBAeY.jpg" alt=""></p>
<p>首先需要安装lxml，windows下安装lxml是个大坑，知乎上有人给出了解决方法<a href="https://www.zhihu.com/question/30047496" target="_blank" rel="external">Python LXML模块死活安装不了怎么办？</a></p>
<p>详细的用法可以参考<a href="https://zhuanlan.zhihu.com/p/25572729" target="_blank" rel="external">爬虫入门到精通-网页的解析（xpath）</a></p>
<p>在这里我们尝试使用xpath来迅速获取数据。</p>
<p>例如想要获熊猫直播<a href="http://www.huya.com/g/lol" target="_blank" rel="external">虎牙直播</a>下主播的ID</p>
<p><img src="http://i.imgur.com/HEGXkuM.jpg" alt=""></p>
<pre><code>import requests
from lxml import etree

url = &apos;http://www.huya.com/g/lol&apos;
headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
res = requests.get(url,headers=headers).text
s = etree.HTML(res)
print(s.xpath(&apos;//i[@class=&quot;nick&quot;]/text()&apos;))
</code></pre><p>输出：</p>
<p><img src="http://i.imgur.com/czo9c1M.png" alt=""></p>
<p>下面一步步讲解为什么这样做。</p>
<pre><code>import requests
from lxml import etree
</code></pre><p>首先是导入模块，<code>requests</code>很常见，但是xpath需要    <code>from lxml import etree</code>，你肯点想问为什么这样写，回答是“我也不知道”，就像是约定俗成的东西一样。</p>
<pre><code>url = &apos;http://www.huya.com/g/lol&apos;
headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
res = requests.get(url,headers=headers).text
</code></pre><p>这三步就是平常获取源码的过程，很简单。</p>
<pre><code>s = etree.HTML(res)
</code></pre><p>给一个html，返回xml结构，为什么这样写？？答案和上面一样。最重要的就是下面的这一步：</p>
<pre><code>s.xpath(&apos;//i[@class=&quot;nick&quot;]/text()&apos;)
</code></pre><p><img src="http://i.imgur.com/QP54jeA.png" alt=""></p>
<p>按下F12看到“爱拍-古手羽”在<code>i</code>标签下，接着我们右键打开“查看网页源代码”，搜索“爱拍-古手羽”</p>
<p><img src="http://i.imgur.com/BkKsKgH.png" alt=""></p>
<p>确实找到了“爱拍-古手羽”就在<code>i</code>标签下，那我们就把他提出来吧！</p>
<p><code>s.xpath(&#39;//i[@class=&quot;nick&quot;]/text()&#39;)</code></p>
<p>这个段代码意思是，找到class为“nick”的<code>i</code>标签，返回其中的文本信息，当然你也可以返回<code>i</code>标签中的<code>title</code>，写法如下：</p>
<p><code>s.xpath(&#39;//i[@class=&quot;nick&quot;]/@title&#39;)</code></p>
<p><code>text()</code>返回的是文本信息，<code>@title</code>则是标签里面的具体属性的值，例如我想知道观众人数</p>
<pre><code>import requests
from lxml import etree

url = &apos;http://www.huya.com/g/lol&apos;
headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;}
res = requests.get(url,headers=headers).text
s = etree.HTML(res)
print(s.xpath(&apos;//i[@class=&quot;js-num&quot;]/text()&apos;))
</code></pre><p>只需在原来基础上修改一个属性，<code>i</code>标签class为“js-num”里面的值</p>
<p><img src="http://i.imgur.com/jTKeBqW.png" alt=""></p>
<pre><code>print(s.xpath(&apos;//i[@class=&quot;js-num&quot;]/text()&apos;))
</code></pre><p>返回结果是：</p>
<p><img src="http://i.imgur.com/nuuT658.png" alt=""></p>
<p>说明：在运行代码中，发现虎牙反爬虫做得挺好的，瞬间就识别爬虫身份并封了IP，所以我换了IP去访问，至于如何设置代理，在我的上一篇文章中有说到，去看看吧。</p>
<p>在实际操作中，你可能会遇到更加复杂的情况，所以一定记得去看看详细的教程。<a href="https://zhuanlan.zhihu.com/p/25572729" target="_blank" rel="external">爬虫入门到精通-网页的解析（xpath）</a></p>
<p>小广告：喜欢爬虫、数据的可以关注一下我的微信公众号（<strong>zhangslob</strong>），多多交流。</p>
<p><img src="http://i.imgur.com/PZMOvTP.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;这是崔斯特的第十四篇原创文章
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/Dw2WeLb.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;爬虫三步走：获取源码、解析源码、数据储存&lt;/p&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="https://zhangslob.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python入门" scheme="https://zhangslob.github.io/tags/Python%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
</feed>
